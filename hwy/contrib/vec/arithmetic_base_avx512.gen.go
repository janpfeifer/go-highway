// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package vec

import (
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
)

func BaseAdd_avx512_Float16(dst []hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Add(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vd1.Add(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vs2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vd2.Add(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vs3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vd3.Add(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseAdd_fallback_Float16(dst[i:n], s[i:n])
	}
}

func BaseAdd_avx512_BFloat16(dst []hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Add(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vd1.Add(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vs2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vd2.Add(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vs3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vd3.Add(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseAdd_fallback_BFloat16(dst[i:n], s[i:n])
	}
}

func BaseAdd_avx512(dst []float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i])))
		vs := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i])))
		result := vd.Add(vs)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vs1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+16])))
		result1 := vd1.Add(vs1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vd2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vs2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+32])))
		result2 := vd2.Add(vs2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vd3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+48])))
		vs3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+48])))
		result3 := vd3.Add(vs3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	if i < n {
		BaseAdd_fallback(dst[i:n], s[i:n])
	}
}

func BaseAdd_avx512_Float64(dst []float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i])))
		vs := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i])))
		result := vd.Add(vs)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vs1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+8])))
		result1 := vd1.Add(vs1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vd2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vs2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+16])))
		result2 := vd2.Add(vs2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vd3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+24])))
		vs3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+24])))
		result3 := vd3.Add(vs3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	if i < n {
		BaseAdd_fallback_Float64(dst[i:n], s[i:n])
	}
}

func BaseAddTo_avx512_Float16(dst []hwy.Float16, a []hwy.Float16, b []hwy.Float16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Add(vb)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		va1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+16:][0]))
		result1 := va1.Add(vb1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		va2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+32:][0]))
		vb2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+32:][0]))
		result2 := va2.Add(vb2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		va3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+48:][0]))
		vb3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+48:][0]))
		result3 := va3.Add(vb3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseAddTo_fallback_Float16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseAddTo_avx512_BFloat16(dst []hwy.BFloat16, a []hwy.BFloat16, b []hwy.BFloat16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Add(vb)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		va1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+16:][0]))
		result1 := va1.Add(vb1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		va2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+32:][0]))
		vb2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+32:][0]))
		result2 := va2.Add(vb2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		va3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+48:][0]))
		vb3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+48:][0]))
		result3 := va3.Add(vb3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseAddTo_fallback_BFloat16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseAddTo_avx512(dst []float32, a []float32, b []float32) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i])))
		vb := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i])))
		result := va.Add(vb)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		va1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+16])))
		vb1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+16])))
		result1 := va1.Add(vb1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		va2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+32])))
		vb2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+32])))
		result2 := va2.Add(vb2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		va3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+48])))
		vb3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+48])))
		result3 := va3.Add(vb3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	if i < n {
		BaseAddTo_fallback(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseAddTo_avx512_Float64(dst []float64, a []float64, b []float64) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i])))
		vb := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i])))
		result := va.Add(vb)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		va1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+8])))
		vb1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+8])))
		result1 := va1.Add(vb1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		va2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+16])))
		vb2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+16])))
		result2 := va2.Add(vb2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		va3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+24])))
		vb3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+24])))
		result3 := va3.Add(vb3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	if i < n {
		BaseAddTo_fallback_Float64(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseSub_avx512_Float16(dst []hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Sub(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vd1.Sub(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vs2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vd2.Sub(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vs3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vd3.Sub(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseSub_fallback_Float16(dst[i:n], s[i:n])
	}
}

func BaseSub_avx512_BFloat16(dst []hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Sub(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vd1.Sub(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vs2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vd2.Sub(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vs3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vd3.Sub(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseSub_fallback_BFloat16(dst[i:n], s[i:n])
	}
}

func BaseSub_avx512(dst []float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i])))
		vs := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i])))
		result := vd.Sub(vs)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vs1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+16])))
		result1 := vd1.Sub(vs1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vd2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vs2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+32])))
		result2 := vd2.Sub(vs2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vd3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+48])))
		vs3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+48])))
		result3 := vd3.Sub(vs3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	if i < n {
		BaseSub_fallback(dst[i:n], s[i:n])
	}
}

func BaseSub_avx512_Float64(dst []float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i])))
		vs := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i])))
		result := vd.Sub(vs)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vs1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+8])))
		result1 := vd1.Sub(vs1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vd2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vs2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+16])))
		result2 := vd2.Sub(vs2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vd3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+24])))
		vs3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+24])))
		result3 := vd3.Sub(vs3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	if i < n {
		BaseSub_fallback_Float64(dst[i:n], s[i:n])
	}
}

func BaseSubTo_avx512_Float16(dst []hwy.Float16, a []hwy.Float16, b []hwy.Float16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Sub(vb)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		va1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+16:][0]))
		result1 := va1.Sub(vb1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		va2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+32:][0]))
		vb2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+32:][0]))
		result2 := va2.Sub(vb2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		va3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+48:][0]))
		vb3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+48:][0]))
		result3 := va3.Sub(vb3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseSubTo_fallback_Float16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseSubTo_avx512_BFloat16(dst []hwy.BFloat16, a []hwy.BFloat16, b []hwy.BFloat16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Sub(vb)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		va1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+16:][0]))
		result1 := va1.Sub(vb1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		va2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+32:][0]))
		vb2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+32:][0]))
		result2 := va2.Sub(vb2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		va3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+48:][0]))
		vb3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+48:][0]))
		result3 := va3.Sub(vb3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseSubTo_fallback_BFloat16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseSubTo_avx512(dst []float32, a []float32, b []float32) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i])))
		vb := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i])))
		result := va.Sub(vb)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		va1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+16])))
		vb1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+16])))
		result1 := va1.Sub(vb1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		va2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+32])))
		vb2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+32])))
		result2 := va2.Sub(vb2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		va3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+48])))
		vb3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+48])))
		result3 := va3.Sub(vb3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	if i < n {
		BaseSubTo_fallback(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseSubTo_avx512_Float64(dst []float64, a []float64, b []float64) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i])))
		vb := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i])))
		result := va.Sub(vb)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		va1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+8])))
		vb1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+8])))
		result1 := va1.Sub(vb1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		va2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+16])))
		vb2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+16])))
		result2 := va2.Sub(vb2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		va3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+24])))
		vb3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+24])))
		result3 := va3.Sub(vb3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	if i < n {
		BaseSubTo_fallback_Float64(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseMul_avx512_Float16(dst []hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Mul(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vd1.Mul(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vs2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vd2.Mul(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vs3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vd3.Mul(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseMul_fallback_Float16(dst[i:n], s[i:n])
	}
}

func BaseMul_avx512_BFloat16(dst []hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Mul(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vd1.Mul(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vs2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vd2.Mul(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vs3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vd3.Mul(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseMul_fallback_BFloat16(dst[i:n], s[i:n])
	}
}

func BaseMul_avx512(dst []float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i])))
		vs := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i])))
		result := vd.Mul(vs)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vs1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+16])))
		result1 := vd1.Mul(vs1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vd2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vs2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+32])))
		result2 := vd2.Mul(vs2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vd3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+48])))
		vs3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+48])))
		result3 := vd3.Mul(vs3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	if i < n {
		BaseMul_fallback(dst[i:n], s[i:n])
	}
}

func BaseMul_avx512_Float64(dst []float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i])))
		vs := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i])))
		result := vd.Mul(vs)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vs1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+8])))
		result1 := vd1.Mul(vs1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vd2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vs2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+16])))
		result2 := vd2.Mul(vs2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vd3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+24])))
		vs3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+24])))
		result3 := vd3.Mul(vs3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	if i < n {
		BaseMul_fallback_Float64(dst[i:n], s[i:n])
	}
}

func BaseMulTo_avx512_Float16(dst []hwy.Float16, a []hwy.Float16, b []hwy.Float16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Mul(vb)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		va1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+16:][0]))
		result1 := va1.Mul(vb1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		va2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+32:][0]))
		vb2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+32:][0]))
		result2 := va2.Mul(vb2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		va3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+48:][0]))
		vb3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+48:][0]))
		result3 := va3.Mul(vb3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseMulTo_fallback_Float16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseMulTo_avx512_BFloat16(dst []hwy.BFloat16, a []hwy.BFloat16, b []hwy.BFloat16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Mul(vb)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		va1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+16:][0]))
		result1 := va1.Mul(vb1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		va2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+32:][0]))
		vb2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+32:][0]))
		result2 := va2.Mul(vb2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		va3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+48:][0]))
		vb3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+48:][0]))
		result3 := va3.Mul(vb3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseMulTo_fallback_BFloat16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseMulTo_avx512(dst []float32, a []float32, b []float32) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i])))
		vb := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i])))
		result := va.Mul(vb)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		va1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+16])))
		vb1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+16])))
		result1 := va1.Mul(vb1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		va2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+32])))
		vb2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+32])))
		result2 := va2.Mul(vb2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		va3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+48])))
		vb3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+48])))
		result3 := va3.Mul(vb3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	if i < n {
		BaseMulTo_fallback(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseMulTo_avx512_Float64(dst []float64, a []float64, b []float64) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i])))
		vb := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i])))
		result := va.Mul(vb)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		va1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+8])))
		vb1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+8])))
		result1 := va1.Mul(vb1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		va2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+16])))
		vb2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+16])))
		result2 := va2.Mul(vb2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		va3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+24])))
		vb3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+24])))
		result3 := va3.Mul(vb3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	if i < n {
		BaseMulTo_fallback_Float64(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseDiv_avx512_Float16(dst []hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Div(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vd1.Div(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vs2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vd2.Div(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vs3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vd3.Div(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseDiv_fallback_Float16(dst[i:n], s[i:n])
	}
}

func BaseDiv_avx512_BFloat16(dst []hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Div(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vd1.Div(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vs2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vd2.Div(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vs3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vd3.Div(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseDiv_fallback_BFloat16(dst[i:n], s[i:n])
	}
}

func BaseDiv_avx512(dst []float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i])))
		vs := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i])))
		result := vd.Div(vs)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vs1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+16])))
		result1 := vd1.Div(vs1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vd2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vs2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+32])))
		result2 := vd2.Div(vs2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vd3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+48])))
		vs3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+48])))
		result3 := vd3.Div(vs3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	if i < n {
		BaseDiv_fallback(dst[i:n], s[i:n])
	}
}

func BaseDiv_avx512_Float64(dst []float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i])))
		vs := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i])))
		result := vd.Div(vs)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vs1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+8])))
		result1 := vd1.Div(vs1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vd2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vs2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+16])))
		result2 := vd2.Div(vs2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vd3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+24])))
		vs3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+24])))
		result3 := vd3.Div(vs3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	if i < n {
		BaseDiv_fallback_Float64(dst[i:n], s[i:n])
	}
}

func BaseDivTo_avx512_Float16(dst []hwy.Float16, a []hwy.Float16, b []hwy.Float16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Div(vb)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		va1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+16:][0]))
		result1 := va1.Div(vb1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		va2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+32:][0]))
		vb2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+32:][0]))
		result2 := va2.Div(vb2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		va3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+48:][0]))
		vb3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+48:][0]))
		result3 := va3.Div(vb3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseDivTo_fallback_Float16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseDivTo_avx512_BFloat16(dst []hwy.BFloat16, a []hwy.BFloat16, b []hwy.BFloat16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Div(vb)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		va1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+16:][0]))
		result1 := va1.Div(vb1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		va2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+32:][0]))
		vb2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+32:][0]))
		result2 := va2.Div(vb2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		va3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&a[i+48:][0]))
		vb3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&b[i+48:][0]))
		result3 := va3.Div(vb3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseDivTo_fallback_BFloat16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseDivTo_avx512(dst []float32, a []float32, b []float32) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i])))
		vb := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i])))
		result := va.Div(vb)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		va1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+16])))
		vb1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+16])))
		result1 := va1.Div(vb1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		va2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+32])))
		vb2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+32])))
		result2 := va2.Div(vb2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		va3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&a[i+48])))
		vb3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&b[i+48])))
		result3 := va3.Div(vb3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	if i < n {
		BaseDivTo_fallback(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseDivTo_avx512_Float64(dst []float64, a []float64, b []float64) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i])))
		vb := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i])))
		result := va.Div(vb)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		va1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+8])))
		vb1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+8])))
		result1 := va1.Div(vb1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		va2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+16])))
		vb2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+16])))
		result2 := va2.Div(vb2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		va3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&a[i+24])))
		vb3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&b[i+24])))
		result3 := va3.Div(vb3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	if i < n {
		BaseDivTo_fallback_Float64(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseScale_avx512_Float16(c hwy.Float16, dst []hwy.Float16) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastFloat16x16AVX512(uint16(c))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		result := vd.Mul(vc)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		result1 := vd1.Mul(vc)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		result2 := vd2.Mul(vc)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		result3 := vd3.Mul(vc)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseScale_fallback_Float16(c, dst[i:n])
	}
}

func BaseScale_avx512_BFloat16(c hwy.BFloat16, dst []hwy.BFloat16) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastBFloat16x16AVX512(uint16(c))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		result := vd.Mul(vc)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		result1 := vd1.Mul(vc)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		result2 := vd2.Mul(vc)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		result3 := vd3.Mul(vc)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseScale_fallback_BFloat16(c, dst[i:n])
	}
}

func BaseScale_avx512(c float32, dst []float32) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := archsimd.BroadcastFloat32x16(c)
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i])))
		result := vd.Mul(vc)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		result1 := vd1.Mul(vc)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vd2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		result2 := vd2.Mul(vc)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vd3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+48])))
		result3 := vd3.Mul(vc)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	for ; i < n; i++ {
		dst[i] *= c
	}
}

func BaseScale_avx512_Float64(c float64, dst []float64) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := archsimd.BroadcastFloat64x8(c)
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i])))
		result := vd.Mul(vc)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		result1 := vd1.Mul(vc)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vd2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		result2 := vd2.Mul(vc)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vd3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+24])))
		result3 := vd3.Mul(vc)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	for ; i < n; i++ {
		dst[i] *= c
	}
}

func BaseScaleTo_avx512_Float16(dst []hwy.Float16, c hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	vc := asm.BroadcastFloat16x16AVX512(uint16(c))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vs := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vc.Mul(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vs1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vc.Mul(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vs2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vc.Mul(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vs3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vc.Mul(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseScaleTo_fallback_Float16(dst[i:n], c, s[i:n])
	}
}

func BaseScaleTo_avx512_BFloat16(dst []hwy.BFloat16, c hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	vc := asm.BroadcastBFloat16x16AVX512(uint16(c))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vs := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i:][0]))
		result := vc.Mul(vs)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vs1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+16:][0]))
		result1 := vc.Mul(vs1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vs2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+32:][0]))
		result2 := vc.Mul(vs2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vs3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&s[i+48:][0]))
		result3 := vc.Mul(vs3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseScaleTo_fallback_BFloat16(dst[i:n], c, s[i:n])
	}
}

func BaseScaleTo_avx512(dst []float32, c float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	vc := archsimd.BroadcastFloat32x16(c)
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vs := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i])))
		result := vc.Mul(vs)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		vs1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+16])))
		result1 := vc.Mul(vs1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vs2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+32])))
		result2 := vc.Mul(vs2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vs3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&s[i+48])))
		result3 := vc.Mul(vs3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	for ; i < n; i++ {
		dst[i] = c * s[i]
	}
}

func BaseScaleTo_avx512_Float64(dst []float64, c float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	vc := archsimd.BroadcastFloat64x8(c)
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vs := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i])))
		result := vc.Mul(vs)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		vs1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+8])))
		result1 := vc.Mul(vs1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vs2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+16])))
		result2 := vc.Mul(vs2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vs3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&s[i+24])))
		result3 := vc.Mul(vs3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	for ; i < n; i++ {
		dst[i] = c * s[i]
	}
}

func BaseAddConst_avx512_Float16(c hwy.Float16, dst []hwy.Float16) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastFloat16x16AVX512(uint16(c))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		result := vd.Add(vc)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		result1 := vd1.Add(vc)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		result2 := vd2.Add(vc)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		result3 := vd3.Add(vc)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseAddConst_fallback_Float16(c, dst[i:n])
	}
}

func BaseAddConst_avx512_BFloat16(c hwy.BFloat16, dst []hwy.BFloat16) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastBFloat16x16AVX512(uint16(c))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		result := vd.Add(vc)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		result1 := vd1.Add(vc)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		result2 := vd2.Add(vc)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		result3 := vd3.Add(vc)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseAddConst_fallback_BFloat16(c, dst[i:n])
	}
}

func BaseAddConst_avx512(c float32, dst []float32) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := archsimd.BroadcastFloat32x16(c)
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i])))
		result := vd.Add(vc)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		result1 := vd1.Add(vc)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vd2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		result2 := vd2.Add(vc)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vd3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+48])))
		result3 := vd3.Add(vc)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	for ; i < n; i++ {
		dst[i] += c
	}
}

func BaseAddConst_avx512_Float64(c float64, dst []float64) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := archsimd.BroadcastFloat64x8(c)
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i])))
		result := vd.Add(vc)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		result1 := vd1.Add(vc)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vd2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		result2 := vd2.Add(vc)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vd3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+24])))
		result3 := vd3.Add(vc)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	for ; i < n; i++ {
		dst[i] += c
	}
}

func BaseMulConstAddTo_avx512_Float16(dst []hwy.Float16, a hwy.Float16, x []hwy.Float16) {
	if len(dst) == 0 || len(x) == 0 {
		return
	}
	n := min(len(dst), len(x))
	va := asm.BroadcastFloat16x16AVX512(uint16(a))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vx := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&x[i:][0]))
		result := va.MulAdd(vx, vd)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vx1 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&x[i+16:][0]))
		result1 := va.MulAdd(vx1, vd1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vx2 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&x[i+32:][0]))
		result2 := va.MulAdd(vx2, vd2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vx3 := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&x[i+48:][0]))
		result3 := va.MulAdd(vx3, vd3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseMulConstAddTo_fallback_Float16(dst[i:n], a, x[i:n])
	}
}

func BaseMulConstAddTo_avx512_BFloat16(dst []hwy.BFloat16, a hwy.BFloat16, x []hwy.BFloat16) {
	if len(dst) == 0 || len(x) == 0 {
		return
	}
	n := min(len(dst), len(x))
	va := asm.BroadcastBFloat16x16AVX512(uint16(a))
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i:][0]))
		vx := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&x[i:][0]))
		result := va.MulAdd(vx, vd)
		result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i:]))), len(dst[i:])))
		vd1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vx1 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&x[i+16:][0]))
		result1 := va.MulAdd(vx1, vd1)
		result1.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+16:]))), len(dst[i+16:])))
		vd2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+32:][0]))
		vx2 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&x[i+32:][0]))
		result2 := va.MulAdd(vx2, vd2)
		result2.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+32:]))), len(dst[i+32:])))
		vd3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&dst[i+48:][0]))
		vx3 := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&x[i+48:][0]))
		result3 := va.MulAdd(vx3, vd3)
		result3.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(dst[i+48:]))), len(dst[i+48:])))
	}
	if i < n {
		BaseMulConstAddTo_fallback_BFloat16(dst[i:n], a, x[i:n])
	}
}

func BaseMulConstAddTo_avx512(dst []float32, a float32, x []float32) {
	if len(dst) == 0 || len(x) == 0 {
		return
	}
	n := min(len(dst), len(x))
	va := archsimd.BroadcastFloat32x16(a)
	lanes := 16
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i])))
		vx := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&x[i])))
		result := va.MulAdd(vx, vd)
		result.Store((*[16]float32)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vx1 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&x[i+16])))
		result1 := va.MulAdd(vx1, vd1)
		result1.Store((*[16]float32)(unsafe.Pointer(&dst[i+16])))
		vd2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vx2 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&x[i+32])))
		result2 := va.MulAdd(vx2, vd2)
		result2.Store((*[16]float32)(unsafe.Pointer(&dst[i+32])))
		vd3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&dst[i+48])))
		vx3 := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&x[i+48])))
		result3 := va.MulAdd(vx3, vd3)
		result3.Store((*[16]float32)(unsafe.Pointer(&dst[i+48])))
	}
	for ; i < n; i++ {
		dst[i] += a * x[i]
	}
}

func BaseMulConstAddTo_avx512_Float64(dst []float64, a float64, x []float64) {
	if len(dst) == 0 || len(x) == 0 {
		return
	}
	n := min(len(dst), len(x))
	va := archsimd.BroadcastFloat64x8(a)
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i])))
		vx := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&x[i])))
		result := va.MulAdd(vx, vd)
		result.Store((*[8]float64)(unsafe.Pointer(&dst[i])))
		vd1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vx1 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&x[i+8])))
		result1 := va.MulAdd(vx1, vd1)
		result1.Store((*[8]float64)(unsafe.Pointer(&dst[i+8])))
		vd2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vx2 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&x[i+16])))
		result2 := va.MulAdd(vx2, vd2)
		result2.Store((*[8]float64)(unsafe.Pointer(&dst[i+16])))
		vd3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&dst[i+24])))
		vx3 := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&x[i+24])))
		result3 := va.MulAdd(vx3, vd3)
		result3.Store((*[8]float64)(unsafe.Pointer(&dst[i+24])))
	}
	for ; i < n; i++ {
		dst[i] += a * x[i]
	}
}
