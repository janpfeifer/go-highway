// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build arm64

package vec

import (
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
)

func BaseAdd_neon_Float16(dst []hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Add(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vs1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vd1.Add(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vd2.Add(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vs3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vd3.Add(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseAdd_fallback_Float16(dst[i:n], s[i:n])
	}
}

func BaseAdd_neon_BFloat16(dst []hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Add(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vs1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vd1.Add(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vd2.Add(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vs3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vd3.Add(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseAdd_fallback_BFloat16(dst[i:n], s[i:n])
	}
}

func BaseAdd_neon(dst []float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i])))
		vs := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i])))
		result := vd.Add(vs)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vs1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+4])))
		result1 := vd1.Add(vs1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vd2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vs2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+8])))
		result2 := vd2.Add(vs2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vd3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+12])))
		vs3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+12])))
		result3 := vd3.Add(vs3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	if i < n {
		BaseAdd_fallback(dst[i:n], s[i:n])
	}
}

func BaseAdd_neon_Float64(dst []float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i])))
		vs := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i])))
		result := vd.Add(vs)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vs1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+2])))
		result1 := vd1.Add(vs1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vd2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vs2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+4])))
		result2 := vd2.Add(vs2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vd3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+6])))
		vs3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+6])))
		result3 := vd3.Add(vs3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	if i < n {
		BaseAdd_fallback_Float64(dst[i:n], s[i:n])
	}
}

func BaseAddTo_neon_Float16(dst []hwy.Float16, a []hwy.Float16, b []hwy.Float16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Add(vb)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		va1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+8:][0]))
		vb1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+8:][0]))
		result1 := va1.Add(vb1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		va2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+16:][0]))
		result2 := va2.Add(vb2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		va3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+24:][0]))
		vb3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+24:][0]))
		result3 := va3.Add(vb3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseAddTo_fallback_Float16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseAddTo_neon_BFloat16(dst []hwy.BFloat16, a []hwy.BFloat16, b []hwy.BFloat16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Add(vb)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		va1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+8:][0]))
		vb1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+8:][0]))
		result1 := va1.Add(vb1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		va2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+16:][0]))
		result2 := va2.Add(vb2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		va3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+24:][0]))
		vb3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+24:][0]))
		result3 := va3.Add(vb3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseAddTo_fallback_BFloat16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseAddTo_neon(dst []float32, a []float32, b []float32) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i])))
		vb := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i])))
		result := va.Add(vb)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		va1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+4])))
		vb1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+4])))
		result1 := va1.Add(vb1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		va2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+8])))
		vb2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+8])))
		result2 := va2.Add(vb2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		va3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+12])))
		vb3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+12])))
		result3 := va3.Add(vb3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	if i < n {
		BaseAddTo_fallback(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseAddTo_neon_Float64(dst []float64, a []float64, b []float64) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i])))
		vb := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i])))
		result := va.Add(vb)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		va1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+2])))
		vb1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+2])))
		result1 := va1.Add(vb1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		va2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+4])))
		vb2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+4])))
		result2 := va2.Add(vb2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		va3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+6])))
		vb3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+6])))
		result3 := va3.Add(vb3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	if i < n {
		BaseAddTo_fallback_Float64(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseSub_neon_Float16(dst []hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Sub(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vs1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vd1.Sub(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vd2.Sub(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vs3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vd3.Sub(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseSub_fallback_Float16(dst[i:n], s[i:n])
	}
}

func BaseSub_neon_BFloat16(dst []hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Sub(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vs1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vd1.Sub(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vd2.Sub(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vs3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vd3.Sub(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseSub_fallback_BFloat16(dst[i:n], s[i:n])
	}
}

func BaseSub_neon(dst []float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i])))
		vs := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i])))
		result := vd.Sub(vs)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vs1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+4])))
		result1 := vd1.Sub(vs1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vd2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vs2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+8])))
		result2 := vd2.Sub(vs2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vd3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+12])))
		vs3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+12])))
		result3 := vd3.Sub(vs3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	if i < n {
		BaseSub_fallback(dst[i:n], s[i:n])
	}
}

func BaseSub_neon_Float64(dst []float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i])))
		vs := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i])))
		result := vd.Sub(vs)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vs1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+2])))
		result1 := vd1.Sub(vs1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vd2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vs2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+4])))
		result2 := vd2.Sub(vs2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vd3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+6])))
		vs3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+6])))
		result3 := vd3.Sub(vs3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	if i < n {
		BaseSub_fallback_Float64(dst[i:n], s[i:n])
	}
}

func BaseSubTo_neon_Float16(dst []hwy.Float16, a []hwy.Float16, b []hwy.Float16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Sub(vb)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		va1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+8:][0]))
		vb1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+8:][0]))
		result1 := va1.Sub(vb1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		va2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+16:][0]))
		result2 := va2.Sub(vb2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		va3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+24:][0]))
		vb3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+24:][0]))
		result3 := va3.Sub(vb3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseSubTo_fallback_Float16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseSubTo_neon_BFloat16(dst []hwy.BFloat16, a []hwy.BFloat16, b []hwy.BFloat16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Sub(vb)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		va1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+8:][0]))
		vb1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+8:][0]))
		result1 := va1.Sub(vb1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		va2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+16:][0]))
		result2 := va2.Sub(vb2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		va3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+24:][0]))
		vb3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+24:][0]))
		result3 := va3.Sub(vb3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseSubTo_fallback_BFloat16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseSubTo_neon(dst []float32, a []float32, b []float32) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i])))
		vb := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i])))
		result := va.Sub(vb)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		va1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+4])))
		vb1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+4])))
		result1 := va1.Sub(vb1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		va2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+8])))
		vb2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+8])))
		result2 := va2.Sub(vb2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		va3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+12])))
		vb3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+12])))
		result3 := va3.Sub(vb3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	if i < n {
		BaseSubTo_fallback(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseSubTo_neon_Float64(dst []float64, a []float64, b []float64) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i])))
		vb := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i])))
		result := va.Sub(vb)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		va1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+2])))
		vb1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+2])))
		result1 := va1.Sub(vb1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		va2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+4])))
		vb2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+4])))
		result2 := va2.Sub(vb2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		va3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+6])))
		vb3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+6])))
		result3 := va3.Sub(vb3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	if i < n {
		BaseSubTo_fallback_Float64(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseMul_neon_Float16(dst []hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Mul(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vs1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vd1.Mul(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vd2.Mul(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vs3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vd3.Mul(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseMul_fallback_Float16(dst[i:n], s[i:n])
	}
}

func BaseMul_neon_BFloat16(dst []hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Mul(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vs1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vd1.Mul(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vd2.Mul(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vs3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vd3.Mul(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseMul_fallback_BFloat16(dst[i:n], s[i:n])
	}
}

func BaseMul_neon(dst []float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i])))
		vs := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i])))
		result := vd.Mul(vs)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vs1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+4])))
		result1 := vd1.Mul(vs1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vd2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vs2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+8])))
		result2 := vd2.Mul(vs2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vd3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+12])))
		vs3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+12])))
		result3 := vd3.Mul(vs3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	if i < n {
		BaseMul_fallback(dst[i:n], s[i:n])
	}
}

func BaseMul_neon_Float64(dst []float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i])))
		vs := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i])))
		result := vd.Mul(vs)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vs1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+2])))
		result1 := vd1.Mul(vs1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vd2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vs2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+4])))
		result2 := vd2.Mul(vs2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vd3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+6])))
		vs3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+6])))
		result3 := vd3.Mul(vs3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	if i < n {
		BaseMul_fallback_Float64(dst[i:n], s[i:n])
	}
}

func BaseMulTo_neon_Float16(dst []hwy.Float16, a []hwy.Float16, b []hwy.Float16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Mul(vb)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		va1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+8:][0]))
		vb1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+8:][0]))
		result1 := va1.Mul(vb1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		va2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+16:][0]))
		result2 := va2.Mul(vb2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		va3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+24:][0]))
		vb3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+24:][0]))
		result3 := va3.Mul(vb3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseMulTo_fallback_Float16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseMulTo_neon_BFloat16(dst []hwy.BFloat16, a []hwy.BFloat16, b []hwy.BFloat16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Mul(vb)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		va1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+8:][0]))
		vb1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+8:][0]))
		result1 := va1.Mul(vb1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		va2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+16:][0]))
		result2 := va2.Mul(vb2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		va3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+24:][0]))
		vb3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+24:][0]))
		result3 := va3.Mul(vb3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseMulTo_fallback_BFloat16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseMulTo_neon(dst []float32, a []float32, b []float32) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i])))
		vb := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i])))
		result := va.Mul(vb)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		va1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+4])))
		vb1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+4])))
		result1 := va1.Mul(vb1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		va2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+8])))
		vb2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+8])))
		result2 := va2.Mul(vb2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		va3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+12])))
		vb3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+12])))
		result3 := va3.Mul(vb3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	if i < n {
		BaseMulTo_fallback(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseMulTo_neon_Float64(dst []float64, a []float64, b []float64) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i])))
		vb := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i])))
		result := va.Mul(vb)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		va1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+2])))
		vb1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+2])))
		result1 := va1.Mul(vb1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		va2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+4])))
		vb2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+4])))
		result2 := va2.Mul(vb2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		va3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+6])))
		vb3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+6])))
		result3 := va3.Mul(vb3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	if i < n {
		BaseMulTo_fallback_Float64(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseDiv_neon_Float16(dst []hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Div(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vs1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vd1.Div(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vd2.Div(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vs3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vd3.Div(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseDiv_fallback_Float16(dst[i:n], s[i:n])
	}
}

func BaseDiv_neon_BFloat16(dst []hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vs := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vd.Div(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vs1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vd1.Div(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vs2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vd2.Div(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vs3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vd3.Div(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseDiv_fallback_BFloat16(dst[i:n], s[i:n])
	}
}

func BaseDiv_neon(dst []float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i])))
		vs := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i])))
		result := vd.Div(vs)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vs1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+4])))
		result1 := vd1.Div(vs1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vd2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vs2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+8])))
		result2 := vd2.Div(vs2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vd3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+12])))
		vs3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+12])))
		result3 := vd3.Div(vs3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	if i < n {
		BaseDiv_fallback(dst[i:n], s[i:n])
	}
}

func BaseDiv_neon_Float64(dst []float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i])))
		vs := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i])))
		result := vd.Div(vs)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vs1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+2])))
		result1 := vd1.Div(vs1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vd2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vs2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+4])))
		result2 := vd2.Div(vs2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vd3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+6])))
		vs3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+6])))
		result3 := vd3.Div(vs3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	if i < n {
		BaseDiv_fallback_Float64(dst[i:n], s[i:n])
	}
}

func BaseDivTo_neon_Float16(dst []hwy.Float16, a []hwy.Float16, b []hwy.Float16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Div(vb)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		va1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+8:][0]))
		vb1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+8:][0]))
		result1 := va1.Div(vb1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		va2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+16:][0]))
		result2 := va2.Div(vb2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		va3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&a[i+24:][0]))
		vb3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&b[i+24:][0]))
		result3 := va3.Div(vb3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseDivTo_fallback_Float16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseDivTo_neon_BFloat16(dst []hwy.BFloat16, a []hwy.BFloat16, b []hwy.BFloat16) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i:][0]))
		vb := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i:][0]))
		result := va.Div(vb)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		va1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+8:][0]))
		vb1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+8:][0]))
		result1 := va1.Div(vb1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		va2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+16:][0]))
		vb2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+16:][0]))
		result2 := va2.Div(vb2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		va3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&a[i+24:][0]))
		vb3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&b[i+24:][0]))
		result3 := va3.Div(vb3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseDivTo_fallback_BFloat16(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseDivTo_neon(dst []float32, a []float32, b []float32) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i])))
		vb := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i])))
		result := va.Div(vb)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		va1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+4])))
		vb1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+4])))
		result1 := va1.Div(vb1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		va2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+8])))
		vb2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+8])))
		result2 := va2.Div(vb2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		va3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&a[i+12])))
		vb3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&b[i+12])))
		result3 := va3.Div(vb3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	if i < n {
		BaseDivTo_fallback(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseDivTo_neon_Float64(dst []float64, a []float64, b []float64) {
	if len(dst) == 0 || len(a) == 0 || len(b) == 0 {
		return
	}
	n := min(len(dst), min(len(a), len(b)))
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		va := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i])))
		vb := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i])))
		result := va.Div(vb)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		va1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+2])))
		vb1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+2])))
		result1 := va1.Div(vb1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		va2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+4])))
		vb2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+4])))
		result2 := va2.Div(vb2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		va3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&a[i+6])))
		vb3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&b[i+6])))
		result3 := va3.Div(vb3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	if i < n {
		BaseDivTo_fallback_Float64(dst[i:n], a[i:n], b[i:n])
	}
}

func BaseScale_neon_Float16(c hwy.Float16, dst []hwy.Float16) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastFloat16x8(uint16(c))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		result := vd.Mul(vc)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		result1 := vd1.Mul(vc)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		result2 := vd2.Mul(vc)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		result3 := vd3.Mul(vc)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseScale_fallback_Float16(c, dst[i:n])
	}
}

func BaseScale_neon_BFloat16(c hwy.BFloat16, dst []hwy.BFloat16) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastBFloat16x8(uint16(c))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		result := vd.Mul(vc)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		result1 := vd1.Mul(vc)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		result2 := vd2.Mul(vc)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		result3 := vd3.Mul(vc)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseScale_fallback_BFloat16(c, dst[i:n])
	}
}

func BaseScale_neon(c float32, dst []float32) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastFloat32x4(c)
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i])))
		result := vd.Mul(vc)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		result1 := vd1.Mul(vc)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vd2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		result2 := vd2.Mul(vc)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vd3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+12])))
		result3 := vd3.Mul(vc)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	for ; i < n; i++ {
		dst[i] *= c
	}
}

func BaseScale_neon_Float64(c float64, dst []float64) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastFloat64x2(c)
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i])))
		result := vd.Mul(vc)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		result1 := vd1.Mul(vc)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vd2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		result2 := vd2.Mul(vc)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vd3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+6])))
		result3 := vd3.Mul(vc)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	for ; i < n; i++ {
		dst[i] *= c
	}
}

func BaseScaleTo_neon_Float16(dst []hwy.Float16, c hwy.Float16, s []hwy.Float16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	vc := asm.BroadcastFloat16x8(uint16(c))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vs := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vc.Mul(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vs1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vc.Mul(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vs2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vc.Mul(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vs3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vc.Mul(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseScaleTo_fallback_Float16(dst[i:n], c, s[i:n])
	}
}

func BaseScaleTo_neon_BFloat16(dst []hwy.BFloat16, c hwy.BFloat16, s []hwy.BFloat16) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	vc := asm.BroadcastBFloat16x8(uint16(c))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vs := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i:][0]))
		result := vc.Mul(vs)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vs1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+8:][0]))
		result1 := vc.Mul(vs1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vs2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+16:][0]))
		result2 := vc.Mul(vs2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vs3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&s[i+24:][0]))
		result3 := vc.Mul(vs3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseScaleTo_fallback_BFloat16(dst[i:n], c, s[i:n])
	}
}

func BaseScaleTo_neon(dst []float32, c float32, s []float32) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	vc := asm.BroadcastFloat32x4(c)
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vs := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i])))
		result := vc.Mul(vs)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		vs1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+4])))
		result1 := vc.Mul(vs1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vs2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+8])))
		result2 := vc.Mul(vs2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vs3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&s[i+12])))
		result3 := vc.Mul(vs3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	for ; i < n; i++ {
		dst[i] = c * s[i]
	}
}

func BaseScaleTo_neon_Float64(dst []float64, c float64, s []float64) {
	if len(dst) == 0 || len(s) == 0 {
		return
	}
	n := min(len(dst), len(s))
	vc := asm.BroadcastFloat64x2(c)
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vs := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i])))
		result := vc.Mul(vs)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		vs1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+2])))
		result1 := vc.Mul(vs1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vs2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+4])))
		result2 := vc.Mul(vs2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vs3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&s[i+6])))
		result3 := vc.Mul(vs3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	for ; i < n; i++ {
		dst[i] = c * s[i]
	}
}

func BaseAddConst_neon_Float16(c hwy.Float16, dst []hwy.Float16) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastFloat16x8(uint16(c))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		result := vd.Add(vc)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		result1 := vd1.Add(vc)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		result2 := vd2.Add(vc)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		result3 := vd3.Add(vc)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseAddConst_fallback_Float16(c, dst[i:n])
	}
}

func BaseAddConst_neon_BFloat16(c hwy.BFloat16, dst []hwy.BFloat16) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastBFloat16x8(uint16(c))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		result := vd.Add(vc)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		result1 := vd1.Add(vc)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		result2 := vd2.Add(vc)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		result3 := vd3.Add(vc)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseAddConst_fallback_BFloat16(c, dst[i:n])
	}
}

func BaseAddConst_neon(c float32, dst []float32) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastFloat32x4(c)
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i])))
		result := vd.Add(vc)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		result1 := vd1.Add(vc)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vd2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		result2 := vd2.Add(vc)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vd3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+12])))
		result3 := vd3.Add(vc)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	for ; i < n; i++ {
		dst[i] += c
	}
}

func BaseAddConst_neon_Float64(c float64, dst []float64) {
	if len(dst) == 0 {
		return
	}
	n := len(dst)
	vc := asm.BroadcastFloat64x2(c)
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i])))
		result := vd.Add(vc)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		result1 := vd1.Add(vc)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vd2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		result2 := vd2.Add(vc)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vd3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+6])))
		result3 := vd3.Add(vc)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	for ; i < n; i++ {
		dst[i] += c
	}
}

func BaseMulConstAddTo_neon_Float16(dst []hwy.Float16, a hwy.Float16, x []hwy.Float16) {
	if len(dst) == 0 || len(x) == 0 {
		return
	}
	n := min(len(dst), len(x))
	va := asm.BroadcastFloat16x8(uint16(a))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vx := asm.LoadFloat16x8Ptr(unsafe.Pointer(&x[i:][0]))
		result := va.MulAdd(vx, vd)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vx1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&x[i+8:][0]))
		result1 := va.MulAdd(vx1, vd1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vx2 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&x[i+16:][0]))
		result2 := va.MulAdd(vx2, vd2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vx3 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&x[i+24:][0]))
		result3 := va.MulAdd(vx3, vd3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseMulConstAddTo_fallback_Float16(dst[i:n], a, x[i:n])
	}
}

func BaseMulConstAddTo_neon_BFloat16(dst []hwy.BFloat16, a hwy.BFloat16, x []hwy.BFloat16) {
	if len(dst) == 0 || len(x) == 0 {
		return
	}
	n := min(len(dst), len(x))
	va := asm.BroadcastBFloat16x8(uint16(a))
	lanes := 8
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i:][0]))
		vx := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&x[i:][0]))
		result := va.MulAdd(vx, vd)
		result.StorePtr(unsafe.Pointer(&dst[i:][0]))
		vd1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+8:][0]))
		vx1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&x[i+8:][0]))
		result1 := va.MulAdd(vx1, vd1)
		result1.StorePtr(unsafe.Pointer(&dst[i+8:][0]))
		vd2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+16:][0]))
		vx2 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&x[i+16:][0]))
		result2 := va.MulAdd(vx2, vd2)
		result2.StorePtr(unsafe.Pointer(&dst[i+16:][0]))
		vd3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&dst[i+24:][0]))
		vx3 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&x[i+24:][0]))
		result3 := va.MulAdd(vx3, vd3)
		result3.StorePtr(unsafe.Pointer(&dst[i+24:][0]))
	}
	if i < n {
		BaseMulConstAddTo_fallback_BFloat16(dst[i:n], a, x[i:n])
	}
}

func BaseMulConstAddTo_neon(dst []float32, a float32, x []float32) {
	if len(dst) == 0 || len(x) == 0 {
		return
	}
	n := min(len(dst), len(x))
	va := asm.BroadcastFloat32x4(a)
	lanes := 4
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i])))
		vx := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&x[i])))
		result := va.MulAdd(vx, vd)
		result.Store((*[4]float32)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vx1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&x[i+4])))
		result1 := va.MulAdd(vx1, vd1)
		result1.Store((*[4]float32)(unsafe.Pointer(&dst[i+4])))
		vd2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vx2 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&x[i+8])))
		result2 := va.MulAdd(vx2, vd2)
		result2.Store((*[4]float32)(unsafe.Pointer(&dst[i+8])))
		vd3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&dst[i+12])))
		vx3 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&x[i+12])))
		result3 := va.MulAdd(vx3, vd3)
		result3.Store((*[4]float32)(unsafe.Pointer(&dst[i+12])))
	}
	for ; i < n; i++ {
		dst[i] += a * x[i]
	}
}

func BaseMulConstAddTo_neon_Float64(dst []float64, a float64, x []float64) {
	if len(dst) == 0 || len(x) == 0 {
		return
	}
	n := min(len(dst), len(x))
	va := asm.BroadcastFloat64x2(a)
	lanes := 2
	var i int
	i = 0
	for ; i+lanes*4 <= n; i += lanes * 4 {
		vd := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i])))
		vx := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&x[i])))
		result := va.MulAdd(vx, vd)
		result.Store((*[2]float64)(unsafe.Pointer(&dst[i])))
		vd1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vx1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&x[i+2])))
		result1 := va.MulAdd(vx1, vd1)
		result1.Store((*[2]float64)(unsafe.Pointer(&dst[i+2])))
		vd2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vx2 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&x[i+4])))
		result2 := va.MulAdd(vx2, vd2)
		result2.Store((*[2]float64)(unsafe.Pointer(&dst[i+4])))
		vd3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&dst[i+6])))
		vx3 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&x[i+6])))
		result3 := va.MulAdd(vx3, vd3)
		result3.Store((*[2]float64)(unsafe.Pointer(&dst[i+6])))
	}
	for ; i < n; i++ {
		dst[i] += a * x[i]
	}
}
