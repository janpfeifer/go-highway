// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package matmul

import (
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
)

func BaseTranspose2DStrided_avx2_Float16(src []hwy.Float16, rowStart int, rowEnd int, k int, dstM int, dst []hwy.Float16) {
	if rowStart >= rowEnd {
		return
	}
	m := rowEnd - rowStart
	lanes := 8
	i := rowStart
	for ; i <= rowEnd-lanes; i += lanes * 2 {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]asm.Float16x8AVX2{}
				for r_1 := range lanes {
					rows_1[r_1] = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j:][0]))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]asm.Float16x8AVX2{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = rows_1[i_1+j_1].InterleaveLower(rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = rows_1[i_1+j_1].InterleaveUpper(rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].StorePtr(unsafe.Pointer(&dst[(j+c_1)*dstM+i:][0]))
				}
			}
		}
		for j1 := 0; j1 <= k-lanes; j1 += lanes {
			{
				rows_11 := [8]asm.Float16x8AVX2{}
				for r_1 := range lanes {
					rows_11[r_1] = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j1:][0]))
				}
				for level_11 := 0; (1 << level_11) < lanes; level_11++ {
					stride_11 := 1 << level_11
					newRows_11 := [8]asm.Float16x8AVX2{}
					for i_11 := 0; i_11 < lanes; i_11 += 2 * stride_11 {
						for j_1 := range stride_11 {
							newRows_11[i_11+j_1] = rows_11[i_11+j_1].InterleaveLower(rows_11[i_11+j_1+stride_11])
							newRows_11[i_11+j_1+stride_11] = rows_11[i_11+j_1].InterleaveUpper(rows_11[i_11+j_1+stride_11])
						}
					}
					rows_11 = newRows_11
				}
				for c_1 := range lanes {
					rows_11[c_1].StorePtr(unsafe.Pointer(&dst[(j1+c_1)*dstM+i:][0]))
				}
			}
		}
	}
	for ; i <= rowEnd-lanes; i += lanes {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]asm.Float16x8AVX2{}
				for r_1 := range lanes {
					rows_1[r_1] = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j:][0]))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]asm.Float16x8AVX2{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = rows_1[i_1+j_1].InterleaveLower(rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = rows_1[i_1+j_1].InterleaveUpper(rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].StorePtr(unsafe.Pointer(&dst[(j+c_1)*dstM+i:][0]))
				}
			}
		}
	}
	{
		blockRowStart_2 := ((rowStart + lanes - 1) / lanes) * lanes
		blockRowEnd_2 := (rowEnd / lanes) * lanes
		blockK_2 := (k / lanes) * lanes
		for i_2 := rowStart; i_2 < rowEnd; i_2++ {
			for j_2 := blockK_2; j_2 < k; j_2++ {
				dst[j_2*dstM+i_2] = hwy.Float32ToFloat16(src[i_2*k+j_2].Float32())
			}
		}
		for i_2 := rowStart; i_2 < min(blockRowStart_2, rowEnd); i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*dstM+i_2] = hwy.Float32ToFloat16(src[i_2*k+j_2].Float32())
			}
		}
		for i_2 := max(blockRowEnd_2, rowStart); i_2 < rowEnd; i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*dstM+i_2] = hwy.Float32ToFloat16(src[i_2*k+j_2].Float32())
			}
		}
	}
	_ = m
}

func BaseTranspose2DStrided_avx2_BFloat16(src []hwy.BFloat16, rowStart int, rowEnd int, k int, dstM int, dst []hwy.BFloat16) {
	if rowStart >= rowEnd {
		return
	}
	m := rowEnd - rowStart
	lanes := 8
	i := rowStart
	for ; i <= rowEnd-lanes; i += lanes * 2 {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]asm.BFloat16x8AVX2{}
				for r_1 := range lanes {
					rows_1[r_1] = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j:][0]))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]asm.BFloat16x8AVX2{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = rows_1[i_1+j_1].InterleaveLower(rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = rows_1[i_1+j_1].InterleaveUpper(rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].StorePtr(unsafe.Pointer(&dst[(j+c_1)*dstM+i:][0]))
				}
			}
		}
		for j1 := 0; j1 <= k-lanes; j1 += lanes {
			{
				rows_11 := [8]asm.BFloat16x8AVX2{}
				for r_1 := range lanes {
					rows_11[r_1] = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j1:][0]))
				}
				for level_11 := 0; (1 << level_11) < lanes; level_11++ {
					stride_11 := 1 << level_11
					newRows_11 := [8]asm.BFloat16x8AVX2{}
					for i_11 := 0; i_11 < lanes; i_11 += 2 * stride_11 {
						for j_1 := range stride_11 {
							newRows_11[i_11+j_1] = rows_11[i_11+j_1].InterleaveLower(rows_11[i_11+j_1+stride_11])
							newRows_11[i_11+j_1+stride_11] = rows_11[i_11+j_1].InterleaveUpper(rows_11[i_11+j_1+stride_11])
						}
					}
					rows_11 = newRows_11
				}
				for c_1 := range lanes {
					rows_11[c_1].StorePtr(unsafe.Pointer(&dst[(j1+c_1)*dstM+i:][0]))
				}
			}
		}
	}
	for ; i <= rowEnd-lanes; i += lanes {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]asm.BFloat16x8AVX2{}
				for r_1 := range lanes {
					rows_1[r_1] = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j:][0]))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]asm.BFloat16x8AVX2{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = rows_1[i_1+j_1].InterleaveLower(rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = rows_1[i_1+j_1].InterleaveUpper(rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].StorePtr(unsafe.Pointer(&dst[(j+c_1)*dstM+i:][0]))
				}
			}
		}
	}
	{
		blockRowStart_2 := ((rowStart + lanes - 1) / lanes) * lanes
		blockRowEnd_2 := (rowEnd / lanes) * lanes
		blockK_2 := (k / lanes) * lanes
		for i_2 := rowStart; i_2 < rowEnd; i_2++ {
			for j_2 := blockK_2; j_2 < k; j_2++ {
				dst[j_2*dstM+i_2] = hwy.Float32ToBFloat16(src[i_2*k+j_2].Float32())
			}
		}
		for i_2 := rowStart; i_2 < min(blockRowStart_2, rowEnd); i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*dstM+i_2] = hwy.Float32ToBFloat16(src[i_2*k+j_2].Float32())
			}
		}
		for i_2 := max(blockRowEnd_2, rowStart); i_2 < rowEnd; i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*dstM+i_2] = hwy.Float32ToBFloat16(src[i_2*k+j_2].Float32())
			}
		}
	}
	_ = m
}

func BaseTranspose2DStrided_avx2(src []float32, rowStart int, rowEnd int, k int, dstM int, dst []float32) {
	if rowStart >= rowEnd {
		return
	}
	m := rowEnd - rowStart
	lanes := 8
	i := rowStart
	for ; i <= rowEnd-lanes; i += lanes * 2 {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]archsimd.Float32x8{}
				for r_1 := range lanes {
					rows_1[r_1] = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[(i+r_1)*k+j])))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]archsimd.Float32x8{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = hwy.InterleaveLower_AVX2_F32x8(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = hwy.InterleaveUpper_AVX2_F32x8(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].Store((*[8]float32)(unsafe.Pointer(&dst[(j+c_1)*dstM+i])))
				}
			}
		}
		for j1 := 0; j1 <= k-lanes; j1 += lanes {
			{
				rows_11 := [8]archsimd.Float32x8{}
				for r_1 := range lanes {
					rows_11[r_1] = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[(i+r_1)*k+j1])))
				}
				for level_11 := 0; (1 << level_11) < lanes; level_11++ {
					stride_11 := 1 << level_11
					newRows_11 := [8]archsimd.Float32x8{}
					for i_11 := 0; i_11 < lanes; i_11 += 2 * stride_11 {
						for j_1 := range stride_11 {
							newRows_11[i_11+j_1] = hwy.InterleaveLower_AVX2_F32x8(rows_11[i_11+j_1], rows_11[i_11+j_1+stride_11])
							newRows_11[i_11+j_1+stride_11] = hwy.InterleaveUpper_AVX2_F32x8(rows_11[i_11+j_1], rows_11[i_11+j_1+stride_11])
						}
					}
					rows_11 = newRows_11
				}
				for c_1 := range lanes {
					rows_11[c_1].Store((*[8]float32)(unsafe.Pointer(&dst[(j1+c_1)*dstM+i])))
				}
			}
		}
	}
	for ; i <= rowEnd-lanes; i += lanes {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]archsimd.Float32x8{}
				for r_1 := range lanes {
					rows_1[r_1] = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[(i+r_1)*k+j])))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]archsimd.Float32x8{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = hwy.InterleaveLower_AVX2_F32x8(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = hwy.InterleaveUpper_AVX2_F32x8(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].Store((*[8]float32)(unsafe.Pointer(&dst[(j+c_1)*dstM+i])))
				}
			}
		}
	}
	{
		blockRowStart_2 := ((rowStart + lanes - 1) / lanes) * lanes
		blockRowEnd_2 := (rowEnd / lanes) * lanes
		blockK_2 := (k / lanes) * lanes
		for i_2 := rowStart; i_2 < rowEnd; i_2++ {
			for j_2 := blockK_2; j_2 < k; j_2++ {
				dst[j_2*dstM+i_2] = src[i_2*k+j_2]
			}
		}
		for i_2 := rowStart; i_2 < min(blockRowStart_2, rowEnd); i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*dstM+i_2] = src[i_2*k+j_2]
			}
		}
		for i_2 := max(blockRowEnd_2, rowStart); i_2 < rowEnd; i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*dstM+i_2] = src[i_2*k+j_2]
			}
		}
	}
	_ = m
}

func BaseTranspose2DStrided_avx2_Float64(src []float64, rowStart int, rowEnd int, k int, dstM int, dst []float64) {
	if rowStart >= rowEnd {
		return
	}
	m := rowEnd - rowStart
	lanes := 4
	i := rowStart
	for ; i <= rowEnd-lanes; i += lanes * 2 {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [4]archsimd.Float64x4{}
				for r_1 := range lanes {
					rows_1[r_1] = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[(i+r_1)*k+j])))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [4]archsimd.Float64x4{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = hwy.InterleaveLower_AVX2_F64x4(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = hwy.InterleaveUpper_AVX2_F64x4(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].Store((*[4]float64)(unsafe.Pointer(&dst[(j+c_1)*dstM+i])))
				}
			}
		}
		for j1 := 0; j1 <= k-lanes; j1 += lanes {
			{
				rows_11 := [4]archsimd.Float64x4{}
				for r_1 := range lanes {
					rows_11[r_1] = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[(i+r_1)*k+j1])))
				}
				for level_11 := 0; (1 << level_11) < lanes; level_11++ {
					stride_11 := 1 << level_11
					newRows_11 := [4]archsimd.Float64x4{}
					for i_11 := 0; i_11 < lanes; i_11 += 2 * stride_11 {
						for j_1 := range stride_11 {
							newRows_11[i_11+j_1] = hwy.InterleaveLower_AVX2_F64x4(rows_11[i_11+j_1], rows_11[i_11+j_1+stride_11])
							newRows_11[i_11+j_1+stride_11] = hwy.InterleaveUpper_AVX2_F64x4(rows_11[i_11+j_1], rows_11[i_11+j_1+stride_11])
						}
					}
					rows_11 = newRows_11
				}
				for c_1 := range lanes {
					rows_11[c_1].Store((*[4]float64)(unsafe.Pointer(&dst[(j1+c_1)*dstM+i])))
				}
			}
		}
	}
	for ; i <= rowEnd-lanes; i += lanes {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [4]archsimd.Float64x4{}
				for r_1 := range lanes {
					rows_1[r_1] = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[(i+r_1)*k+j])))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [4]archsimd.Float64x4{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = hwy.InterleaveLower_AVX2_F64x4(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = hwy.InterleaveUpper_AVX2_F64x4(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].Store((*[4]float64)(unsafe.Pointer(&dst[(j+c_1)*dstM+i])))
				}
			}
		}
	}
	{
		blockRowStart_2 := ((rowStart + lanes - 1) / lanes) * lanes
		blockRowEnd_2 := (rowEnd / lanes) * lanes
		blockK_2 := (k / lanes) * lanes
		for i_2 := rowStart; i_2 < rowEnd; i_2++ {
			for j_2 := blockK_2; j_2 < k; j_2++ {
				dst[j_2*dstM+i_2] = src[i_2*k+j_2]
			}
		}
		for i_2 := rowStart; i_2 < min(blockRowStart_2, rowEnd); i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*dstM+i_2] = src[i_2*k+j_2]
			}
		}
		for i_2 := max(blockRowEnd_2, rowStart); i_2 < rowEnd; i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*dstM+i_2] = src[i_2*k+j_2]
			}
		}
	}
	_ = m
}

func BaseTranspose2D_avx2_Float16(src []hwy.Float16, m int, k int, dst []hwy.Float16) {
	if len(src) < m*k || len(dst) < k*m {
		return
	}
	lanes := 8
	i := 0
	for ; i <= m-lanes; i += lanes * 2 {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]asm.Float16x8AVX2{}
				for r_1 := range lanes {
					rows_1[r_1] = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j:][0]))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]asm.Float16x8AVX2{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = rows_1[i_1+j_1].InterleaveLower(rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = rows_1[i_1+j_1].InterleaveUpper(rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].StorePtr(unsafe.Pointer(&dst[(j+c_1)*m+i:][0]))
				}
			}
		}
		for j1 := 0; j1 <= k-lanes; j1 += lanes {
			{
				rows_11 := [8]asm.Float16x8AVX2{}
				for r_1 := range lanes {
					rows_11[r_1] = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j1:][0]))
				}
				for level_11 := 0; (1 << level_11) < lanes; level_11++ {
					stride_11 := 1 << level_11
					newRows_11 := [8]asm.Float16x8AVX2{}
					for i_11 := 0; i_11 < lanes; i_11 += 2 * stride_11 {
						for j_1 := range stride_11 {
							newRows_11[i_11+j_1] = rows_11[i_11+j_1].InterleaveLower(rows_11[i_11+j_1+stride_11])
							newRows_11[i_11+j_1+stride_11] = rows_11[i_11+j_1].InterleaveUpper(rows_11[i_11+j_1+stride_11])
						}
					}
					rows_11 = newRows_11
				}
				for c_1 := range lanes {
					rows_11[c_1].StorePtr(unsafe.Pointer(&dst[(j1+c_1)*m+i:][0]))
				}
			}
		}
	}
	for ; i <= m-lanes; i += lanes {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]asm.Float16x8AVX2{}
				for r_1 := range lanes {
					rows_1[r_1] = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j:][0]))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]asm.Float16x8AVX2{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = rows_1[i_1+j_1].InterleaveLower(rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = rows_1[i_1+j_1].InterleaveUpper(rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].StorePtr(unsafe.Pointer(&dst[(j+c_1)*m+i:][0]))
				}
			}
		}
	}
	{
		blockM_2 := (m / lanes) * lanes
		blockK_2 := (k / lanes) * lanes
		for i_2 := range m {
			for j_2 := blockK_2; j_2 < k; j_2++ {
				dst[j_2*m+i_2] = hwy.Float32ToFloat16(src[i_2*k+j_2].Float32())
			}
		}
		for i_2 := blockM_2; i_2 < m; i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*m+i_2] = hwy.Float32ToFloat16(src[i_2*k+j_2].Float32())
			}
		}
	}
}

func BaseTranspose2D_avx2_BFloat16(src []hwy.BFloat16, m int, k int, dst []hwy.BFloat16) {
	if len(src) < m*k || len(dst) < k*m {
		return
	}
	lanes := 8
	i := 0
	for ; i <= m-lanes; i += lanes * 2 {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]asm.BFloat16x8AVX2{}
				for r_1 := range lanes {
					rows_1[r_1] = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j:][0]))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]asm.BFloat16x8AVX2{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = rows_1[i_1+j_1].InterleaveLower(rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = rows_1[i_1+j_1].InterleaveUpper(rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].StorePtr(unsafe.Pointer(&dst[(j+c_1)*m+i:][0]))
				}
			}
		}
		for j1 := 0; j1 <= k-lanes; j1 += lanes {
			{
				rows_11 := [8]asm.BFloat16x8AVX2{}
				for r_1 := range lanes {
					rows_11[r_1] = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j1:][0]))
				}
				for level_11 := 0; (1 << level_11) < lanes; level_11++ {
					stride_11 := 1 << level_11
					newRows_11 := [8]asm.BFloat16x8AVX2{}
					for i_11 := 0; i_11 < lanes; i_11 += 2 * stride_11 {
						for j_1 := range stride_11 {
							newRows_11[i_11+j_1] = rows_11[i_11+j_1].InterleaveLower(rows_11[i_11+j_1+stride_11])
							newRows_11[i_11+j_1+stride_11] = rows_11[i_11+j_1].InterleaveUpper(rows_11[i_11+j_1+stride_11])
						}
					}
					rows_11 = newRows_11
				}
				for c_1 := range lanes {
					rows_11[c_1].StorePtr(unsafe.Pointer(&dst[(j1+c_1)*m+i:][0]))
				}
			}
		}
	}
	for ; i <= m-lanes; i += lanes {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]asm.BFloat16x8AVX2{}
				for r_1 := range lanes {
					rows_1[r_1] = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[(i+r_1)*k+j:][0]))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]asm.BFloat16x8AVX2{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = rows_1[i_1+j_1].InterleaveLower(rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = rows_1[i_1+j_1].InterleaveUpper(rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].StorePtr(unsafe.Pointer(&dst[(j+c_1)*m+i:][0]))
				}
			}
		}
	}
	{
		blockM_2 := (m / lanes) * lanes
		blockK_2 := (k / lanes) * lanes
		for i_2 := range m {
			for j_2 := blockK_2; j_2 < k; j_2++ {
				dst[j_2*m+i_2] = hwy.Float32ToBFloat16(src[i_2*k+j_2].Float32())
			}
		}
		for i_2 := blockM_2; i_2 < m; i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*m+i_2] = hwy.Float32ToBFloat16(src[i_2*k+j_2].Float32())
			}
		}
	}
}

func BaseTranspose2D_avx2(src []float32, m int, k int, dst []float32) {
	if len(src) < m*k || len(dst) < k*m {
		return
	}
	lanes := 8
	i := 0
	for ; i <= m-lanes; i += lanes * 2 {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]archsimd.Float32x8{}
				for r_1 := range lanes {
					rows_1[r_1] = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[(i+r_1)*k+j])))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]archsimd.Float32x8{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = hwy.InterleaveLower_AVX2_F32x8(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = hwy.InterleaveUpper_AVX2_F32x8(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].Store((*[8]float32)(unsafe.Pointer(&dst[(j+c_1)*m+i])))
				}
			}
		}
		for j1 := 0; j1 <= k-lanes; j1 += lanes {
			{
				rows_11 := [8]archsimd.Float32x8{}
				for r_1 := range lanes {
					rows_11[r_1] = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[(i+r_1)*k+j1])))
				}
				for level_11 := 0; (1 << level_11) < lanes; level_11++ {
					stride_11 := 1 << level_11
					newRows_11 := [8]archsimd.Float32x8{}
					for i_11 := 0; i_11 < lanes; i_11 += 2 * stride_11 {
						for j_1 := range stride_11 {
							newRows_11[i_11+j_1] = hwy.InterleaveLower_AVX2_F32x8(rows_11[i_11+j_1], rows_11[i_11+j_1+stride_11])
							newRows_11[i_11+j_1+stride_11] = hwy.InterleaveUpper_AVX2_F32x8(rows_11[i_11+j_1], rows_11[i_11+j_1+stride_11])
						}
					}
					rows_11 = newRows_11
				}
				for c_1 := range lanes {
					rows_11[c_1].Store((*[8]float32)(unsafe.Pointer(&dst[(j1+c_1)*m+i])))
				}
			}
		}
	}
	for ; i <= m-lanes; i += lanes {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [8]archsimd.Float32x8{}
				for r_1 := range lanes {
					rows_1[r_1] = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[(i+r_1)*k+j])))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [8]archsimd.Float32x8{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = hwy.InterleaveLower_AVX2_F32x8(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = hwy.InterleaveUpper_AVX2_F32x8(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].Store((*[8]float32)(unsafe.Pointer(&dst[(j+c_1)*m+i])))
				}
			}
		}
	}
	{
		blockM_2 := (m / lanes) * lanes
		blockK_2 := (k / lanes) * lanes
		for i_2 := range m {
			for j_2 := blockK_2; j_2 < k; j_2++ {
				dst[j_2*m+i_2] = src[i_2*k+j_2]
			}
		}
		for i_2 := blockM_2; i_2 < m; i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*m+i_2] = src[i_2*k+j_2]
			}
		}
	}
}

func BaseTranspose2D_avx2_Float64(src []float64, m int, k int, dst []float64) {
	if len(src) < m*k || len(dst) < k*m {
		return
	}
	lanes := 4
	i := 0
	for ; i <= m-lanes; i += lanes * 2 {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [4]archsimd.Float64x4{}
				for r_1 := range lanes {
					rows_1[r_1] = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[(i+r_1)*k+j])))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [4]archsimd.Float64x4{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = hwy.InterleaveLower_AVX2_F64x4(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = hwy.InterleaveUpper_AVX2_F64x4(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].Store((*[4]float64)(unsafe.Pointer(&dst[(j+c_1)*m+i])))
				}
			}
		}
		for j1 := 0; j1 <= k-lanes; j1 += lanes {
			{
				rows_11 := [4]archsimd.Float64x4{}
				for r_1 := range lanes {
					rows_11[r_1] = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[(i+r_1)*k+j1])))
				}
				for level_11 := 0; (1 << level_11) < lanes; level_11++ {
					stride_11 := 1 << level_11
					newRows_11 := [4]archsimd.Float64x4{}
					for i_11 := 0; i_11 < lanes; i_11 += 2 * stride_11 {
						for j_1 := range stride_11 {
							newRows_11[i_11+j_1] = hwy.InterleaveLower_AVX2_F64x4(rows_11[i_11+j_1], rows_11[i_11+j_1+stride_11])
							newRows_11[i_11+j_1+stride_11] = hwy.InterleaveUpper_AVX2_F64x4(rows_11[i_11+j_1], rows_11[i_11+j_1+stride_11])
						}
					}
					rows_11 = newRows_11
				}
				for c_1 := range lanes {
					rows_11[c_1].Store((*[4]float64)(unsafe.Pointer(&dst[(j1+c_1)*m+i])))
				}
			}
		}
	}
	for ; i <= m-lanes; i += lanes {
		for j := 0; j <= k-lanes; j += lanes {
			{
				rows_1 := [4]archsimd.Float64x4{}
				for r_1 := range lanes {
					rows_1[r_1] = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[(i+r_1)*k+j])))
				}
				for level_1 := 0; (1 << level_1) < lanes; level_1++ {
					stride_1 := 1 << level_1
					newRows_1 := [4]archsimd.Float64x4{}
					for i_1 := 0; i_1 < lanes; i_1 += 2 * stride_1 {
						for j_1 := range stride_1 {
							newRows_1[i_1+j_1] = hwy.InterleaveLower_AVX2_F64x4(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
							newRows_1[i_1+j_1+stride_1] = hwy.InterleaveUpper_AVX2_F64x4(rows_1[i_1+j_1], rows_1[i_1+j_1+stride_1])
						}
					}
					rows_1 = newRows_1
				}
				for c_1 := range lanes {
					rows_1[c_1].Store((*[4]float64)(unsafe.Pointer(&dst[(j+c_1)*m+i])))
				}
			}
		}
	}
	{
		blockM_2 := (m / lanes) * lanes
		blockK_2 := (k / lanes) * lanes
		for i_2 := range m {
			for j_2 := blockK_2; j_2 < k; j_2++ {
				dst[j_2*m+i_2] = src[i_2*k+j_2]
			}
		}
		for i_2 := blockM_2; i_2 < m; i_2++ {
			for j_2 := range blockK_2 {
				dst[j_2*m+i_2] = src[i_2*k+j_2]
			}
		}
	}
}
