// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package matmul

import (
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
)

func BasePackedMicroKernel_avx512_Float16(packedA []hwy.Float16, packedB []hwy.Float16, c []hwy.Float16, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 16
	if mr != 4 || nr != lanes*2 {
		basePackedMicroKernelGeneral(packedA, packedB, c, n, ir, jr, kc, mr, nr)
		return
	}
	acc00 := asm.ZeroFloat16x16AVX512()
	acc01 := asm.ZeroFloat16x16AVX512()
	acc10 := asm.ZeroFloat16x16AVX512()
	acc11 := asm.ZeroFloat16x16AVX512()
	acc20 := asm.ZeroFloat16x16AVX512()
	acc21 := asm.ZeroFloat16x16AVX512()
	acc30 := asm.ZeroFloat16x16AVX512()
	acc31 := asm.ZeroFloat16x16AVX512()
	aIdx := 0
	bIdx := 0
	for p := 0; p < kc; p++ {
		a0 := packedA[aIdx]
		a1 := packedA[aIdx+1]
		a2 := packedA[aIdx+2]
		a3 := packedA[aIdx+3]
		aIdx += 4
		vA0 := asm.BroadcastFloat16x16AVX512(uint16(a0))
		vA1 := asm.BroadcastFloat16x16AVX512(uint16(a1))
		vA2 := asm.BroadcastFloat16x16AVX512(uint16(a2))
		vA3 := asm.BroadcastFloat16x16AVX512(uint16(a3))
		vB0 := asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(packedB[bIdx:]))), len(packedB[bIdx:])))
		vB1 := asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(packedB[bIdx+lanes:]))), len(packedB[bIdx+lanes:])))
		bIdx += nr
		acc00 = vA0.MulAdd(vB0, acc00)
		acc01 = vA0.MulAdd(vB1, acc01)
		acc10 = vA1.MulAdd(vB0, acc10)
		acc11 = vA1.MulAdd(vB1, acc11)
		acc20 = vA2.MulAdd(vB0, acc20)
		acc21 = vA2.MulAdd(vB1, acc21)
		acc30 = vA3.MulAdd(vB0, acc30)
		acc31 = vA3.MulAdd(vB1, acc31)
	}
	cRow0 := ir * n
	cRow1 := (ir + 1) * n
	cRow2 := (ir + 2) * n
	cRow3 := (ir + 3) * n
	vC := asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow0+jr:]))), len(c[cRow0+jr:])))
	vC = vC.Add(acc00)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow0+jr:]))), len(c[cRow0+jr:])))
	vC = asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow0+jr+lanes:]))), len(c[cRow0+jr+lanes:])))
	vC = vC.Add(acc01)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow0+jr+lanes:]))), len(c[cRow0+jr+lanes:])))
	vC = asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow1+jr:]))), len(c[cRow1+jr:])))
	vC = vC.Add(acc10)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow1+jr:]))), len(c[cRow1+jr:])))
	vC = asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow1+jr+lanes:]))), len(c[cRow1+jr+lanes:])))
	vC = vC.Add(acc11)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow1+jr+lanes:]))), len(c[cRow1+jr+lanes:])))
	vC = asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow2+jr:]))), len(c[cRow2+jr:])))
	vC = vC.Add(acc20)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow2+jr:]))), len(c[cRow2+jr:])))
	vC = asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow2+jr+lanes:]))), len(c[cRow2+jr+lanes:])))
	vC = vC.Add(acc21)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow2+jr+lanes:]))), len(c[cRow2+jr+lanes:])))
	vC = asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow3+jr:]))), len(c[cRow3+jr:])))
	vC = vC.Add(acc30)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow3+jr:]))), len(c[cRow3+jr:])))
	vC = asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow3+jr+lanes:]))), len(c[cRow3+jr+lanes:])))
	vC = vC.Add(acc31)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow3+jr+lanes:]))), len(c[cRow3+jr+lanes:])))
}

func BasePackedMicroKernel_avx512_BFloat16(packedA []hwy.BFloat16, packedB []hwy.BFloat16, c []hwy.BFloat16, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 16
	if mr != 4 || nr != lanes*2 {
		basePackedMicroKernelGeneral(packedA, packedB, c, n, ir, jr, kc, mr, nr)
		return
	}
	acc00 := asm.ZeroBFloat16x16AVX512()
	acc01 := asm.ZeroBFloat16x16AVX512()
	acc10 := asm.ZeroBFloat16x16AVX512()
	acc11 := asm.ZeroBFloat16x16AVX512()
	acc20 := asm.ZeroBFloat16x16AVX512()
	acc21 := asm.ZeroBFloat16x16AVX512()
	acc30 := asm.ZeroBFloat16x16AVX512()
	acc31 := asm.ZeroBFloat16x16AVX512()
	aIdx := 0
	bIdx := 0
	for p := 0; p < kc; p++ {
		a0 := packedA[aIdx]
		a1 := packedA[aIdx+1]
		a2 := packedA[aIdx+2]
		a3 := packedA[aIdx+3]
		aIdx += 4
		vA0 := asm.BroadcastBFloat16x16AVX512(uint16(a0))
		vA1 := asm.BroadcastBFloat16x16AVX512(uint16(a1))
		vA2 := asm.BroadcastBFloat16x16AVX512(uint16(a2))
		vA3 := asm.BroadcastBFloat16x16AVX512(uint16(a3))
		vB0 := asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(packedB[bIdx:]))), len(packedB[bIdx:])))
		vB1 := asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(packedB[bIdx+lanes:]))), len(packedB[bIdx+lanes:])))
		bIdx += nr
		acc00 = vA0.MulAdd(vB0, acc00)
		acc01 = vA0.MulAdd(vB1, acc01)
		acc10 = vA1.MulAdd(vB0, acc10)
		acc11 = vA1.MulAdd(vB1, acc11)
		acc20 = vA2.MulAdd(vB0, acc20)
		acc21 = vA2.MulAdd(vB1, acc21)
		acc30 = vA3.MulAdd(vB0, acc30)
		acc31 = vA3.MulAdd(vB1, acc31)
	}
	cRow0 := ir * n
	cRow1 := (ir + 1) * n
	cRow2 := (ir + 2) * n
	cRow3 := (ir + 3) * n
	vC := asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow0+jr:]))), len(c[cRow0+jr:])))
	vC = vC.Add(acc00)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow0+jr:]))), len(c[cRow0+jr:])))
	vC = asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow0+jr+lanes:]))), len(c[cRow0+jr+lanes:])))
	vC = vC.Add(acc01)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow0+jr+lanes:]))), len(c[cRow0+jr+lanes:])))
	vC = asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow1+jr:]))), len(c[cRow1+jr:])))
	vC = vC.Add(acc10)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow1+jr:]))), len(c[cRow1+jr:])))
	vC = asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow1+jr+lanes:]))), len(c[cRow1+jr+lanes:])))
	vC = vC.Add(acc11)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow1+jr+lanes:]))), len(c[cRow1+jr+lanes:])))
	vC = asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow2+jr:]))), len(c[cRow2+jr:])))
	vC = vC.Add(acc20)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow2+jr:]))), len(c[cRow2+jr:])))
	vC = asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow2+jr+lanes:]))), len(c[cRow2+jr+lanes:])))
	vC = vC.Add(acc21)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow2+jr+lanes:]))), len(c[cRow2+jr+lanes:])))
	vC = asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow3+jr:]))), len(c[cRow3+jr:])))
	vC = vC.Add(acc30)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow3+jr:]))), len(c[cRow3+jr:])))
	vC = asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow3+jr+lanes:]))), len(c[cRow3+jr+lanes:])))
	vC = vC.Add(acc31)
	vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRow3+jr+lanes:]))), len(c[cRow3+jr+lanes:])))
}

func BasePackedMicroKernel_avx512(packedA []float32, packedB []float32, c []float32, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 16
	if mr != 4 || nr != lanes*2 {
		basePackedMicroKernelGeneral(packedA, packedB, c, n, ir, jr, kc, mr, nr)
		return
	}
	acc00 := archsimd.BroadcastFloat32x16(0)
	acc01 := archsimd.BroadcastFloat32x16(0)
	acc10 := archsimd.BroadcastFloat32x16(0)
	acc11 := archsimd.BroadcastFloat32x16(0)
	acc20 := archsimd.BroadcastFloat32x16(0)
	acc21 := archsimd.BroadcastFloat32x16(0)
	acc30 := archsimd.BroadcastFloat32x16(0)
	acc31 := archsimd.BroadcastFloat32x16(0)
	aIdx := 0
	bIdx := 0
	for p := 0; p < kc; p++ {
		a0 := packedA[aIdx]
		a1 := packedA[aIdx+1]
		a2 := packedA[aIdx+2]
		a3 := packedA[aIdx+3]
		aIdx += 4
		vA0 := archsimd.BroadcastFloat32x16(a0)
		vA1 := archsimd.BroadcastFloat32x16(a1)
		vA2 := archsimd.BroadcastFloat32x16(a2)
		vA3 := archsimd.BroadcastFloat32x16(a3)
		vB0 := archsimd.LoadFloat32x16Slice(packedB[bIdx:])
		vB1 := archsimd.LoadFloat32x16Slice(packedB[bIdx+lanes:])
		bIdx += nr
		acc00 = vA0.MulAdd(vB0, acc00)
		acc01 = vA0.MulAdd(vB1, acc01)
		acc10 = vA1.MulAdd(vB0, acc10)
		acc11 = vA1.MulAdd(vB1, acc11)
		acc20 = vA2.MulAdd(vB0, acc20)
		acc21 = vA2.MulAdd(vB1, acc21)
		acc30 = vA3.MulAdd(vB0, acc30)
		acc31 = vA3.MulAdd(vB1, acc31)
	}
	cRow0 := ir * n
	cRow1 := (ir + 1) * n
	cRow2 := (ir + 2) * n
	cRow3 := (ir + 3) * n
	vC := archsimd.LoadFloat32x16Slice(c[cRow0+jr:])
	vC = vC.Add(acc00)
	vC.StoreSlice(c[cRow0+jr:])
	vC = archsimd.LoadFloat32x16Slice(c[cRow0+jr+lanes:])
	vC = vC.Add(acc01)
	vC.StoreSlice(c[cRow0+jr+lanes:])
	vC = archsimd.LoadFloat32x16Slice(c[cRow1+jr:])
	vC = vC.Add(acc10)
	vC.StoreSlice(c[cRow1+jr:])
	vC = archsimd.LoadFloat32x16Slice(c[cRow1+jr+lanes:])
	vC = vC.Add(acc11)
	vC.StoreSlice(c[cRow1+jr+lanes:])
	vC = archsimd.LoadFloat32x16Slice(c[cRow2+jr:])
	vC = vC.Add(acc20)
	vC.StoreSlice(c[cRow2+jr:])
	vC = archsimd.LoadFloat32x16Slice(c[cRow2+jr+lanes:])
	vC = vC.Add(acc21)
	vC.StoreSlice(c[cRow2+jr+lanes:])
	vC = archsimd.LoadFloat32x16Slice(c[cRow3+jr:])
	vC = vC.Add(acc30)
	vC.StoreSlice(c[cRow3+jr:])
	vC = archsimd.LoadFloat32x16Slice(c[cRow3+jr+lanes:])
	vC = vC.Add(acc31)
	vC.StoreSlice(c[cRow3+jr+lanes:])
}

func BasePackedMicroKernel_avx512_Float64(packedA []float64, packedB []float64, c []float64, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 8
	if mr != 4 || nr != lanes*2 {
		basePackedMicroKernelGeneral(packedA, packedB, c, n, ir, jr, kc, mr, nr)
		return
	}
	acc00 := archsimd.BroadcastFloat64x8(0)
	acc01 := archsimd.BroadcastFloat64x8(0)
	acc10 := archsimd.BroadcastFloat64x8(0)
	acc11 := archsimd.BroadcastFloat64x8(0)
	acc20 := archsimd.BroadcastFloat64x8(0)
	acc21 := archsimd.BroadcastFloat64x8(0)
	acc30 := archsimd.BroadcastFloat64x8(0)
	acc31 := archsimd.BroadcastFloat64x8(0)
	aIdx := 0
	bIdx := 0
	for p := 0; p < kc; p++ {
		a0 := packedA[aIdx]
		a1 := packedA[aIdx+1]
		a2 := packedA[aIdx+2]
		a3 := packedA[aIdx+3]
		aIdx += 4
		vA0 := archsimd.BroadcastFloat64x8(a0)
		vA1 := archsimd.BroadcastFloat64x8(a1)
		vA2 := archsimd.BroadcastFloat64x8(a2)
		vA3 := archsimd.BroadcastFloat64x8(a3)
		vB0 := archsimd.LoadFloat64x8Slice(packedB[bIdx:])
		vB1 := archsimd.LoadFloat64x8Slice(packedB[bIdx+lanes:])
		bIdx += nr
		acc00 = vA0.MulAdd(vB0, acc00)
		acc01 = vA0.MulAdd(vB1, acc01)
		acc10 = vA1.MulAdd(vB0, acc10)
		acc11 = vA1.MulAdd(vB1, acc11)
		acc20 = vA2.MulAdd(vB0, acc20)
		acc21 = vA2.MulAdd(vB1, acc21)
		acc30 = vA3.MulAdd(vB0, acc30)
		acc31 = vA3.MulAdd(vB1, acc31)
	}
	cRow0 := ir * n
	cRow1 := (ir + 1) * n
	cRow2 := (ir + 2) * n
	cRow3 := (ir + 3) * n
	vC := archsimd.LoadFloat64x8Slice(c[cRow0+jr:])
	vC = vC.Add(acc00)
	vC.StoreSlice(c[cRow0+jr:])
	vC = archsimd.LoadFloat64x8Slice(c[cRow0+jr+lanes:])
	vC = vC.Add(acc01)
	vC.StoreSlice(c[cRow0+jr+lanes:])
	vC = archsimd.LoadFloat64x8Slice(c[cRow1+jr:])
	vC = vC.Add(acc10)
	vC.StoreSlice(c[cRow1+jr:])
	vC = archsimd.LoadFloat64x8Slice(c[cRow1+jr+lanes:])
	vC = vC.Add(acc11)
	vC.StoreSlice(c[cRow1+jr+lanes:])
	vC = archsimd.LoadFloat64x8Slice(c[cRow2+jr:])
	vC = vC.Add(acc20)
	vC.StoreSlice(c[cRow2+jr:])
	vC = archsimd.LoadFloat64x8Slice(c[cRow2+jr+lanes:])
	vC = vC.Add(acc21)
	vC.StoreSlice(c[cRow2+jr+lanes:])
	vC = archsimd.LoadFloat64x8Slice(c[cRow3+jr:])
	vC = vC.Add(acc30)
	vC.StoreSlice(c[cRow3+jr:])
	vC = archsimd.LoadFloat64x8Slice(c[cRow3+jr+lanes:])
	vC = vC.Add(acc31)
	vC.StoreSlice(c[cRow3+jr+lanes:])
}

func basePackedMicroKernelGeneral_avx512_Float16(packedA []hwy.Float16, packedB []hwy.Float16, c []hwy.Float16, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 16
	for r := 0; r < mr; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= nr; col += lanes {
			acc := asm.ZeroFloat16x16AVX512()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastFloat16x16AVX512(uint16(aVal))
				vB := asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(packedB[p*nr+col:]))), len(packedB[p*nr+col:])))
				acc = vA.MulAdd(vB, acc)
			}
			vC := asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRowStart+jr+col:]))), len(c[cRowStart+jr+col:])))
			vC = vC.Add(acc)
			vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRowStart+jr+col:]))), len(c[cRowStart+jr+col:])))
		}
		for ; col < nr; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r].Float32() * packedB[p*nr+col].Float32()
			}
			c[cRowStart+jr+col] = hwy.Float32ToFloat16(c[cRowStart+jr+col].Float32() + sum)
		}
	}
}

func basePackedMicroKernelGeneral_avx512_BFloat16(packedA []hwy.BFloat16, packedB []hwy.BFloat16, c []hwy.BFloat16, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 16
	for r := 0; r < mr; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= nr; col += lanes {
			acc := asm.ZeroBFloat16x16AVX512()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastBFloat16x16AVX512(uint16(aVal))
				vB := asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(packedB[p*nr+col:]))), len(packedB[p*nr+col:])))
				acc = vA.MulAdd(vB, acc)
			}
			vC := asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRowStart+jr+col:]))), len(c[cRowStart+jr+col:])))
			vC = vC.Add(acc)
			vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRowStart+jr+col:]))), len(c[cRowStart+jr+col:])))
		}
		for ; col < nr; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r].Float32() * packedB[p*nr+col].Float32()
			}
			c[cRowStart+jr+col] = hwy.Float32ToBFloat16(c[cRowStart+jr+col].Float32() + sum)
		}
	}
}

func basePackedMicroKernelGeneral_avx512(packedA []float32, packedB []float32, c []float32, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 16
	for r := 0; r < mr; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= nr; col += lanes {
			acc := archsimd.BroadcastFloat32x16(0)
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := archsimd.BroadcastFloat32x16(aVal)
				vB := archsimd.LoadFloat32x16Slice(packedB[p*nr+col:])
				acc = vA.MulAdd(vB, acc)
			}
			vC := archsimd.LoadFloat32x16Slice(c[cRowStart+jr+col:])
			vC = vC.Add(acc)
			vC.StoreSlice(c[cRowStart+jr+col:])
		}
		for ; col < nr; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r] * packedB[p*nr+col]
			}
			c[cRowStart+jr+col] += sum
		}
	}
}

func basePackedMicroKernelGeneral_avx512_Float64(packedA []float64, packedB []float64, c []float64, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 8
	for r := 0; r < mr; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= nr; col += lanes {
			acc := archsimd.BroadcastFloat64x8(0)
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := archsimd.BroadcastFloat64x8(aVal)
				vB := archsimd.LoadFloat64x8Slice(packedB[p*nr+col:])
				acc = vA.MulAdd(vB, acc)
			}
			vC := archsimd.LoadFloat64x8Slice(c[cRowStart+jr+col:])
			vC = vC.Add(acc)
			vC.StoreSlice(c[cRowStart+jr+col:])
		}
		for ; col < nr; col++ {
			var sum float64
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r] * packedB[p*nr+col]
			}
			c[cRowStart+jr+col] += sum
		}
	}
}

func BasePackedMicroKernelPartial_avx512_Float16(packedA []hwy.Float16, packedB []hwy.Float16, c []hwy.Float16, n int, ir int, jr int, kc int, mr int, nr int, activeRows int, activeCols int) {
	lanes := 16
	for r := 0; r < activeRows; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= activeCols; col += lanes {
			acc := asm.ZeroFloat16x16AVX512()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastFloat16x16AVX512(uint16(aVal))
				vB := asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(packedB[p*nr+col:]))), len(packedB[p*nr+col:])))
				acc = vA.MulAdd(vB, acc)
			}
			vC := asm.LoadFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRowStart+jr+col:]))), len(c[cRowStart+jr+col:])))
			vC = vC.Add(acc)
			vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRowStart+jr+col:]))), len(c[cRowStart+jr+col:])))
		}
		for ; col < activeCols; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r].Float32() * packedB[p*nr+col].Float32()
			}
			c[cRowStart+jr+col] = hwy.Float32ToFloat16(c[cRowStart+jr+col].Float32() + sum)
		}
	}
}

func BasePackedMicroKernelPartial_avx512_BFloat16(packedA []hwy.BFloat16, packedB []hwy.BFloat16, c []hwy.BFloat16, n int, ir int, jr int, kc int, mr int, nr int, activeRows int, activeCols int) {
	lanes := 16
	for r := 0; r < activeRows; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= activeCols; col += lanes {
			acc := asm.ZeroBFloat16x16AVX512()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastBFloat16x16AVX512(uint16(aVal))
				vB := asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(packedB[p*nr+col:]))), len(packedB[p*nr+col:])))
				acc = vA.MulAdd(vB, acc)
			}
			vC := asm.LoadBFloat16x16AVX512Slice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRowStart+jr+col:]))), len(c[cRowStart+jr+col:])))
			vC = vC.Add(acc)
			vC.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(c[cRowStart+jr+col:]))), len(c[cRowStart+jr+col:])))
		}
		for ; col < activeCols; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r].Float32() * packedB[p*nr+col].Float32()
			}
			c[cRowStart+jr+col] = hwy.Float32ToBFloat16(c[cRowStart+jr+col].Float32() + sum)
		}
	}
}

func BasePackedMicroKernelPartial_avx512(packedA []float32, packedB []float32, c []float32, n int, ir int, jr int, kc int, mr int, nr int, activeRows int, activeCols int) {
	lanes := 16
	for r := 0; r < activeRows; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= activeCols; col += lanes {
			acc := archsimd.BroadcastFloat32x16(0)
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := archsimd.BroadcastFloat32x16(aVal)
				vB := archsimd.LoadFloat32x16Slice(packedB[p*nr+col:])
				acc = vA.MulAdd(vB, acc)
			}
			vC := archsimd.LoadFloat32x16Slice(c[cRowStart+jr+col:])
			vC = vC.Add(acc)
			vC.StoreSlice(c[cRowStart+jr+col:])
		}
		for ; col < activeCols; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r] * packedB[p*nr+col]
			}
			c[cRowStart+jr+col] += sum
		}
	}
}

func BasePackedMicroKernelPartial_avx512_Float64(packedA []float64, packedB []float64, c []float64, n int, ir int, jr int, kc int, mr int, nr int, activeRows int, activeCols int) {
	lanes := 8
	for r := 0; r < activeRows; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= activeCols; col += lanes {
			acc := archsimd.BroadcastFloat64x8(0)
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := archsimd.BroadcastFloat64x8(aVal)
				vB := archsimd.LoadFloat64x8Slice(packedB[p*nr+col:])
				acc = vA.MulAdd(vB, acc)
			}
			vC := archsimd.LoadFloat64x8Slice(c[cRowStart+jr+col:])
			vC = vC.Add(acc)
			vC.StoreSlice(c[cRowStart+jr+col:])
		}
		for ; col < activeCols; col++ {
			var sum float64
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r] * packedB[p*nr+col]
			}
			c[cRowStart+jr+col] += sum
		}
	}
}
