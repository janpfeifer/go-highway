// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package matmul

import (
	"simd/archsimd"

	"github.com/ajroetker/go-highway/hwy"
)

var BlockMulAddFloat16 func(aT []hwy.Float16, b []hwy.Float16, c []hwy.Float16, blockDim int)
var BlockMulAddBFloat16 func(aT []hwy.BFloat16, b []hwy.BFloat16, c []hwy.BFloat16, blockDim int)
var BlockMulAddFloat32 func(aT []float32, b []float32, c []float32, blockDim int)
var BlockMulAddFloat64 func(aT []float64, b []float64, c []float64, blockDim int)
var BlockMulAdd2Float16 func(aT []hwy.Float16, b []hwy.Float16, c []hwy.Float16, blockDim int)
var BlockMulAdd2BFloat16 func(aT []hwy.BFloat16, b []hwy.BFloat16, c []hwy.BFloat16, blockDim int)
var BlockMulAdd2Float32 func(aT []float32, b []float32, c []float32, blockDim int)
var BlockMulAdd2Float64 func(aT []float64, b []float64, c []float64, blockDim int)
var BlockMulAddRegBlockedFloat16 func(aT []hwy.Float16, b []hwy.Float16, c []hwy.Float16, blockDim int)
var BlockMulAddRegBlockedBFloat16 func(aT []hwy.BFloat16, b []hwy.BFloat16, c []hwy.BFloat16, blockDim int)
var BlockMulAddRegBlockedFloat32 func(aT []float32, b []float32, c []float32, blockDim int)
var BlockMulAddRegBlockedFloat64 func(aT []float64, b []float64, c []float64, blockDim int)
var BlockMulAdd4Float16 func(aT []hwy.Float16, b []hwy.Float16, c []hwy.Float16, blockDim int)
var BlockMulAdd4BFloat16 func(aT []hwy.BFloat16, b []hwy.BFloat16, c []hwy.BFloat16, blockDim int)
var BlockMulAdd4Float32 func(aT []float32, b []float32, c []float32, blockDim int)
var BlockMulAdd4Float64 func(aT []float64, b []float64, c []float64, blockDim int)

// BlockMulAdd computes C += A * B for square blocks.
//
// This is designed for cache-tiled matrix multiplication where:
//   - aT is blockDim × blockDim (PRE-TRANSPOSED A, so rows are original A columns)
//   - b is blockDim × blockDim (row-major, rows are B rows)
//   - c is blockDim × blockDim (row-major, accumulated into)
//
// The caller passes A^T (transposed A) and B (normal), and the function computes:
//
//	C += (A^T)^T * B = A * B
//
// This layout is optimal for SIMD:
//   - A^T[k, i:i+lanes] gives us A[i:i+lanes, k] (contiguous in A^T)
//   - B[k, j:j+lanes] gives us B[k, j:j+lanes] (contiguous in B)
//
// For standard matmul C = A * B where you have A and B:
//  1. Transpose A to get A^T
//  2. Call BaseBlockMulAdd(A^T, B, C, blockDim)
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func BlockMulAdd[T hwy.Floats](aT []T, b []T, c []T, blockDim int) {
	switch any(aT).(type) {
	case []hwy.Float16:
		BlockMulAddFloat16(any(aT).([]hwy.Float16), any(b).([]hwy.Float16), any(c).([]hwy.Float16), blockDim)
	case []hwy.BFloat16:
		BlockMulAddBFloat16(any(aT).([]hwy.BFloat16), any(b).([]hwy.BFloat16), any(c).([]hwy.BFloat16), blockDim)
	case []float32:
		BlockMulAddFloat32(any(aT).([]float32), any(b).([]float32), any(c).([]float32), blockDim)
	case []float64:
		BlockMulAddFloat64(any(aT).([]float64), any(b).([]float64), any(c).([]float64), blockDim)
	}
}

// BlockMulAdd2 computes C += A * B processing 2 rows of C at a time.
//
// Loop unrolling improves performance by reusing B loads and increasing ILP.
// Same semantics as BaseBlockMulAdd but with 2-way row unrolling.
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func BlockMulAdd2[T hwy.Floats](aT []T, b []T, c []T, blockDim int) {
	switch any(aT).(type) {
	case []hwy.Float16:
		BlockMulAdd2Float16(any(aT).([]hwy.Float16), any(b).([]hwy.Float16), any(c).([]hwy.Float16), blockDim)
	case []hwy.BFloat16:
		BlockMulAdd2BFloat16(any(aT).([]hwy.BFloat16), any(b).([]hwy.BFloat16), any(c).([]hwy.BFloat16), blockDim)
	case []float32:
		BlockMulAdd2Float32(any(aT).([]float32), any(b).([]float32), any(c).([]float32), blockDim)
	case []float64:
		BlockMulAdd2Float64(any(aT).([]float64), any(b).([]float64), any(c).([]float64), blockDim)
	}
}

// BlockMulAddRegBlocked computes C += A * B using register blocking.
//
// This is the highest-performance kernel that holds accumulators in registers
// across the entire K dimension, minimizing memory traffic.
//
// The kernel processes:
//   - 4 rows of C (Mr=4)
//   - 2 vector widths of columns (Nr=2*lanes, e.g., 32 cols for AVX-512)
//   - The full K dimension with accumulators held in registers
//
// This matches the register-blocking strategy used by high-performance BLAS
// implementations like OpenBLAS and MKL.
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func BlockMulAddRegBlocked[T hwy.Floats](aT []T, b []T, c []T, blockDim int) {
	switch any(aT).(type) {
	case []hwy.Float16:
		BlockMulAddRegBlockedFloat16(any(aT).([]hwy.Float16), any(b).([]hwy.Float16), any(c).([]hwy.Float16), blockDim)
	case []hwy.BFloat16:
		BlockMulAddRegBlockedBFloat16(any(aT).([]hwy.BFloat16), any(b).([]hwy.BFloat16), any(c).([]hwy.BFloat16), blockDim)
	case []float32:
		BlockMulAddRegBlockedFloat32(any(aT).([]float32), any(b).([]float32), any(c).([]float32), blockDim)
	case []float64:
		BlockMulAddRegBlockedFloat64(any(aT).([]float64), any(b).([]float64), any(c).([]float64), blockDim)
	}
}

// BlockMulAdd4 computes C += A * B processing 4 rows of C at a time.
//
// 4-way loop unrolling for maximum performance on large blocks.
// Same semantics as BaseBlockMulAdd but with 4-way row unrolling.
//
// With aT layout, A[i,k], A[i+1,k], A[i+2,k], A[i+3,k] are consecutive
// in memory: aT[k*blockDim+i], aT[k*blockDim+i+1], etc.
// This provides excellent cache locality compared to the old interface.
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func BlockMulAdd4[T hwy.Floats](aT []T, b []T, c []T, blockDim int) {
	switch any(aT).(type) {
	case []hwy.Float16:
		BlockMulAdd4Float16(any(aT).([]hwy.Float16), any(b).([]hwy.Float16), any(c).([]hwy.Float16), blockDim)
	case []hwy.BFloat16:
		BlockMulAdd4BFloat16(any(aT).([]hwy.BFloat16), any(b).([]hwy.BFloat16), any(c).([]hwy.BFloat16), blockDim)
	case []float32:
		BlockMulAdd4Float32(any(aT).([]float32), any(b).([]float32), any(c).([]float32), blockDim)
	case []float64:
		BlockMulAdd4Float64(any(aT).([]float64), any(b).([]float64), any(c).([]float64), blockDim)
	}
}

func init() {
	if hwy.NoSimdEnv() {
		initBlockkernelFallback()
		return
	}
	if archsimd.X86.AVX512() {
		initBlockkernelAVX512()
		return
	}
	if archsimd.X86.AVX2() {
		initBlockkernelAVX2()
		return
	}
	initBlockkernelFallback()
}

func initBlockkernelAVX2() {
	BlockMulAddFloat16 = BaseBlockMulAdd_avx2_Float16
	BlockMulAddBFloat16 = BaseBlockMulAdd_avx2_BFloat16
	BlockMulAddFloat32 = BaseBlockMulAdd_avx2
	BlockMulAddFloat64 = BaseBlockMulAdd_avx2_Float64
	BlockMulAdd2Float16 = BaseBlockMulAdd2_avx2_Float16
	BlockMulAdd2BFloat16 = BaseBlockMulAdd2_avx2_BFloat16
	BlockMulAdd2Float32 = BaseBlockMulAdd2_avx2
	BlockMulAdd2Float64 = BaseBlockMulAdd2_avx2_Float64
	BlockMulAddRegBlockedFloat16 = BaseBlockMulAddRegBlocked_avx2_Float16
	BlockMulAddRegBlockedBFloat16 = BaseBlockMulAddRegBlocked_avx2_BFloat16
	BlockMulAddRegBlockedFloat32 = BaseBlockMulAddRegBlocked_avx2
	BlockMulAddRegBlockedFloat64 = BaseBlockMulAddRegBlocked_avx2_Float64
	BlockMulAdd4Float16 = BaseBlockMulAdd4_avx2_Float16
	BlockMulAdd4BFloat16 = BaseBlockMulAdd4_avx2_BFloat16
	BlockMulAdd4Float32 = BaseBlockMulAdd4_avx2
	BlockMulAdd4Float64 = BaseBlockMulAdd4_avx2_Float64
}

func initBlockkernelAVX512() {
	BlockMulAddFloat16 = BaseBlockMulAdd_avx512_Float16
	BlockMulAddBFloat16 = BaseBlockMulAdd_avx512_BFloat16
	BlockMulAddFloat32 = BaseBlockMulAdd_avx512
	BlockMulAddFloat64 = BaseBlockMulAdd_avx512_Float64
	BlockMulAdd2Float16 = BaseBlockMulAdd2_avx512_Float16
	BlockMulAdd2BFloat16 = BaseBlockMulAdd2_avx512_BFloat16
	BlockMulAdd2Float32 = BaseBlockMulAdd2_avx512
	BlockMulAdd2Float64 = BaseBlockMulAdd2_avx512_Float64
	BlockMulAddRegBlockedFloat16 = BaseBlockMulAddRegBlocked_avx512_Float16
	BlockMulAddRegBlockedBFloat16 = BaseBlockMulAddRegBlocked_avx512_BFloat16
	BlockMulAddRegBlockedFloat32 = BaseBlockMulAddRegBlocked_avx512
	BlockMulAddRegBlockedFloat64 = BaseBlockMulAddRegBlocked_avx512_Float64
	BlockMulAdd4Float16 = BaseBlockMulAdd4_avx512_Float16
	BlockMulAdd4BFloat16 = BaseBlockMulAdd4_avx512_BFloat16
	BlockMulAdd4Float32 = BaseBlockMulAdd4_avx512
	BlockMulAdd4Float64 = BaseBlockMulAdd4_avx512_Float64
}

func initBlockkernelFallback() {
	BlockMulAddFloat16 = BaseBlockMulAdd_fallback_Float16
	BlockMulAddBFloat16 = BaseBlockMulAdd_fallback_BFloat16
	BlockMulAddFloat32 = BaseBlockMulAdd_fallback
	BlockMulAddFloat64 = BaseBlockMulAdd_fallback_Float64
	BlockMulAdd2Float16 = BaseBlockMulAdd2_fallback_Float16
	BlockMulAdd2BFloat16 = BaseBlockMulAdd2_fallback_BFloat16
	BlockMulAdd2Float32 = BaseBlockMulAdd2_fallback
	BlockMulAdd2Float64 = BaseBlockMulAdd2_fallback_Float64
	BlockMulAddRegBlockedFloat16 = BaseBlockMulAddRegBlocked_fallback_Float16
	BlockMulAddRegBlockedBFloat16 = BaseBlockMulAddRegBlocked_fallback_BFloat16
	BlockMulAddRegBlockedFloat32 = BaseBlockMulAddRegBlocked_fallback
	BlockMulAddRegBlockedFloat64 = BaseBlockMulAddRegBlocked_fallback_Float64
	BlockMulAdd4Float16 = BaseBlockMulAdd4_fallback_Float16
	BlockMulAdd4BFloat16 = BaseBlockMulAdd4_fallback_BFloat16
	BlockMulAdd4Float32 = BaseBlockMulAdd4_fallback
	BlockMulAdd4Float64 = BaseBlockMulAdd4_fallback_Float64
}
