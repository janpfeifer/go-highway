// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build arm64

package matmul

import (
	"github.com/ajroetker/go-highway/hwy"
)

var MatMulKLastFloat16 func(a []hwy.Float16, b []hwy.Float16, c []hwy.Float16, m int, n int, k int)
var MatMulKLastBFloat16 func(a []hwy.BFloat16, b []hwy.BFloat16, c []hwy.BFloat16, m int, n int, k int)
var MatMulKLastFloat32 func(a []float32, b []float32, c []float32, m int, n int, k int)
var MatMulKLastFloat64 func(a []float64, b []float64, c []float64, m int, n int, k int)
var MatMulKLastBlockedFloat16 func(a []hwy.Float16, b []hwy.Float16, c []hwy.Float16, m int, n int, k int)
var MatMulKLastBlockedBFloat16 func(a []hwy.BFloat16, b []hwy.BFloat16, c []hwy.BFloat16, m int, n int, k int)
var MatMulKLastBlockedFloat32 func(a []float32, b []float32, c []float32, m int, n int, k int)
var MatMulKLastBlockedFloat64 func(a []float64, b []float64, c []float64, m int, n int, k int)

// MatMulKLast computes C = A * B^T where:
//   - A is M x K (row-major, K last)
//   - B is N x K (row-major, K last - PyTorch weight format)
//   - C is M x N (row-major)
//
// This is the "K-last" layout where both input matrices have K as their
// last dimension. This is the natural format for PyTorch weights and
// enables efficient SIMD since both A rows and B rows are contiguous.
//
// Each output element: C[i,j] = dot(A[i,:], B[j,:])
//
// The algorithm vectorizes along the K dimension:
//  1. Load SIMD-width elements from A row i
//  2. Load SIMD-width elements from B row j
//  3. Multiply and accumulate into a vector accumulator
//  4. Horizontal sum at the end to produce C[i,j]
//
// Memory access pattern:
//   - A row i: A[i*K : i*K+K] - sequential (cache friendly)
//   - B row j: B[j*K : j*K+K] - sequential (cache friendly)
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func MatMulKLast[T hwy.Floats](a []T, b []T, c []T, m int, n int, k int) {
	switch any(a).(type) {
	case []hwy.Float16:
		MatMulKLastFloat16(any(a).([]hwy.Float16), any(b).([]hwy.Float16), any(c).([]hwy.Float16), m, n, k)
	case []hwy.BFloat16:
		MatMulKLastBFloat16(any(a).([]hwy.BFloat16), any(b).([]hwy.BFloat16), any(c).([]hwy.BFloat16), m, n, k)
	case []float32:
		MatMulKLastFloat32(any(a).([]float32), any(b).([]float32), any(c).([]float32), m, n, k)
	case []float64:
		MatMulKLastFloat64(any(a).([]float64), any(b).([]float64), any(c).([]float64), m, n, k)
	}
}

// MatMulKLastBlocked is a cache-blocked version of MatMulKLast.
// It processes the output in tiles to improve cache locality for large matrices.
//
// Block sizes are chosen to fit in L1/L2 cache:
//   - blockM, blockN: output tile dimensions
//   - blockK: reduction tile along K dimension
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func MatMulKLastBlocked[T hwy.Floats](a []T, b []T, c []T, m int, n int, k int) {
	switch any(a).(type) {
	case []hwy.Float16:
		MatMulKLastBlockedFloat16(any(a).([]hwy.Float16), any(b).([]hwy.Float16), any(c).([]hwy.Float16), m, n, k)
	case []hwy.BFloat16:
		MatMulKLastBlockedBFloat16(any(a).([]hwy.BFloat16), any(b).([]hwy.BFloat16), any(c).([]hwy.BFloat16), m, n, k)
	case []float32:
		MatMulKLastBlockedFloat32(any(a).([]float32), any(b).([]float32), any(c).([]float32), m, n, k)
	case []float64:
		MatMulKLastBlockedFloat64(any(a).([]float64), any(b).([]float64), any(c).([]float64), m, n, k)
	}
}

func init() {
	if hwy.NoSimdEnv() {
		initMatmulklastFallback()
		return
	}
	initMatmulklastNEON()
	return
}

func initMatmulklastNEON() {
	MatMulKLastFloat16 = BaseMatMulKLast_neon_Float16
	MatMulKLastBFloat16 = BaseMatMulKLast_neon_BFloat16
	MatMulKLastFloat32 = BaseMatMulKLast_neon
	MatMulKLastFloat64 = BaseMatMulKLast_neon_Float64
	MatMulKLastBlockedFloat16 = BaseMatMulKLastBlocked_neon_Float16
	MatMulKLastBlockedBFloat16 = BaseMatMulKLastBlocked_neon_BFloat16
	MatMulKLastBlockedFloat32 = BaseMatMulKLastBlocked_neon
	MatMulKLastBlockedFloat64 = BaseMatMulKLastBlocked_neon_Float64
}

func initMatmulklastFallback() {
	MatMulKLastFloat16 = BaseMatMulKLast_fallback_Float16
	MatMulKLastBFloat16 = BaseMatMulKLast_fallback_BFloat16
	MatMulKLastFloat32 = BaseMatMulKLast_fallback
	MatMulKLastFloat64 = BaseMatMulKLast_fallback_Float64
	MatMulKLastBlockedFloat16 = BaseMatMulKLastBlocked_fallback_Float16
	MatMulKLastBlockedBFloat16 = BaseMatMulKLastBlocked_fallback_BFloat16
	MatMulKLastBlockedFloat32 = BaseMatMulKLastBlocked_fallback
	MatMulKLastBlockedFloat64 = BaseMatMulKLastBlocked_fallback_Float64
}
