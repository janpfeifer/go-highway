// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build arm64

package matmul

import (
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
)

func BasePackedMicroKernel_neon_Float16(packedA []hwy.Float16, packedB []hwy.Float16, c []hwy.Float16, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 8
	if mr != 4 || nr != lanes*2 {
		basePackedMicroKernelGeneral(packedA, packedB, c, n, ir, jr, kc, mr, nr)
		return
	}
	acc00 := asm.ZeroFloat16x8()
	acc01 := asm.ZeroFloat16x8()
	acc10 := asm.ZeroFloat16x8()
	acc11 := asm.ZeroFloat16x8()
	acc20 := asm.ZeroFloat16x8()
	acc21 := asm.ZeroFloat16x8()
	acc30 := asm.ZeroFloat16x8()
	acc31 := asm.ZeroFloat16x8()
	aIdx := 0
	bIdx := 0
	for p := 0; p < kc; p++ {
		a0 := packedA[aIdx]
		a1 := packedA[aIdx+1]
		a2 := packedA[aIdx+2]
		a3 := packedA[aIdx+3]
		aIdx += 4
		vA0 := asm.BroadcastFloat16x8(uint16(a0))
		vA1 := asm.BroadcastFloat16x8(uint16(a1))
		vA2 := asm.BroadcastFloat16x8(uint16(a2))
		vA3 := asm.BroadcastFloat16x8(uint16(a3))
		vB0 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&packedB[bIdx:][0]))
		vB1 := asm.LoadFloat16x8Ptr(unsafe.Pointer(&packedB[bIdx+lanes:][0]))
		bIdx += nr
		vA0.MulAddAcc(vB0, &acc00)
		vA0.MulAddAcc(vB1, &acc01)
		vA1.MulAddAcc(vB0, &acc10)
		vA1.MulAddAcc(vB1, &acc11)
		vA2.MulAddAcc(vB0, &acc20)
		vA2.MulAddAcc(vB1, &acc21)
		vA3.MulAddAcc(vB0, &acc30)
		vA3.MulAddAcc(vB1, &acc31)
	}
	cRow0 := ir * n
	cRow1 := (ir + 1) * n
	cRow2 := (ir + 2) * n
	cRow3 := (ir + 3) * n
	vC := asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRow0+jr:][0]))
	vC = vC.Add(acc00)
	vC.StorePtr(unsafe.Pointer(&c[cRow0+jr:][0]))
	vC = asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRow0+jr+lanes:][0]))
	vC = vC.Add(acc01)
	vC.StorePtr(unsafe.Pointer(&c[cRow0+jr+lanes:][0]))
	vC = asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRow1+jr:][0]))
	vC = vC.Add(acc10)
	vC.StorePtr(unsafe.Pointer(&c[cRow1+jr:][0]))
	vC = asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRow1+jr+lanes:][0]))
	vC = vC.Add(acc11)
	vC.StorePtr(unsafe.Pointer(&c[cRow1+jr+lanes:][0]))
	vC = asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRow2+jr:][0]))
	vC = vC.Add(acc20)
	vC.StorePtr(unsafe.Pointer(&c[cRow2+jr:][0]))
	vC = asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRow2+jr+lanes:][0]))
	vC = vC.Add(acc21)
	vC.StorePtr(unsafe.Pointer(&c[cRow2+jr+lanes:][0]))
	vC = asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRow3+jr:][0]))
	vC = vC.Add(acc30)
	vC.StorePtr(unsafe.Pointer(&c[cRow3+jr:][0]))
	vC = asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRow3+jr+lanes:][0]))
	vC = vC.Add(acc31)
	vC.StorePtr(unsafe.Pointer(&c[cRow3+jr+lanes:][0]))
}

func BasePackedMicroKernel_neon_BFloat16(packedA []hwy.BFloat16, packedB []hwy.BFloat16, c []hwy.BFloat16, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 8
	if mr != 4 || nr != lanes*2 {
		basePackedMicroKernelGeneral(packedA, packedB, c, n, ir, jr, kc, mr, nr)
		return
	}
	acc00 := asm.ZeroBFloat16x8()
	acc01 := asm.ZeroBFloat16x8()
	acc10 := asm.ZeroBFloat16x8()
	acc11 := asm.ZeroBFloat16x8()
	acc20 := asm.ZeroBFloat16x8()
	acc21 := asm.ZeroBFloat16x8()
	acc30 := asm.ZeroBFloat16x8()
	acc31 := asm.ZeroBFloat16x8()
	aIdx := 0
	bIdx := 0
	for p := 0; p < kc; p++ {
		a0 := packedA[aIdx]
		a1 := packedA[aIdx+1]
		a2 := packedA[aIdx+2]
		a3 := packedA[aIdx+3]
		aIdx += 4
		vA0 := asm.BroadcastBFloat16x8(uint16(a0))
		vA1 := asm.BroadcastBFloat16x8(uint16(a1))
		vA2 := asm.BroadcastBFloat16x8(uint16(a2))
		vA3 := asm.BroadcastBFloat16x8(uint16(a3))
		vB0 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&packedB[bIdx:][0]))
		vB1 := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&packedB[bIdx+lanes:][0]))
		bIdx += nr
		vA0.MulAddAcc(vB0, &acc00)
		vA0.MulAddAcc(vB1, &acc01)
		vA1.MulAddAcc(vB0, &acc10)
		vA1.MulAddAcc(vB1, &acc11)
		vA2.MulAddAcc(vB0, &acc20)
		vA2.MulAddAcc(vB1, &acc21)
		vA3.MulAddAcc(vB0, &acc30)
		vA3.MulAddAcc(vB1, &acc31)
	}
	cRow0 := ir * n
	cRow1 := (ir + 1) * n
	cRow2 := (ir + 2) * n
	cRow3 := (ir + 3) * n
	vC := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRow0+jr:][0]))
	vC = vC.Add(acc00)
	vC.StorePtr(unsafe.Pointer(&c[cRow0+jr:][0]))
	vC = asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRow0+jr+lanes:][0]))
	vC = vC.Add(acc01)
	vC.StorePtr(unsafe.Pointer(&c[cRow0+jr+lanes:][0]))
	vC = asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRow1+jr:][0]))
	vC = vC.Add(acc10)
	vC.StorePtr(unsafe.Pointer(&c[cRow1+jr:][0]))
	vC = asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRow1+jr+lanes:][0]))
	vC = vC.Add(acc11)
	vC.StorePtr(unsafe.Pointer(&c[cRow1+jr+lanes:][0]))
	vC = asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRow2+jr:][0]))
	vC = vC.Add(acc20)
	vC.StorePtr(unsafe.Pointer(&c[cRow2+jr:][0]))
	vC = asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRow2+jr+lanes:][0]))
	vC = vC.Add(acc21)
	vC.StorePtr(unsafe.Pointer(&c[cRow2+jr+lanes:][0]))
	vC = asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRow3+jr:][0]))
	vC = vC.Add(acc30)
	vC.StorePtr(unsafe.Pointer(&c[cRow3+jr:][0]))
	vC = asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRow3+jr+lanes:][0]))
	vC = vC.Add(acc31)
	vC.StorePtr(unsafe.Pointer(&c[cRow3+jr+lanes:][0]))
}

func BasePackedMicroKernel_neon(packedA []float32, packedB []float32, c []float32, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 4
	if mr != 4 || nr != lanes*2 {
		basePackedMicroKernelGeneral(packedA, packedB, c, n, ir, jr, kc, mr, nr)
		return
	}
	acc00 := asm.ZeroFloat32x4()
	acc01 := asm.ZeroFloat32x4()
	acc10 := asm.ZeroFloat32x4()
	acc11 := asm.ZeroFloat32x4()
	acc20 := asm.ZeroFloat32x4()
	acc21 := asm.ZeroFloat32x4()
	acc30 := asm.ZeroFloat32x4()
	acc31 := asm.ZeroFloat32x4()
	aIdx := 0
	bIdx := 0
	for p := 0; p < kc; p++ {
		a0 := packedA[aIdx]
		a1 := packedA[aIdx+1]
		a2 := packedA[aIdx+2]
		a3 := packedA[aIdx+3]
		aIdx += 4
		vA0 := asm.BroadcastFloat32x4(a0)
		vA1 := asm.BroadcastFloat32x4(a1)
		vA2 := asm.BroadcastFloat32x4(a2)
		vA3 := asm.BroadcastFloat32x4(a3)
		vB0 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&packedB[bIdx])))
		vB1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&packedB[bIdx+lanes])))
		bIdx += nr
		vA0.MulAddAcc(vB0, &acc00)
		vA0.MulAddAcc(vB1, &acc01)
		vA1.MulAddAcc(vB0, &acc10)
		vA1.MulAddAcc(vB1, &acc11)
		vA2.MulAddAcc(vB0, &acc20)
		vA2.MulAddAcc(vB1, &acc21)
		vA3.MulAddAcc(vB0, &acc30)
		vA3.MulAddAcc(vB1, &acc31)
	}
	cRow0 := ir * n
	cRow1 := (ir + 1) * n
	cRow2 := (ir + 2) * n
	cRow3 := (ir + 3) * n
	vC := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRow0+jr])))
	vC = vC.Add(acc00)
	vC.Store((*[4]float32)(unsafe.Pointer(&c[cRow0+jr])))
	vC = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRow0+jr+lanes])))
	vC = vC.Add(acc01)
	vC.Store((*[4]float32)(unsafe.Pointer(&c[cRow0+jr+lanes])))
	vC = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRow1+jr])))
	vC = vC.Add(acc10)
	vC.Store((*[4]float32)(unsafe.Pointer(&c[cRow1+jr])))
	vC = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRow1+jr+lanes])))
	vC = vC.Add(acc11)
	vC.Store((*[4]float32)(unsafe.Pointer(&c[cRow1+jr+lanes])))
	vC = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRow2+jr])))
	vC = vC.Add(acc20)
	vC.Store((*[4]float32)(unsafe.Pointer(&c[cRow2+jr])))
	vC = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRow2+jr+lanes])))
	vC = vC.Add(acc21)
	vC.Store((*[4]float32)(unsafe.Pointer(&c[cRow2+jr+lanes])))
	vC = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRow3+jr])))
	vC = vC.Add(acc30)
	vC.Store((*[4]float32)(unsafe.Pointer(&c[cRow3+jr])))
	vC = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRow3+jr+lanes])))
	vC = vC.Add(acc31)
	vC.Store((*[4]float32)(unsafe.Pointer(&c[cRow3+jr+lanes])))
}

func BasePackedMicroKernel_neon_Float64(packedA []float64, packedB []float64, c []float64, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 2
	if mr != 4 || nr != lanes*2 {
		basePackedMicroKernelGeneral(packedA, packedB, c, n, ir, jr, kc, mr, nr)
		return
	}
	acc00 := asm.ZeroFloat64x2()
	acc01 := asm.ZeroFloat64x2()
	acc10 := asm.ZeroFloat64x2()
	acc11 := asm.ZeroFloat64x2()
	acc20 := asm.ZeroFloat64x2()
	acc21 := asm.ZeroFloat64x2()
	acc30 := asm.ZeroFloat64x2()
	acc31 := asm.ZeroFloat64x2()
	aIdx := 0
	bIdx := 0
	for p := 0; p < kc; p++ {
		a0 := packedA[aIdx]
		a1 := packedA[aIdx+1]
		a2 := packedA[aIdx+2]
		a3 := packedA[aIdx+3]
		aIdx += 4
		vA0 := asm.BroadcastFloat64x2(a0)
		vA1 := asm.BroadcastFloat64x2(a1)
		vA2 := asm.BroadcastFloat64x2(a2)
		vA3 := asm.BroadcastFloat64x2(a3)
		vB0 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&packedB[bIdx])))
		vB1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&packedB[bIdx+lanes])))
		bIdx += nr
		vA0.MulAddAcc(vB0, &acc00)
		vA0.MulAddAcc(vB1, &acc01)
		vA1.MulAddAcc(vB0, &acc10)
		vA1.MulAddAcc(vB1, &acc11)
		vA2.MulAddAcc(vB0, &acc20)
		vA2.MulAddAcc(vB1, &acc21)
		vA3.MulAddAcc(vB0, &acc30)
		vA3.MulAddAcc(vB1, &acc31)
	}
	cRow0 := ir * n
	cRow1 := (ir + 1) * n
	cRow2 := (ir + 2) * n
	cRow3 := (ir + 3) * n
	vC := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRow0+jr])))
	vC = vC.Add(acc00)
	vC.Store((*[2]float64)(unsafe.Pointer(&c[cRow0+jr])))
	vC = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRow0+jr+lanes])))
	vC = vC.Add(acc01)
	vC.Store((*[2]float64)(unsafe.Pointer(&c[cRow0+jr+lanes])))
	vC = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRow1+jr])))
	vC = vC.Add(acc10)
	vC.Store((*[2]float64)(unsafe.Pointer(&c[cRow1+jr])))
	vC = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRow1+jr+lanes])))
	vC = vC.Add(acc11)
	vC.Store((*[2]float64)(unsafe.Pointer(&c[cRow1+jr+lanes])))
	vC = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRow2+jr])))
	vC = vC.Add(acc20)
	vC.Store((*[2]float64)(unsafe.Pointer(&c[cRow2+jr])))
	vC = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRow2+jr+lanes])))
	vC = vC.Add(acc21)
	vC.Store((*[2]float64)(unsafe.Pointer(&c[cRow2+jr+lanes])))
	vC = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRow3+jr])))
	vC = vC.Add(acc30)
	vC.Store((*[2]float64)(unsafe.Pointer(&c[cRow3+jr])))
	vC = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRow3+jr+lanes])))
	vC = vC.Add(acc31)
	vC.Store((*[2]float64)(unsafe.Pointer(&c[cRow3+jr+lanes])))
}

func basePackedMicroKernelGeneral_neon_Float16(packedA []hwy.Float16, packedB []hwy.Float16, c []hwy.Float16, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 8
	for r := 0; r < mr; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= nr; col += lanes {
			acc := asm.ZeroFloat16x8()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastFloat16x8(uint16(aVal))
				vB := asm.LoadFloat16x8Ptr(unsafe.Pointer(&packedB[p*nr+col:][0]))
				vA.MulAddAcc(vB, &acc)
			}
			vC := asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRowStart+jr+col:][0]))
			vC = vC.Add(acc)
			vC.StorePtr(unsafe.Pointer(&c[cRowStart+jr+col:][0]))
		}
		for ; col < nr; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r].Float32() * packedB[p*nr+col].Float32()
			}
			c[cRowStart+jr+col] = hwy.Float32ToFloat16(c[cRowStart+jr+col].Float32() + sum)
		}
	}
}

func basePackedMicroKernelGeneral_neon_BFloat16(packedA []hwy.BFloat16, packedB []hwy.BFloat16, c []hwy.BFloat16, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 8
	for r := 0; r < mr; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= nr; col += lanes {
			acc := asm.ZeroBFloat16x8()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastBFloat16x8(uint16(aVal))
				vB := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&packedB[p*nr+col:][0]))
				vA.MulAddAcc(vB, &acc)
			}
			vC := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRowStart+jr+col:][0]))
			vC = vC.Add(acc)
			vC.StorePtr(unsafe.Pointer(&c[cRowStart+jr+col:][0]))
		}
		for ; col < nr; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r].Float32() * packedB[p*nr+col].Float32()
			}
			c[cRowStart+jr+col] = hwy.Float32ToBFloat16(c[cRowStart+jr+col].Float32() + sum)
		}
	}
}

func basePackedMicroKernelGeneral_neon(packedA []float32, packedB []float32, c []float32, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 4
	for r := 0; r < mr; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= nr; col += lanes {
			acc := asm.ZeroFloat32x4()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastFloat32x4(aVal)
				vB := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&packedB[p*nr+col])))
				vA.MulAddAcc(vB, &acc)
			}
			vC := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRowStart+jr+col])))
			vC = vC.Add(acc)
			vC.Store((*[4]float32)(unsafe.Pointer(&c[cRowStart+jr+col])))
		}
		for ; col < nr; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r] * packedB[p*nr+col]
			}
			c[cRowStart+jr+col] += sum
		}
	}
}

func basePackedMicroKernelGeneral_neon_Float64(packedA []float64, packedB []float64, c []float64, n int, ir int, jr int, kc int, mr int, nr int) {
	lanes := 2
	for r := 0; r < mr; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= nr; col += lanes {
			acc := asm.ZeroFloat64x2()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastFloat64x2(aVal)
				vB := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&packedB[p*nr+col])))
				vA.MulAddAcc(vB, &acc)
			}
			vC := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRowStart+jr+col])))
			vC = vC.Add(acc)
			vC.Store((*[2]float64)(unsafe.Pointer(&c[cRowStart+jr+col])))
		}
		for ; col < nr; col++ {
			var sum float64
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r] * packedB[p*nr+col]
			}
			c[cRowStart+jr+col] += sum
		}
	}
}

func BasePackedMicroKernelPartial_neon_Float16(packedA []hwy.Float16, packedB []hwy.Float16, c []hwy.Float16, n int, ir int, jr int, kc int, mr int, nr int, activeRows int, activeCols int) {
	lanes := 8
	for r := 0; r < activeRows; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= activeCols; col += lanes {
			acc := asm.ZeroFloat16x8()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastFloat16x8(uint16(aVal))
				vB := asm.LoadFloat16x8Ptr(unsafe.Pointer(&packedB[p*nr+col:][0]))
				vA.MulAddAcc(vB, &acc)
			}
			vC := asm.LoadFloat16x8Ptr(unsafe.Pointer(&c[cRowStart+jr+col:][0]))
			vC = vC.Add(acc)
			vC.StorePtr(unsafe.Pointer(&c[cRowStart+jr+col:][0]))
		}
		for ; col < activeCols; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r].Float32() * packedB[p*nr+col].Float32()
			}
			c[cRowStart+jr+col] = hwy.Float32ToFloat16(c[cRowStart+jr+col].Float32() + sum)
		}
	}
}

func BasePackedMicroKernelPartial_neon_BFloat16(packedA []hwy.BFloat16, packedB []hwy.BFloat16, c []hwy.BFloat16, n int, ir int, jr int, kc int, mr int, nr int, activeRows int, activeCols int) {
	lanes := 8
	for r := 0; r < activeRows; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= activeCols; col += lanes {
			acc := asm.ZeroBFloat16x8()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastBFloat16x8(uint16(aVal))
				vB := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&packedB[p*nr+col:][0]))
				vA.MulAddAcc(vB, &acc)
			}
			vC := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&c[cRowStart+jr+col:][0]))
			vC = vC.Add(acc)
			vC.StorePtr(unsafe.Pointer(&c[cRowStart+jr+col:][0]))
		}
		for ; col < activeCols; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r].Float32() * packedB[p*nr+col].Float32()
			}
			c[cRowStart+jr+col] = hwy.Float32ToBFloat16(c[cRowStart+jr+col].Float32() + sum)
		}
	}
}

func BasePackedMicroKernelPartial_neon(packedA []float32, packedB []float32, c []float32, n int, ir int, jr int, kc int, mr int, nr int, activeRows int, activeCols int) {
	lanes := 4
	for r := 0; r < activeRows; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= activeCols; col += lanes {
			acc := asm.ZeroFloat32x4()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastFloat32x4(aVal)
				vB := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&packedB[p*nr+col])))
				vA.MulAddAcc(vB, &acc)
			}
			vC := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&c[cRowStart+jr+col])))
			vC = vC.Add(acc)
			vC.Store((*[4]float32)(unsafe.Pointer(&c[cRowStart+jr+col])))
		}
		for ; col < activeCols; col++ {
			var sum float32
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r] * packedB[p*nr+col]
			}
			c[cRowStart+jr+col] += sum
		}
	}
}

func BasePackedMicroKernelPartial_neon_Float64(packedA []float64, packedB []float64, c []float64, n int, ir int, jr int, kc int, mr int, nr int, activeRows int, activeCols int) {
	lanes := 2
	for r := 0; r < activeRows; r++ {
		cRowStart := (ir + r) * n
		var col int
		for col = 0; col+lanes <= activeCols; col += lanes {
			acc := asm.ZeroFloat64x2()
			for p := 0; p < kc; p++ {
				aVal := packedA[p*mr+r]
				vA := asm.BroadcastFloat64x2(aVal)
				vB := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&packedB[p*nr+col])))
				vA.MulAddAcc(vB, &acc)
			}
			vC := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&c[cRowStart+jr+col])))
			vC = vC.Add(acc)
			vC.Store((*[2]float64)(unsafe.Pointer(&c[cRowStart+jr+col])))
		}
		for ; col < activeCols; col++ {
			var sum float64
			for p := 0; p < kc; p++ {
				sum += packedA[p*mr+r] * packedB[p*nr+col]
			}
			c[cRowStart+jr+col] += sum
		}
	}
}
