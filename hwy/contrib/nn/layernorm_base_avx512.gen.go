// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package nn

import (
	stdmath "math"
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
)

func BaseLayerNorm_avx512_Float16(input []hwy.Float16, output []hwy.Float16, normSize int, gamma []hwy.Float16, beta []hwy.Float16, epsilon hwy.Float16) {
	size := min(len(input), len(output))
	if size == 0 || normSize <= 0 {
		return
	}
	numGroups := size / normSize
	invN := float32(1.0) / float32(normSize)
	lanes := 16
	for g := 0; g < numGroups; g++ {
		off := g * normSize
		sumAcc := asm.ZeroFloat16x16AVX512()
		ii := 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
			sumAcc = sumAcc.Add(x)
		}
		mean := sumAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			mean += input[off+i].Float32()
		}
		mean *= invN
		vMean := asm.BroadcastFloat16x16AVX512(uint16(hwy.Float32ToFloat16(mean)))
		varAcc := asm.ZeroFloat16x16AVX512()
		ii = 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
			diff := x.Sub(vMean)
			varAcc = diff.MulAdd(diff, varAcc)
		}
		variance := varAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			diff := input[off+i].Float32() - mean
			variance += diff * diff
		}
		variance *= invN
		invStd := float32(1.0 / stdmath.Sqrt(float64(variance+epsilon.Float32())))
		vInvStd := asm.BroadcastFloat16x16AVX512(uint16(hwy.Float32ToFloat16(invStd)))
		if gamma != nil && beta != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&gamma[ii:][0]))
				b := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&beta[ii:][0]))
				result := normed.MulAdd(g, b)
				result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(output[off+ii:]))), len(output[off+ii:])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i].Float32() - mean) * invStd
				output[off+i] = hwy.Float32ToFloat16(normed*gamma[i].Float32() + beta[i].Float32())
			}
		} else if gamma != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&gamma[ii:][0]))
				result := normed.Mul(g)
				result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(output[off+ii:]))), len(output[off+ii:])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i].Float32() - mean) * invStd
				output[off+i] = hwy.Float32ToFloat16(normed * gamma[i].Float32())
			}
		} else {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				result := diff.Mul(vInvStd)
				result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(output[off+ii:]))), len(output[off+ii:])))
			}
			for i := ii; i < normSize; i++ {
				output[off+i] = hwy.Float32ToFloat16((input[off+i].Float32() - mean) * invStd)
			}
		}
	}
}

func BaseLayerNorm_avx512_BFloat16(input []hwy.BFloat16, output []hwy.BFloat16, normSize int, gamma []hwy.BFloat16, beta []hwy.BFloat16, epsilon hwy.BFloat16) {
	size := min(len(input), len(output))
	if size == 0 || normSize <= 0 {
		return
	}
	numGroups := size / normSize
	invN := float32(1.0) / float32(normSize)
	lanes := 16
	for g := 0; g < numGroups; g++ {
		off := g * normSize
		sumAcc := asm.ZeroBFloat16x16AVX512()
		ii := 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
			sumAcc = sumAcc.Add(x)
		}
		mean := sumAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			mean += input[off+i].Float32()
		}
		mean *= invN
		vMean := asm.BroadcastBFloat16x16AVX512(uint16(hwy.Float32ToBFloat16(mean)))
		varAcc := asm.ZeroBFloat16x16AVX512()
		ii = 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
			diff := x.Sub(vMean)
			varAcc = diff.MulAdd(diff, varAcc)
		}
		variance := varAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			diff := input[off+i].Float32() - mean
			variance += diff * diff
		}
		variance *= invN
		invStd := float32(1.0 / stdmath.Sqrt(float64(variance+epsilon.Float32())))
		vInvStd := asm.BroadcastBFloat16x16AVX512(uint16(hwy.Float32ToBFloat16(invStd)))
		if gamma != nil && beta != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&gamma[ii:][0]))
				b := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&beta[ii:][0]))
				result := normed.MulAdd(g, b)
				result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(output[off+ii:]))), len(output[off+ii:])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i].Float32() - mean) * invStd
				output[off+i] = hwy.Float32ToBFloat16(normed*gamma[i].Float32() + beta[i].Float32())
			}
		} else if gamma != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&gamma[ii:][0]))
				result := normed.Mul(g)
				result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(output[off+ii:]))), len(output[off+ii:])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i].Float32() - mean) * invStd
				output[off+i] = hwy.Float32ToBFloat16(normed * gamma[i].Float32())
			}
		} else {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadBFloat16x16AVX512Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				result := diff.Mul(vInvStd)
				result.StoreSlice(unsafe.Slice((*uint16)(unsafe.Pointer(unsafe.SliceData(output[off+ii:]))), len(output[off+ii:])))
			}
			for i := ii; i < normSize; i++ {
				output[off+i] = hwy.Float32ToBFloat16((input[off+i].Float32() - mean) * invStd)
			}
		}
	}
}

func BaseLayerNorm_avx512(input []float32, output []float32, normSize int, gamma []float32, beta []float32, epsilon float32) {
	size := min(len(input), len(output))
	if size == 0 || normSize <= 0 {
		return
	}
	numGroups := size / normSize
	invN := float32(1.0) / float32(normSize)
	lanes := 16
	for g := 0; g < numGroups; g++ {
		off := g * normSize
		sumAcc := archsimd.BroadcastFloat32x16(0)
		ii := 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&input[off+ii])))
			sumAcc = sumAcc.Add(x)
		}
		mean := hwy.ReduceSum_AVX512_F32x16(sumAcc)
		for i := ii; i < normSize; i++ {
			mean += input[off+i]
		}
		mean *= invN
		vMean := archsimd.BroadcastFloat32x16(mean)
		varAcc := archsimd.BroadcastFloat32x16(0)
		ii = 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&input[off+ii])))
			diff := x.Sub(vMean)
			varAcc = diff.MulAdd(diff, varAcc)
		}
		variance := hwy.ReduceSum_AVX512_F32x16(varAcc)
		for i := ii; i < normSize; i++ {
			diff := input[off+i] - mean
			variance += diff * diff
		}
		variance *= invN
		invStd := float32(1.0 / stdmath.Sqrt(float64(variance+epsilon)))
		vInvStd := archsimd.BroadcastFloat32x16(invStd)
		if gamma != nil && beta != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&gamma[ii])))
				b := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&beta[ii])))
				result := normed.MulAdd(g, b)
				result.Store((*[16]float32)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i] - mean) * invStd
				output[off+i] = normed*gamma[i] + beta[i]
			}
		} else if gamma != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&gamma[ii])))
				result := normed.Mul(g)
				result.Store((*[16]float32)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i] - mean) * invStd
				output[off+i] = normed * gamma[i]
			}
		} else {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := archsimd.LoadFloat32x16((*[16]float32)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				result := diff.Mul(vInvStd)
				result.Store((*[16]float32)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				output[off+i] = (input[off+i] - mean) * invStd
			}
		}
	}
}

func BaseLayerNorm_avx512_Float64(input []float64, output []float64, normSize int, gamma []float64, beta []float64, epsilon float64) {
	size := min(len(input), len(output))
	if size == 0 || normSize <= 0 {
		return
	}
	numGroups := size / normSize
	invN := float64(1.0) / float64(normSize)
	lanes := 8
	for g := 0; g < numGroups; g++ {
		off := g * normSize
		sumAcc := archsimd.BroadcastFloat64x8(0)
		ii := 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&input[off+ii])))
			sumAcc = sumAcc.Add(x)
		}
		mean := hwy.ReduceSum_AVX512_F64x8(sumAcc)
		for i := ii; i < normSize; i++ {
			mean += input[off+i]
		}
		mean *= invN
		vMean := archsimd.BroadcastFloat64x8(mean)
		varAcc := archsimd.BroadcastFloat64x8(0)
		ii = 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&input[off+ii])))
			diff := x.Sub(vMean)
			varAcc = diff.MulAdd(diff, varAcc)
		}
		variance := hwy.ReduceSum_AVX512_F64x8(varAcc)
		for i := ii; i < normSize; i++ {
			diff := input[off+i] - mean
			variance += diff * diff
		}
		variance *= invN
		invStd := float64(1.0 / stdmath.Sqrt(float64(variance+epsilon)))
		vInvStd := archsimd.BroadcastFloat64x8(invStd)
		if gamma != nil && beta != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&gamma[ii])))
				b := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&beta[ii])))
				result := normed.MulAdd(g, b)
				result.Store((*[8]float64)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i] - mean) * invStd
				output[off+i] = normed*gamma[i] + beta[i]
			}
		} else if gamma != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&gamma[ii])))
				result := normed.Mul(g)
				result.Store((*[8]float64)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i] - mean) * invStd
				output[off+i] = normed * gamma[i]
			}
		} else {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := archsimd.LoadFloat64x8((*[8]float64)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				result := diff.Mul(vInvStd)
				result.Store((*[8]float64)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				output[off+i] = (input[off+i] - mean) * invStd
			}
		}
	}
}
