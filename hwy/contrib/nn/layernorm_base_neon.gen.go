// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build arm64

package nn

import (
	stdmath "math"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
)

func BaseLayerNorm_neon_Float16(input []hwy.Float16, output []hwy.Float16, normSize int, gamma []hwy.Float16, beta []hwy.Float16, epsilon hwy.Float16) {
	size := min(len(input), len(output))
	if size == 0 || normSize <= 0 {
		return
	}
	numGroups := size / normSize
	invN := float32(1.0) / float32(normSize)
	lanes := 8
	for g := range numGroups {
		off := g * normSize
		sumAcc := asm.ZeroFloat16x8()
		ii := 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
			sumAcc = sumAcc.Add(x)
		}
		mean := sumAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			mean += input[off+i].Float32()
		}
		mean *= invN
		vMean := asm.BroadcastFloat16x8(uint16(hwy.Float32ToFloat16(mean)))
		varAcc := asm.ZeroFloat16x8()
		ii = 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
			diff := x.Sub(vMean)
			diff.MulAddAcc(diff, &varAcc)
		}
		variance := varAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			diff := input[off+i].Float32() - mean
			variance += diff * diff
		}
		variance *= invN
		invStd := float32(1.0 / stdmath.Sqrt(float64(variance+epsilon.Float32())))
		vInvStd := asm.BroadcastFloat16x8(uint16(hwy.Float32ToFloat16(invStd)))
		if gamma != nil && beta != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadFloat16x8Ptr(unsafe.Pointer(&gamma[ii:][0]))
				b := asm.LoadFloat16x8Ptr(unsafe.Pointer(&beta[ii:][0]))
				result := normed.MulAdd(g, b)
				result.StorePtr(unsafe.Pointer(&output[off+ii:][0]))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i].Float32() - mean) * invStd
				output[off+i] = hwy.Float32ToFloat16(normed*gamma[i].Float32() + beta[i].Float32())
			}
		} else if gamma != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadFloat16x8Ptr(unsafe.Pointer(&gamma[ii:][0]))
				result := normed.Mul(g)
				result.StorePtr(unsafe.Pointer(&output[off+ii:][0]))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i].Float32() - mean) * invStd
				output[off+i] = hwy.Float32ToFloat16(normed * gamma[i].Float32())
			}
		} else {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				result := diff.Mul(vInvStd)
				result.StorePtr(unsafe.Pointer(&output[off+ii:][0]))
			}
			for i := ii; i < normSize; i++ {
				output[off+i] = hwy.Float32ToFloat16((input[off+i].Float32() - mean) * invStd)
			}
		}
	}
}

func BaseLayerNorm_neon_BFloat16(input []hwy.BFloat16, output []hwy.BFloat16, normSize int, gamma []hwy.BFloat16, beta []hwy.BFloat16, epsilon hwy.BFloat16) {
	size := min(len(input), len(output))
	if size == 0 || normSize <= 0 {
		return
	}
	numGroups := size / normSize
	invN := float32(1.0) / float32(normSize)
	lanes := 8
	for g := range numGroups {
		off := g * normSize
		sumAcc := asm.ZeroBFloat16x8()
		ii := 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
			sumAcc = sumAcc.Add(x)
		}
		mean := sumAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			mean += input[off+i].Float32()
		}
		mean *= invN
		vMean := asm.BroadcastBFloat16x8(uint16(hwy.Float32ToBFloat16(mean)))
		varAcc := asm.ZeroBFloat16x8()
		ii = 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
			diff := x.Sub(vMean)
			diff.MulAddAcc(diff, &varAcc)
		}
		variance := varAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			diff := input[off+i].Float32() - mean
			variance += diff * diff
		}
		variance *= invN
		invStd := float32(1.0 / stdmath.Sqrt(float64(variance+epsilon.Float32())))
		vInvStd := asm.BroadcastBFloat16x8(uint16(hwy.Float32ToBFloat16(invStd)))
		if gamma != nil && beta != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&gamma[ii:][0]))
				b := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&beta[ii:][0]))
				result := normed.MulAdd(g, b)
				result.StorePtr(unsafe.Pointer(&output[off+ii:][0]))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i].Float32() - mean) * invStd
				output[off+i] = hwy.Float32ToBFloat16(normed*gamma[i].Float32() + beta[i].Float32())
			}
		} else if gamma != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&gamma[ii:][0]))
				result := normed.Mul(g)
				result.StorePtr(unsafe.Pointer(&output[off+ii:][0]))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i].Float32() - mean) * invStd
				output[off+i] = hwy.Float32ToBFloat16(normed * gamma[i].Float32())
			}
		} else {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadBFloat16x8Ptr(unsafe.Pointer(&input[off+ii:][0]))
				diff := x.Sub(vMean)
				result := diff.Mul(vInvStd)
				result.StorePtr(unsafe.Pointer(&output[off+ii:][0]))
			}
			for i := ii; i < normSize; i++ {
				output[off+i] = hwy.Float32ToBFloat16((input[off+i].Float32() - mean) * invStd)
			}
		}
	}
}

func BaseLayerNorm_neon(input []float32, output []float32, normSize int, gamma []float32, beta []float32, epsilon float32) {
	size := min(len(input), len(output))
	if size == 0 || normSize <= 0 {
		return
	}
	numGroups := size / normSize
	invN := float32(1.0) / float32(normSize)
	lanes := 4
	for g := range numGroups {
		off := g * normSize
		sumAcc := asm.ZeroFloat32x4()
		ii := 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&input[off+ii])))
			sumAcc = sumAcc.Add(x)
		}
		mean := sumAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			mean += input[off+i]
		}
		mean *= invN
		vMean := asm.BroadcastFloat32x4(mean)
		varAcc := asm.ZeroFloat32x4()
		ii = 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&input[off+ii])))
			diff := x.Sub(vMean)
			diff.MulAddAcc(diff, &varAcc)
		}
		variance := varAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			diff := input[off+i] - mean
			variance += diff * diff
		}
		variance *= invN
		invStd := float32(1.0 / stdmath.Sqrt(float64(variance+epsilon)))
		vInvStd := asm.BroadcastFloat32x4(invStd)
		if gamma != nil && beta != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&gamma[ii])))
				b := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&beta[ii])))
				result := normed.MulAdd(g, b)
				result.Store((*[4]float32)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i] - mean) * invStd
				output[off+i] = normed*gamma[i] + beta[i]
			}
		} else if gamma != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&gamma[ii])))
				result := normed.Mul(g)
				result.Store((*[4]float32)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i] - mean) * invStd
				output[off+i] = normed * gamma[i]
			}
		} else {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				result := diff.Mul(vInvStd)
				result.Store((*[4]float32)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				output[off+i] = (input[off+i] - mean) * invStd
			}
		}
	}
}

func BaseLayerNorm_neon_Float64(input []float64, output []float64, normSize int, gamma []float64, beta []float64, epsilon float64) {
	size := min(len(input), len(output))
	if size == 0 || normSize <= 0 {
		return
	}
	numGroups := size / normSize
	invN := float64(1.0) / float64(normSize)
	lanes := 2
	for g := range numGroups {
		off := g * normSize
		sumAcc := asm.ZeroFloat64x2()
		ii := 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&input[off+ii])))
			sumAcc = sumAcc.Add(x)
		}
		mean := sumAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			mean += input[off+i]
		}
		mean *= invN
		vMean := asm.BroadcastFloat64x2(mean)
		varAcc := asm.ZeroFloat64x2()
		ii = 0
		for ; ii+lanes <= normSize; ii += lanes {
			x := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&input[off+ii])))
			diff := x.Sub(vMean)
			diff.MulAddAcc(diff, &varAcc)
		}
		variance := varAcc.ReduceSum()
		for i := ii; i < normSize; i++ {
			diff := input[off+i] - mean
			variance += diff * diff
		}
		variance *= invN
		invStd := float64(1.0 / stdmath.Sqrt(float64(variance+epsilon)))
		vInvStd := asm.BroadcastFloat64x2(invStd)
		if gamma != nil && beta != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&gamma[ii])))
				b := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&beta[ii])))
				result := normed.MulAdd(g, b)
				result.Store((*[2]float64)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i] - mean) * invStd
				output[off+i] = normed*gamma[i] + beta[i]
			}
		} else if gamma != nil {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				normed := diff.Mul(vInvStd)
				g := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&gamma[ii])))
				result := normed.Mul(g)
				result.Store((*[2]float64)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				normed := (input[off+i] - mean) * invStd
				output[off+i] = normed * gamma[i]
			}
		} else {
			ii = 0
			for ; ii+lanes <= normSize; ii += lanes {
				x := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&input[off+ii])))
				diff := x.Sub(vMean)
				result := diff.Mul(vInvStd)
				result.Store((*[2]float64)(unsafe.Pointer(&output[off+ii])))
			}
			for i := ii; i < normSize; i++ {
				output[off+i] = (input[off+i] - mean) * invStd
			}
		}
	}
}
