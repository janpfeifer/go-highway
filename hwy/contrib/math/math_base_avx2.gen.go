// Code generated by hwygen. DO NOT EDIT.
//go:build amd64 && goexperiment.simd

package math

import (
	"github.com/ajroetker/go-highway/hwy"
	stdmath "math"
	"simd/archsimd"
)

// Hoisted constants - pre-broadcasted at package init time
var (
	BaseExpPoly_AVX2_c1_f64              = archsimd.BroadcastFloat64x4(float64(expC1_f64))
	BaseAsinPoly_AVX2_zero_f32           = archsimd.BroadcastFloat32x8(float32(asinZero_f32))
	BaseAcosPoly_AVX2_piOver2_f32        = archsimd.BroadcastFloat32x8(float32(asinPiOver2_f32))
	BaseAcosPoly_AVX2_p4_f32             = archsimd.BroadcastFloat32x8(float32(asinP4_f32))
	BaseAcosPoly_AVX2_zero_f64           = archsimd.BroadcastFloat64x4(float64(asinZero_f64))
	BaseAtan2Poly_AVX2_piOver2_f32       = archsimd.BroadcastFloat32x8(float32(atanPiOver2_f32))
	BaseExpPoly_AVX2_c3_f64              = archsimd.BroadcastFloat64x4(float64(expC3_f64))
	BaseSinPoly_AVX2_intOne_i32_f64      = archsimd.BroadcastInt32x4(1)
	BaseSinPoly_AVX2_s2_f64              = archsimd.BroadcastFloat64x4(float64(trigS2_f64))
	BaseSinCosPoly_AVX2_intTwo_i32_f64   = archsimd.BroadcastInt32x4(2)
	BaseExpPoly_AVX2_overflow_f32        = archsimd.BroadcastFloat32x8(float32(expOverflow_f32))
	BaseExpPoly_AVX2_c6_f64              = archsimd.BroadcastFloat64x4(float64(expC6_f64))
	BaseAtanPoly_AVX2_piOver4_f64        = archsimd.BroadcastFloat64x4(float64(atanPiOver4_f64))
	BaseAcosPoly_AVX2_p2_f64             = archsimd.BroadcastFloat64x4(float64(asinP2_f64))
	BaseSinhPoly_AVX2_half_f32           = archsimd.BroadcastFloat32x8(float32(0.5))
	BaseCoshPoly_AVX2_half_f64           = archsimd.BroadcastFloat64x4(float64(0.5))
	BaseLog1pPoly_AVX2_zero_f32          = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseLog1pPoly_AVX2_c4_f64            = archsimd.BroadcastFloat64x4(float64(log1pC4_f64))
	BaseAsinPoly_AVX2_zero_f64           = archsimd.BroadcastFloat64x4(float64(asinZero_f64))
	BaseAcosPoly_AVX2_zero_f32           = archsimd.BroadcastFloat32x8(float32(asinZero_f32))
	BaseLog1pPoly_AVX2_negOne_f32        = archsimd.BroadcastFloat32x8(float32(-1.0))
	BaseLogPoly_AVX2_two_f32             = archsimd.BroadcastFloat32x8(float32(logTwo_f32))
	BaseSinCosPoly_AVX2_c3_f64           = archsimd.BroadcastFloat64x4(float64(trigC3_f64))
	BasePowPoly_AVX2_zero_f64            = archsimd.BroadcastFloat64x4(float64(powZero_f64))
	BaseExpPoly_AVX2_c2_f32              = archsimd.BroadcastFloat32x8(float32(expC2_f32))
	BaseExpm1Poly_AVX2_negOne_f32        = archsimd.BroadcastFloat32x8(float32(-1.0))
	BaseExpm1Poly_AVX2_c5_f64            = archsimd.BroadcastFloat64x4(float64(expm1C5_f64))
	BaseLogPoly_AVX2_ln2Lo_f32           = archsimd.BroadcastFloat32x8(float32(logLn2Lo_f32))
	BaseLogPoly_AVX2_ln2Lo_f64           = archsimd.BroadcastFloat64x4(float64(logLn2Lo_f64))
	BaseLog1pPoly_AVX2_one_f64           = archsimd.BroadcastFloat64x4(float64(log1pOne_f64))
	BaseSinCosPoly_AVX2_c1_f32           = archsimd.BroadcastFloat32x8(float32(trigC1_f32))
	BaseSinPoly_AVX2_piOver2Lo_f64       = archsimd.BroadcastFloat64x4(float64(trigPiOver2Lo_f64))
	BaseCosPoly_AVX2_intTwo_i32_f64      = archsimd.BroadcastInt32x4(2)
	BaseAtanPoly_AVX2_c4_f64             = archsimd.BroadcastFloat64x4(float64(atanC4_f64))
	BaseExpm1Poly_AVX2_c3_f32            = archsimd.BroadcastFloat32x8(float32(expm1C3_f32))
	BaseSinCosPoly_AVX2_s3_f64           = archsimd.BroadcastFloat64x4(float64(trigS3_f64))
	BaseSigmoidPoly_AVX2_satHi_f32       = archsimd.BroadcastFloat32x8(float32(20.0))
	BaseCosPoly_AVX2_piOver2Hi_f64       = archsimd.BroadcastFloat64x4(float64(trigPiOver2Hi_f64))
	BaseLogPoly_AVX2_ln2Hi_f32           = archsimd.BroadcastFloat32x8(float32(logLn2Hi_f32))
	BaseLogPoly_AVX2_one_f64             = archsimd.BroadcastFloat64x4(float64(logOne_f64))
	BaseLogPoly_AVX2_two_f64             = archsimd.BroadcastFloat64x4(float64(logTwo_f64))
	BaseSinCosPoly_AVX2_c4_f32           = archsimd.BroadcastFloat32x8(float32(trigC4_f32))
	BaseSinCosPoly_AVX2_piOver2Lo_f64    = archsimd.BroadcastFloat64x4(float64(trigPiOver2Lo_f64))
	BaseSinCosPoly_AVX2_intThree_i32_f64 = archsimd.BroadcastInt32x4(3)
	BaseTanhPoly_AVX2_one_f64            = archsimd.BroadcastFloat64x4(float64(tanhOne_f64))
	BaseSigmoidPoly_AVX2_satLo_f32       = archsimd.BroadcastFloat32x8(float32(-20.0))
	BaseCosPoly_AVX2_c1_f32              = archsimd.BroadcastFloat32x8(float32(trigC1_f32))
	BaseAtanPoly_AVX2_zero_f32           = archsimd.BroadcastFloat32x8(float32(atanZero_f32))
	BaseAtanPoly_AVX2_one_f64            = archsimd.BroadcastFloat64x4(float64(atanOne_f64))
	BaseAsinPoly_AVX2_piOver2_f64        = archsimd.BroadcastFloat64x4(float64(asinPiOver2_f64))
	BaseTanhPoly_AVX2_negOne_f64         = archsimd.BroadcastFloat64x4(float64(tanhNegOne_f64))
	BaseSinPoly_AVX2_s4_f32              = archsimd.BroadcastFloat32x8(float32(trigS4_f32))
	BaseAtanPoly_AVX2_c2_f64             = archsimd.BroadcastFloat64x4(float64(atanC2_f64))
	BaseAcosPoly_AVX2_p3_f32             = archsimd.BroadcastFloat32x8(float32(asinP3_f32))
	BaseLog1pPoly_AVX2_c3_f64            = archsimd.BroadcastFloat64x4(float64(log1pC3_f64))
	BaseSinCosPoly_AVX2_intThree_i32_f32 = archsimd.BroadcastInt32x8(3)
	BaseSigmoidPoly_AVX2_zero_f32        = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseSinPoly_AVX2_intTwo_i32_f32      = archsimd.BroadcastInt32x8(2)
	BaseCosPoly_AVX2_s2_f32              = archsimd.BroadcastFloat32x8(float32(trigS2_f32))
	BaseAsinPoly_AVX2_two_f32            = archsimd.BroadcastFloat32x8(float32(asinTwo_f32))
	BaseExpm1Poly_AVX2_c5_f32            = archsimd.BroadcastFloat32x8(float32(expm1C5_f32))
	BaseLogPoly_AVX2_one_f32             = archsimd.BroadcastFloat32x8(float32(logOne_f32))
	BaseAsinPoly_AVX2_p1_f64             = archsimd.BroadcastFloat64x4(float64(asinP1_f64))
	BaseErfPoly_AVX2_zero_f32            = archsimd.BroadcastFloat32x8(float32(erfZero_f32))
	BaseSinPoly_AVX2_s1_f32              = archsimd.BroadcastFloat32x8(float32(trigS1_f32))
	BaseSinPoly_AVX2_s3_f64              = archsimd.BroadcastFloat64x4(float64(trigS3_f64))
	BaseLogPoly_AVX2_c5_f64              = archsimd.BroadcastFloat64x4(float64(logC5_f64))
	BaseSinCosPoly_AVX2_piOver2Hi_f64    = archsimd.BroadcastFloat64x4(float64(trigPiOver2Hi_f64))
	BaseExpPoly_AVX2_c3_f32              = archsimd.BroadcastFloat32x8(float32(expC3_f32))
	BaseAsinPoly_AVX2_p1_f32             = archsimd.BroadcastFloat32x8(float32(asinP1_f32))
	BaseAtan2Poly_AVX2_pi_f32            = archsimd.BroadcastFloat32x8(float32(atanPi_f32))
	BaseSinCosPoly_AVX2_one_f64          = archsimd.BroadcastFloat64x4(float64(trigOne_f64))
	BaseAsinhPoly_AVX2_one_f64           = archsimd.BroadcastFloat64x4(float64(1.0))
	BaseCosPoly_AVX2_twoOverPi_f32       = archsimd.BroadcastFloat32x8(float32(trig2OverPi_f32))
	BaseCosPoly_AVX2_c3_f64              = archsimd.BroadcastFloat64x4(float64(trigC3_f64))
	BaseAsinPoly_AVX2_p4_f32             = archsimd.BroadcastFloat32x8(float32(asinP4_f32))
	BaseExpm1Poly_AVX2_c4_f64            = archsimd.BroadcastFloat64x4(float64(expm1C4_f64))
	BaseTanhPoly_AVX2_threshold_f64      = archsimd.BroadcastFloat64x4(float64(tanhClamp_f64))
	BaseSinPoly_AVX2_s3_f32              = archsimd.BroadcastFloat32x8(float32(trigS3_f32))
	BaseLogPoly_AVX2_c4_f64              = archsimd.BroadcastFloat64x4(float64(logC4_f64))
	BaseSinPoly_AVX2_c4_f64              = archsimd.BroadcastFloat64x4(float64(trigC4_f64))
	BaseCosPoly_AVX2_twoOverPi_f64       = archsimd.BroadcastFloat64x4(float64(trig2OverPi_f64))
	BaseAtanPoly_AVX2_tanPiOver8_f32     = archsimd.BroadcastFloat32x8(float32(atanTanPiOver8_f32))
	BaseAsinPoly_AVX2_p2_f64             = archsimd.BroadcastFloat64x4(float64(asinP2_f64))
	BaseAcosPoly_AVX2_half_f32           = archsimd.BroadcastFloat32x8(float32(asinHalf_f32))
	BaseAtanhPoly_AVX2_one_f64           = archsimd.BroadcastFloat64x4(float64(1.0))
	BaseCosPoly_AVX2_c1_f64              = archsimd.BroadcastFloat64x4(float64(trigC1_f64))
	BaseAtanPoly_AVX2_c2_f32             = archsimd.BroadcastFloat32x8(float32(atanC2_f32))
	BaseExp10Poly_AVX2_ln10_f32_f32      = archsimd.BroadcastFloat32x8(float32(ln10_f32))
	BaseAtanhPoly_AVX2_half_f64          = archsimd.BroadcastFloat64x4(float64(0.5))
	BaseSigmoidPoly_AVX2_satLo_f64       = archsimd.BroadcastFloat64x4(float64(-20.0))
	BaseAcosPoly_AVX2_p5_f32             = archsimd.BroadcastFloat32x8(float32(asinP5_f32))
	BaseErfPoly_AVX2_a1_f32              = archsimd.BroadcastFloat32x8(float32(erfA1_f32))
	BaseLog1pPoly_AVX2_negOne_f64        = archsimd.BroadcastFloat64x4(float64(-1.0))
	BasePowPoly_AVX2_zero_f32            = archsimd.BroadcastFloat32x8(float32(powZero_f32))
	BaseSinPoly_AVX2_one_f32             = archsimd.BroadcastFloat32x8(float32(trigOne_f32))
	BaseCosPoly_AVX2_c2_f64              = archsimd.BroadcastFloat64x4(float64(trigC2_f64))
	BaseAcosPoly_AVX2_half_f64           = archsimd.BroadcastFloat64x4(float64(asinHalf_f64))
	BaseExpm1Poly_AVX2_negOne_f64        = archsimd.BroadcastFloat64x4(float64(-1.0))
	BaseLog1pPoly_AVX2_c2_f32            = archsimd.BroadcastFloat32x8(float32(log1pC2_f32))
	BaseLogPoly_AVX2_c2_f32              = archsimd.BroadcastFloat32x8(float32(logC2_f32))
	BaseSinPoly_AVX2_c2_f32              = archsimd.BroadcastFloat32x8(float32(trigC2_f32))
	BaseSinPoly_AVX2_one_f64             = archsimd.BroadcastFloat64x4(float64(trigOne_f64))
	BaseAcosPoly_AVX2_p3_f64             = archsimd.BroadcastFloat64x4(float64(asinP3_f64))
	BaseExpm1Poly_AVX2_threshold_f32     = archsimd.BroadcastFloat32x8(float32(expm1Threshold_f32))
	BaseSinCosPoly_AVX2_twoOverPi_f32    = archsimd.BroadcastFloat32x8(float32(trig2OverPi_f32))
	BaseCbrtPoly_AVX2_zero_f32           = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseSinPoly_AVX2_s4_f64              = archsimd.BroadcastFloat64x4(float64(trigS4_f64))
	BaseLog1pPoly_AVX2_threshold_f64     = archsimd.BroadcastFloat64x4(float64(log1pHalf_f64))
	BaseLog1pPoly_AVX2_c2_f64            = archsimd.BroadcastFloat64x4(float64(log1pC2_f64))
	BaseLogPoly_AVX2_zero_f32            = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseSinCosPoly_AVX2_one_f32          = archsimd.BroadcastFloat32x8(float32(trigOne_f32))
	BaseSinCosPoly_AVX2_s4_f64           = archsimd.BroadcastFloat64x4(float64(trigS4_f64))
	BaseExp10Poly_AVX2_ln10_f64_f64      = archsimd.BroadcastFloat64x4(float64(ln10_f64))
	BaseExpPoly_AVX2_zero_f64            = archsimd.BroadcastFloat64x4(float64(expZero_f64))
	BaseAtanPoly_AVX2_c3_f32             = archsimd.BroadcastFloat32x8(float32(atanC3_f32))
	BaseAtanPoly_AVX2_c5_f64             = archsimd.BroadcastFloat64x4(float64(atanC5_f64))
	BaseHypotPoly_AVX2_one_f64           = archsimd.BroadcastFloat64x4(float64(1.0))
	BaseAcosPoly_AVX2_two_f32            = archsimd.BroadcastFloat32x8(float32(asinTwo_f32))
	BaseSigmoidPoly_AVX2_zero_f64        = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseExpPoly_AVX2_underflow_f64       = archsimd.BroadcastFloat64x4(float64(expUnderflow_f64))
	BaseCosPoly_AVX2_piOver2Lo_f32       = archsimd.BroadcastFloat32x8(float32(trigPiOver2Lo_f32))
	BaseCosPoly_AVX2_c4_f32              = archsimd.BroadcastFloat32x8(float32(trigC4_f32))
	BaseAtanPoly_AVX2_c5_f32             = archsimd.BroadcastFloat32x8(float32(atanC5_f32))
	BaseSinCosPoly_AVX2_s1_f64           = archsimd.BroadcastFloat64x4(float64(trigS1_f64))
	BaseAtanhPoly_AVX2_zero_f64          = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseErfPoly_AVX2_one_f64             = archsimd.BroadcastFloat64x4(float64(erfOne_f64))
	BaseExpPoly_AVX2_c5_f64              = archsimd.BroadcastFloat64x4(float64(expC5_f64))
	BaseSinPoly_AVX2_c3_f32              = archsimd.BroadcastFloat32x8(float32(trigC3_f32))
	BaseAsinPoly_AVX2_p2_f32             = archsimd.BroadcastFloat32x8(float32(asinP2_f32))
	BaseAtan2Poly_AVX2_piOver2_f64       = archsimd.BroadcastFloat64x4(float64(atanPiOver2_f64))
	BaseAcoshPoly_AVX2_one_f64           = archsimd.BroadcastFloat64x4(float64(1.0))
	BaseExpPoly_AVX2_ln2Hi_f32           = archsimd.BroadcastFloat32x8(float32(expLn2Hi_f32))
	BaseSigmoidPoly_AVX2_one_f32         = archsimd.BroadcastFloat32x8(float32(sigmoidOne_f32))
	BaseSinPoly_AVX2_intOne_i32_f32      = archsimd.BroadcastInt32x8(1)
	BaseSinPoly_AVX2_piOver2Hi_f32       = archsimd.BroadcastFloat32x8(float32(trigPiOver2Hi_f32))
	BaseSinPoly_AVX2_intThree_i32_f64    = archsimd.BroadcastInt32x4(3)
	BaseLogPoly_AVX2_c3_f32              = archsimd.BroadcastFloat32x8(float32(logC3_f32))
	BaseSinPoly_AVX2_s1_f64              = archsimd.BroadcastFloat64x4(float64(trigS1_f64))
	BaseAtanPoly_AVX2_piOver2_f32        = archsimd.BroadcastFloat32x8(float32(atanPiOver2_f32))
	BaseAcosPoly_AVX2_p6_f32             = archsimd.BroadcastFloat32x8(float32(asinP6_f32))
	BaseErfPoly_AVX2_one_f32             = archsimd.BroadcastFloat32x8(float32(erfOne_f32))
	BaseErfPoly_AVX2_a4_f64              = archsimd.BroadcastFloat64x4(float64(erfA4_f64))
	BaseExpm1Poly_AVX2_one_f32           = archsimd.BroadcastFloat32x8(float32(expm1One_f32))
	BaseLog1pPoly_AVX2_c4_f32            = archsimd.BroadcastFloat32x8(float32(log1pC4_f32))
	BaseAtanhPoly_AVX2_half_f32          = archsimd.BroadcastFloat32x8(float32(0.5))
	BaseTanhPoly_AVX2_one_f32            = archsimd.BroadcastFloat32x8(float32(tanhOne_f32))
	BaseAtanPoly_AVX2_one_f32            = archsimd.BroadcastFloat32x8(float32(atanOne_f32))
	BaseSinCosPoly_AVX2_s4_f32           = archsimd.BroadcastFloat32x8(float32(trigS4_f32))
	BaseExpPoly_AVX2_c4_f64              = archsimd.BroadcastFloat64x4(float64(expC4_f64))
	BaseTanhPoly_AVX2_negOne_f32         = archsimd.BroadcastFloat32x8(float32(tanhNegOne_f32))
	BaseErfPoly_AVX2_a3_f32              = archsimd.BroadcastFloat32x8(float32(erfA3_f32))
	BaseAtanhPoly_AVX2_zero_f32          = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseAsinPoly_AVX2_p5_f64             = archsimd.BroadcastFloat64x4(float64(asinP5_f64))
	BaseAsinPoly_AVX2_p4_f64             = archsimd.BroadcastFloat64x4(float64(asinP4_f64))
	BaseAcosPoly_AVX2_p6_f64             = archsimd.BroadcastFloat64x4(float64(asinP6_f64))
	BaseAcosPoly_AVX2_piOver2_f64        = archsimd.BroadcastFloat64x4(float64(asinPiOver2_f64))
	BaseCosPoly_AVX2_intOne_i32_f64      = archsimd.BroadcastInt32x4(1)
	BaseAtanPoly_AVX2_c4_f32             = archsimd.BroadcastFloat32x8(float32(atanC4_f32))
	BaseSinPoly_AVX2_intThree_i32_f32    = archsimd.BroadcastInt32x8(3)
	BaseExpPoly_AVX2_ln2Lo_f64           = archsimd.BroadcastFloat64x4(float64(expLn2Lo_f64))
	BaseExpPoly_AVX2_ln2Lo_f32           = archsimd.BroadcastFloat32x8(float32(expLn2Lo_f32))
	BaseTanhPoly_AVX2_two_f64            = archsimd.BroadcastFloat64x4(float64(2.0))
	BaseCosPoly_AVX2_s2_f64              = archsimd.BroadcastFloat64x4(float64(trigS2_f64))
	BaseTanhPoly_AVX2_two_f32            = archsimd.BroadcastFloat32x8(float32(2.0))
	BaseSinhPoly_AVX2_half_f64           = archsimd.BroadcastFloat64x4(float64(0.5))
	BaseCoshPoly_AVX2_half_f32           = archsimd.BroadcastFloat32x8(float32(0.5))
	BaseExpm1Poly_AVX2_c2_f64            = archsimd.BroadcastFloat64x4(float64(expm1C2_f64))
	BaseAtan2Poly_AVX2_zero_f32          = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseSinCosPoly_AVX2_s2_f64           = archsimd.BroadcastFloat64x4(float64(trigS2_f64))
	BaseExpPoly_AVX2_overflow_f64        = archsimd.BroadcastFloat64x4(float64(expOverflow_f64))
	BaseAsinPoly_AVX2_p5_f32             = archsimd.BroadcastFloat32x8(float32(asinP5_f32))
	BaseAcoshPoly_AVX2_zero_f32          = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseSinPoly_AVX2_intTwo_i32_f64      = archsimd.BroadcastInt32x4(2)
	BaseAtanPoly_AVX2_c1_f32             = archsimd.BroadcastFloat32x8(float32(atanC1_f32))
	BaseAsinPoly_AVX2_one_f64            = archsimd.BroadcastFloat64x4(float64(asinOne_f64))
	BaseExpm1Poly_AVX2_c2_f32            = archsimd.BroadcastFloat32x8(float32(expm1C2_f32))
	BaseAtan2Poly_AVX2_pi_f64            = archsimd.BroadcastFloat64x4(float64(atanPi_f64))
	BaseSinCosPoly_AVX2_s1_f32           = archsimd.BroadcastFloat32x8(float32(trigS1_f32))
	BaseSinCosPoly_AVX2_s3_f32           = archsimd.BroadcastFloat32x8(float32(trigS3_f32))
	BaseSinCosPoly_AVX2_s2_f32           = archsimd.BroadcastFloat32x8(float32(trigS2_f32))
	BaseExpPoly_AVX2_c2_f64              = archsimd.BroadcastFloat64x4(float64(expC2_f64))
	BaseSinPoly_AVX2_twoOverPi_f64       = archsimd.BroadcastFloat64x4(float64(trig2OverPi_f64))
	BaseAsinPoly_AVX2_p3_f64             = archsimd.BroadcastFloat64x4(float64(asinP3_f64))
	BaseSinCosPoly_AVX2_intOne_i32_f64   = archsimd.BroadcastInt32x4(1)
	BaseSinCosPoly_AVX2_twoOverPi_f64    = archsimd.BroadcastFloat64x4(float64(trig2OverPi_f64))
	BaseHypotPoly_AVX2_zero_f64          = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseExpPoly_AVX2_one_f32             = archsimd.BroadcastFloat32x8(float32(expOne_f32))
	BaseSinPoly_AVX2_c1_f32              = archsimd.BroadcastFloat32x8(float32(trigC1_f32))
	BaseLog10Poly_AVX2_log10E_f64_f64    = archsimd.BroadcastFloat64x4(float64(log10E_f64))
	BaseExpPoly_AVX2_c4_f32              = archsimd.BroadcastFloat32x8(float32(expC4_f32))
	BaseSinPoly_AVX2_twoOverPi_f32       = archsimd.BroadcastFloat32x8(float32(trig2OverPi_f32))
	BaseSinPoly_AVX2_s2_f32              = archsimd.BroadcastFloat32x8(float32(trigS2_f32))
	BaseCosPoly_AVX2_intTwo_i32_f32      = archsimd.BroadcastInt32x8(2)
	BaseAsinPoly_AVX2_piOver2_f32        = archsimd.BroadcastFloat32x8(float32(asinPiOver2_f32))
	BaseExpPoly_AVX2_one_f64             = archsimd.BroadcastFloat64x4(float64(expOne_f64))
	BaseCosPoly_AVX2_s4_f32              = archsimd.BroadcastFloat32x8(float32(trigS4_f32))
	BaseErfPoly_AVX2_zero_f64            = archsimd.BroadcastFloat64x4(float64(erfZero_f64))
	BaseLogPoly_AVX2_c1_f32              = archsimd.BroadcastFloat32x8(float32(logC1_f32))
	BaseHypotPoly_AVX2_one_f32           = archsimd.BroadcastFloat32x8(float32(1.0))
	BaseCosPoly_AVX2_one_f32             = archsimd.BroadcastFloat32x8(float32(trigOne_f32))
	BaseCosPoly_AVX2_s1_f32              = archsimd.BroadcastFloat32x8(float32(trigS1_f32))
	BaseCosPoly_AVX2_s3_f64              = archsimd.BroadcastFloat64x4(float64(trigS3_f64))
	BaseAcosPoly_AVX2_p2_f32             = archsimd.BroadcastFloat32x8(float32(asinP2_f32))
	BaseExpm1Poly_AVX2_c4_f32            = archsimd.BroadcastFloat32x8(float32(expm1C4_f32))
	BaseSinCosPoly_AVX2_intTwo_i32_f32   = archsimd.BroadcastInt32x8(2)
	BaseExpPoly_AVX2_c5_f32              = archsimd.BroadcastFloat32x8(float32(expC5_f32))
	BaseSinPoly_AVX2_piOver2Hi_f64       = archsimd.BroadcastFloat64x4(float64(trigPiOver2Hi_f64))
	BaseAcosPoly_AVX2_two_f64            = archsimd.BroadcastFloat64x4(float64(asinTwo_f64))
	BaseExpm1Poly_AVX2_one_f64           = archsimd.BroadcastFloat64x4(float64(expm1One_f64))
	BaseHypotPoly_AVX2_zero_f32          = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseAsinPoly_AVX2_p6_f32             = archsimd.BroadcastFloat32x8(float32(asinP6_f32))
	BaseAcosPoly_AVX2_one_f32            = archsimd.BroadcastFloat32x8(float32(asinOne_f32))
	BaseAtan2Poly_AVX2_zero_f64          = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseSinCosPoly_AVX2_piOver2Hi_f32    = archsimd.BroadcastFloat32x8(float32(trigPiOver2Hi_f32))
	BaseExpPoly_AVX2_invLn2_f32          = archsimd.BroadcastFloat32x8(float32(expInvLn2_f32))
	BaseCosPoly_AVX2_one_f64             = archsimd.BroadcastFloat64x4(float64(trigOne_f64))
	BaseAcoshPoly_AVX2_one_f32           = archsimd.BroadcastFloat32x8(float32(1.0))
	BaseAcoshPoly_AVX2_zero_f64          = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseCosPoly_AVX2_s3_f32              = archsimd.BroadcastFloat32x8(float32(trigS3_f32))
	BaseAsinPoly_AVX2_half_f64           = archsimd.BroadcastFloat64x4(float64(asinHalf_f64))
	BasePowPoly_AVX2_one_f32             = archsimd.BroadcastFloat32x8(float32(powOne_f32))
	BaseSinPoly_AVX2_c3_f64              = archsimd.BroadcastFloat64x4(float64(trigC3_f64))
	BaseLogPoly_AVX2_c1_f64              = archsimd.BroadcastFloat64x4(float64(logC1_f64))
	BaseLog2Poly_AVX2_log2E_f32_f32      = archsimd.BroadcastFloat32x8(float32(log2E_f32))
	BaseSinPoly_AVX2_c1_f64              = archsimd.BroadcastFloat64x4(float64(trigC1_f64))
	BaseCosPoly_AVX2_piOver2Lo_f64       = archsimd.BroadcastFloat64x4(float64(trigPiOver2Lo_f64))
	BaseErfPoly_AVX2_a5_f64              = archsimd.BroadcastFloat64x4(float64(erfA5_f64))
	BaseErfPoly_AVX2_a1_f64              = archsimd.BroadcastFloat64x4(float64(erfA1_f64))
	BaseLogPoly_AVX2_c3_f64              = archsimd.BroadcastFloat64x4(float64(logC3_f64))
	BaseCosPoly_AVX2_c4_f64              = archsimd.BroadcastFloat64x4(float64(trigC4_f64))
	BaseAsinPoly_AVX2_p6_f64             = archsimd.BroadcastFloat64x4(float64(asinP6_f64))
	BaseAcosPoly_AVX2_p4_f64             = archsimd.BroadcastFloat64x4(float64(asinP4_f64))
	BaseAcosPoly_AVX2_p5_f64             = archsimd.BroadcastFloat64x4(float64(asinP5_f64))
	BaseLogPoly_AVX2_ln2Hi_f64           = archsimd.BroadcastFloat64x4(float64(logLn2Hi_f64))
	BasePowPoly_AVX2_one_f64             = archsimd.BroadcastFloat64x4(float64(powOne_f64))
	BaseCbrtPoly_AVX2_zero_f64           = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseTanhPoly_AVX2_threshold_f32      = archsimd.BroadcastFloat32x8(float32(tanhClamp_f32))
	BaseSinPoly_AVX2_c2_f64              = archsimd.BroadcastFloat64x4(float64(trigC2_f64))
	BaseAtanPoly_AVX2_tanPiOver8_f64     = archsimd.BroadcastFloat64x4(float64(atanTanPiOver8_f64))
	BaseErfPoly_AVX2_a4_f32              = archsimd.BroadcastFloat32x8(float32(erfA4_f32))
	BaseLog1pPoly_AVX2_one_f32           = archsimd.BroadcastFloat32x8(float32(log1pOne_f32))
	BaseLog1pPoly_AVX2_c3_f32            = archsimd.BroadcastFloat32x8(float32(log1pC3_f32))
	BaseExp2Poly_AVX2_ln2_f64_f64        = archsimd.BroadcastFloat64x4(float64(ln2_f64))
	BaseExpPoly_AVX2_ln2Hi_f64           = archsimd.BroadcastFloat64x4(float64(expLn2Hi_f64))
	BaseCosPoly_AVX2_intThree_i32_f64    = archsimd.BroadcastInt32x4(3)
	BaseExpm1Poly_AVX2_zero_f64          = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseCosPoly_AVX2_s4_f64              = archsimd.BroadcastFloat64x4(float64(trigS4_f64))
	BaseAtanPoly_AVX2_piOver4_f32        = archsimd.BroadcastFloat32x8(float32(atanPiOver4_f32))
	BaseLog1pPoly_AVX2_c5_f64            = archsimd.BroadcastFloat64x4(float64(log1pC5_f64))
	BaseExpPoly_AVX2_c6_f32              = archsimd.BroadcastFloat32x8(float32(expC6_f32))
	BaseSigmoidPoly_AVX2_one_f64         = archsimd.BroadcastFloat64x4(float64(sigmoidOne_f64))
	BaseCosPoly_AVX2_c2_f32              = archsimd.BroadcastFloat32x8(float32(trigC2_f32))
	BaseAsinPoly_AVX2_half_f32           = archsimd.BroadcastFloat32x8(float32(asinHalf_f32))
	BaseAcosPoly_AVX2_one_f64            = archsimd.BroadcastFloat64x4(float64(asinOne_f64))
	BaseErfPoly_AVX2_a2_f64              = archsimd.BroadcastFloat64x4(float64(erfA2_f64))
	BaseLogPoly_AVX2_c2_f64              = archsimd.BroadcastFloat64x4(float64(logC2_f64))
	BaseCosPoly_AVX2_intThree_i32_f32    = archsimd.BroadcastInt32x8(3)
	BaseErfPoly_AVX2_p_f32               = archsimd.BroadcastFloat32x8(float32(erfP_f32))
	BaseLog1pPoly_AVX2_c5_f32            = archsimd.BroadcastFloat32x8(float32(log1pC5_f32))
	BaseSinCosPoly_AVX2_c2_f32           = archsimd.BroadcastFloat32x8(float32(trigC2_f32))
	BaseExpm1Poly_AVX2_c3_f64            = archsimd.BroadcastFloat64x4(float64(expm1C3_f64))
	BaseLog1pPoly_AVX2_zero_f64          = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseLogPoly_AVX2_c4_f32              = archsimd.BroadcastFloat32x8(float32(logC4_f32))
	BaseSinCosPoly_AVX2_c1_f64           = archsimd.BroadcastFloat64x4(float64(trigC1_f64))
	BaseSinPoly_AVX2_c4_f32              = archsimd.BroadcastFloat32x8(float32(trigC4_f32))
	BaseAtanPoly_AVX2_c1_f64             = archsimd.BroadcastFloat64x4(float64(atanC1_f64))
	BaseLog1pPoly_AVX2_threshold_f32     = archsimd.BroadcastFloat32x8(float32(log1pHalf_f32))
	BaseExpPoly_AVX2_c1_f32              = archsimd.BroadcastFloat32x8(float32(expC1_f32))
	BaseCosPoly_AVX2_piOver2Hi_f32       = archsimd.BroadcastFloat32x8(float32(trigPiOver2Hi_f32))
	BaseAtanPoly_AVX2_c3_f64             = archsimd.BroadcastFloat64x4(float64(atanC3_f64))
	BaseAsinPoly_AVX2_p3_f32             = archsimd.BroadcastFloat32x8(float32(asinP3_f32))
	BaseErfPoly_AVX2_a5_f32              = archsimd.BroadcastFloat32x8(float32(erfA5_f32))
	BaseExpm1Poly_AVX2_threshold_f64     = archsimd.BroadcastFloat64x4(float64(expm1Threshold_f64))
	BaseSinCosPoly_AVX2_piOver2Lo_f32    = archsimd.BroadcastFloat32x8(float32(trigPiOver2Lo_f32))
	BaseExpPoly_AVX2_underflow_f32       = archsimd.BroadcastFloat32x8(float32(expUnderflow_f32))
	BaseSigmoidPoly_AVX2_satHi_f64       = archsimd.BroadcastFloat64x4(float64(20.0))
	BaseCosPoly_AVX2_c3_f32              = archsimd.BroadcastFloat32x8(float32(trigC3_f32))
	BaseAtanPoly_AVX2_zero_f64           = archsimd.BroadcastFloat64x4(float64(atanZero_f64))
	BaseAtanPoly_AVX2_piOver2_f64        = archsimd.BroadcastFloat64x4(float64(atanPiOver2_f64))
	BaseAsinPoly_AVX2_two_f64            = archsimd.BroadcastFloat64x4(float64(asinTwo_f64))
	BaseLog2Poly_AVX2_log2E_f64_f64      = archsimd.BroadcastFloat64x4(float64(log2E_f64))
	BaseSinCosPoly_AVX2_c3_f32           = archsimd.BroadcastFloat32x8(float32(trigC3_f32))
	BaseAcosPoly_AVX2_p1_f64             = archsimd.BroadcastFloat64x4(float64(asinP1_f64))
	BaseErfPoly_AVX2_p_f64               = archsimd.BroadcastFloat64x4(float64(erfP_f64))
	BaseErfPoly_AVX2_a3_f64              = archsimd.BroadcastFloat64x4(float64(erfA3_f64))
	BaseLogPoly_AVX2_c5_f32              = archsimd.BroadcastFloat32x8(float32(logC5_f32))
	BaseSinCosPoly_AVX2_c4_f64           = archsimd.BroadcastFloat64x4(float64(trigC4_f64))
	BaseExp2Poly_AVX2_ln2_f32_f32        = archsimd.BroadcastFloat32x8(float32(ln2_f32))
	BaseAsinhPoly_AVX2_one_f32           = archsimd.BroadcastFloat32x8(float32(1.0))
	BaseExpPoly_AVX2_zero_f32            = archsimd.BroadcastFloat32x8(float32(expZero_f32))
	BaseSinPoly_AVX2_piOver2Lo_f32       = archsimd.BroadcastFloat32x8(float32(trigPiOver2Lo_f32))
	BaseCosPoly_AVX2_s1_f64              = archsimd.BroadcastFloat64x4(float64(trigS1_f64))
	BaseSinCosPoly_AVX2_intOne_i32_f32   = archsimd.BroadcastInt32x8(1)
	BaseSinCosPoly_AVX2_c2_f64           = archsimd.BroadcastFloat64x4(float64(trigC2_f64))
	BaseExpPoly_AVX2_invLn2_f64          = archsimd.BroadcastFloat64x4(float64(expInvLn2_f64))
	BaseCosPoly_AVX2_intOne_i32_f32      = archsimd.BroadcastInt32x8(1)
	BaseAsinPoly_AVX2_one_f32            = archsimd.BroadcastFloat32x8(float32(asinOne_f32))
	BaseAcosPoly_AVX2_p1_f32             = archsimd.BroadcastFloat32x8(float32(asinP1_f32))
	BaseExpm1Poly_AVX2_zero_f32          = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseLogPoly_AVX2_zero_f64            = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseErfPoly_AVX2_a2_f32              = archsimd.BroadcastFloat32x8(float32(erfA2_f32))
	BaseLog10Poly_AVX2_log10E_f32_f32    = archsimd.BroadcastFloat32x8(float32(log10E_f32))
	BaseAtanhPoly_AVX2_one_f32           = archsimd.BroadcastFloat32x8(float32(1.0))
)

func BaseExpPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	overflow := BaseExpPoly_AVX2_overflow_f32
	underflow := BaseExpPoly_AVX2_underflow_f32
	one := BaseExpPoly_AVX2_one_f32
	zero := BaseExpPoly_AVX2_zero_f32
	inf := archsimd.BroadcastFloat32x8(float32(expOverflow_f32 * 2))
	invLn2 := BaseExpPoly_AVX2_invLn2_f32
	ln2Hi := BaseExpPoly_AVX2_ln2Hi_f32
	ln2Lo := BaseExpPoly_AVX2_ln2Lo_f32
	c1 := BaseExpPoly_AVX2_c1_f32
	c2 := BaseExpPoly_AVX2_c2_f32
	c3 := BaseExpPoly_AVX2_c3_f32
	c4 := BaseExpPoly_AVX2_c4_f32
	c5 := BaseExpPoly_AVX2_c5_f32
	c6 := BaseExpPoly_AVX2_c6_f32
	lanes := 8
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		overflowMask := x.Greater(overflow)
		underflowMask := x.Less(underflow)
		kFloat := x.Mul(invLn2).RoundToEven()
		r := x.Sub(kFloat.Mul(ln2Hi))
		r = r.Sub(kFloat.Mul(ln2Lo))
		p := c6.MulAdd(r, c5)
		p = p.MulAdd(r, c4)
		p = p.MulAdd(r, c3)
		p = p.MulAdd(r, c2)
		p = p.MulAdd(r, c1)
		p = p.MulAdd(r, one)
		kInt := kFloat.ConvertToInt32()
		scale := hwy.Pow2_AVX2_F32x8(kInt)
		result := p.Mul(scale)
		result = inf.Merge(result, overflowMask)
		result = zero.Merge(result, underflowMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseExpPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseExpPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	overflow := BaseExpPoly_AVX2_overflow_f64
	underflow := BaseExpPoly_AVX2_underflow_f64
	one := BaseExpPoly_AVX2_one_f64
	zero := BaseExpPoly_AVX2_zero_f64
	inf := archsimd.BroadcastFloat64x4(float64(expOverflow_f64 * 2))
	invLn2 := BaseExpPoly_AVX2_invLn2_f64
	ln2Hi := BaseExpPoly_AVX2_ln2Hi_f64
	ln2Lo := BaseExpPoly_AVX2_ln2Lo_f64
	c1 := BaseExpPoly_AVX2_c1_f64
	c2 := BaseExpPoly_AVX2_c2_f64
	c3 := BaseExpPoly_AVX2_c3_f64
	c4 := BaseExpPoly_AVX2_c4_f64
	c5 := BaseExpPoly_AVX2_c5_f64
	c6 := BaseExpPoly_AVX2_c6_f64
	lanes := 4
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		overflowMask := x.Greater(overflow)
		underflowMask := x.Less(underflow)
		kFloat := x.Mul(invLn2).RoundToEven()
		r := x.Sub(kFloat.Mul(ln2Hi))
		r = r.Sub(kFloat.Mul(ln2Lo))
		p := c6.MulAdd(r, c5)
		p = p.MulAdd(r, c4)
		p = p.MulAdd(r, c3)
		p = p.MulAdd(r, c2)
		p = p.MulAdd(r, c1)
		p = p.MulAdd(r, one)
		kInt := kFloat.ConvertToInt32()
		scale := hwy.Pow2_AVX2_F64x4(kInt)
		result := p.Mul(scale)
		result = inf.Merge(result, overflowMask)
		result = zero.Merge(result, underflowMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseExpPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseTanhPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	two := BaseTanhPoly_AVX2_two_f32
	one := BaseTanhPoly_AVX2_one_f32
	negOne := BaseTanhPoly_AVX2_negOne_f32
	threshold := BaseTanhPoly_AVX2_threshold_f32
	negThreshold := archsimd.BroadcastFloat32x8(0).Sub(threshold)
	lanes := 8
	tempIn := [8]float32{}
	tempOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		twoX := two.Mul(x)
		twoX.StoreSlice(tempIn[:])
		BaseSigmoidPoly_avx2(tempIn[:], tempOut[:])
		sigTwoX := archsimd.LoadFloat32x8Slice(tempOut[:])
		result := two.Mul(sigTwoX).Sub(one)
		result = one.Merge(result, x.Greater(threshold))
		result = negOne.Merge(result, x.Less(negThreshold))
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseTanhPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseTanhPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	two := BaseTanhPoly_AVX2_two_f64
	one := BaseTanhPoly_AVX2_one_f64
	negOne := BaseTanhPoly_AVX2_negOne_f64
	threshold := BaseTanhPoly_AVX2_threshold_f64
	negThreshold := archsimd.BroadcastFloat64x4(0).Sub(threshold)
	lanes := 4
	tempIn := [4]float64{}
	tempOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		twoX := two.Mul(x)
		twoX.StoreSlice(tempIn[:])
		BaseSigmoidPoly_avx2_Float64(tempIn[:], tempOut[:])
		sigTwoX := archsimd.LoadFloat64x4Slice(tempOut[:])
		result := two.Mul(sigTwoX).Sub(one)
		result = one.Merge(result, x.Greater(threshold))
		result = negOne.Merge(result, x.Less(negThreshold))
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseTanhPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseSigmoidPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseSigmoidPoly_AVX2_one_f32
	zero := BaseSigmoidPoly_AVX2_zero_f32
	satHi := BaseSigmoidPoly_AVX2_satHi_f32
	satLo := BaseSigmoidPoly_AVX2_satLo_f32
	lanes := 8
	tempIn := [8]float32{}
	tempOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		clampedX := x.Min(satHi).Max(satLo)
		negX := archsimd.BroadcastFloat32x8(0).Sub(clampedX)
		negX.StoreSlice(tempIn[:])
		BaseExpPoly_avx2(tempIn[:], tempOut[:])
		expNegX := archsimd.LoadFloat32x8Slice(tempOut[:])
		result := one.Div(one.Add(expNegX))
		result = one.Merge(result, x.Greater(satHi))
		result = zero.Merge(result, x.Less(satLo))
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseSigmoidPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseSigmoidPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseSigmoidPoly_AVX2_one_f64
	zero := BaseSigmoidPoly_AVX2_zero_f64
	satHi := BaseSigmoidPoly_AVX2_satHi_f64
	satLo := BaseSigmoidPoly_AVX2_satLo_f64
	lanes := 4
	tempIn := [4]float64{}
	tempOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		clampedX := x.Min(satHi).Max(satLo)
		negX := archsimd.BroadcastFloat64x4(0).Sub(clampedX)
		negX.StoreSlice(tempIn[:])
		BaseExpPoly_avx2_Float64(tempIn[:], tempOut[:])
		expNegX := archsimd.LoadFloat64x4Slice(tempOut[:])
		result := one.Div(one.Add(expNegX))
		result = one.Merge(result, x.Greater(satHi))
		result = zero.Merge(result, x.Less(satLo))
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseSigmoidPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseSinPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	twoOverPi := BaseSinPoly_AVX2_twoOverPi_f32
	piOver2Hi := BaseSinPoly_AVX2_piOver2Hi_f32
	piOver2Lo := BaseSinPoly_AVX2_piOver2Lo_f32
	one := BaseSinPoly_AVX2_one_f32
	s1 := BaseSinPoly_AVX2_s1_f32
	s2 := BaseSinPoly_AVX2_s2_f32
	s3 := BaseSinPoly_AVX2_s3_f32
	s4 := BaseSinPoly_AVX2_s4_f32
	c1 := BaseSinPoly_AVX2_c1_f32
	c2 := BaseSinPoly_AVX2_c2_f32
	c3 := BaseSinPoly_AVX2_c3_f32
	c4 := BaseSinPoly_AVX2_c4_f32
	intOne := BaseSinPoly_AVX2_intOne_i32_f32
	intTwo := BaseSinPoly_AVX2_intTwo_i32_f32
	intThree := BaseSinPoly_AVX2_intThree_i32_f32
	lanes := 8
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		kFloat := x.Mul(twoOverPi).RoundToEven()
		kInt := kFloat.ConvertToInt32()
		r := x.Sub(kFloat.Mul(piOver2Hi))
		r = r.Sub(kFloat.Mul(piOver2Lo))
		r2 := r.Mul(r)
		sinPoly := s4.MulAdd(r2, s3)
		sinPoly = sinPoly.MulAdd(r2, s2)
		sinPoly = sinPoly.MulAdd(r2, s1)
		sinPoly = sinPoly.MulAdd(r2, one)
		sinR := r.Mul(sinPoly)
		cosPoly := c4.MulAdd(r2, c3)
		cosPoly = cosPoly.MulAdd(r2, c2)
		cosPoly = cosPoly.MulAdd(r2, c1)
		cosR := cosPoly.MulAdd(r2, one)
		octant := kInt.And(intThree)
		useCosMask := octant.And(intOne).Equal(intOne)
		negateMask := octant.And(intTwo).Equal(intTwo)
		sinRData := sinR.Data()
		cosRData := cosR.Data()
		resultData := make([]float32, len(sinRData))
		for i := range sinRData {
			if useCosMask.GetBit(i) {
				resultData[i] = cosRData[i]
			} else {
				resultData[i] = sinRData[i]
			}
		}
		result := archsimd.LoadFloat32x8Slice(resultData)
		negResult := archsimd.BroadcastFloat32x8(0).Sub(result)
		negResultData := negResult.Data()
		for i := range resultData {
			if negateMask.GetBit(i) {
				resultData[i] = negResultData[i]
			}
		}
		result = archsimd.LoadFloat32x8Slice(resultData)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseSinPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseSinPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	twoOverPi := BaseSinPoly_AVX2_twoOverPi_f64
	piOver2Hi := BaseSinPoly_AVX2_piOver2Hi_f64
	piOver2Lo := BaseSinPoly_AVX2_piOver2Lo_f64
	one := BaseSinPoly_AVX2_one_f64
	s1 := BaseSinPoly_AVX2_s1_f64
	s2 := BaseSinPoly_AVX2_s2_f64
	s3 := BaseSinPoly_AVX2_s3_f64
	s4 := BaseSinPoly_AVX2_s4_f64
	c1 := BaseSinPoly_AVX2_c1_f64
	c2 := BaseSinPoly_AVX2_c2_f64
	c3 := BaseSinPoly_AVX2_c3_f64
	c4 := BaseSinPoly_AVX2_c4_f64
	intOne := BaseSinPoly_AVX2_intOne_i32_f64
	intTwo := BaseSinPoly_AVX2_intTwo_i32_f64
	intThree := BaseSinPoly_AVX2_intThree_i32_f64
	lanes := 4
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		kFloat := x.Mul(twoOverPi).RoundToEven()
		kInt := kFloat.ConvertToInt32()
		r := x.Sub(kFloat.Mul(piOver2Hi))
		r = r.Sub(kFloat.Mul(piOver2Lo))
		r2 := r.Mul(r)
		sinPoly := s4.MulAdd(r2, s3)
		sinPoly = sinPoly.MulAdd(r2, s2)
		sinPoly = sinPoly.MulAdd(r2, s1)
		sinPoly = sinPoly.MulAdd(r2, one)
		sinR := r.Mul(sinPoly)
		cosPoly := c4.MulAdd(r2, c3)
		cosPoly = cosPoly.MulAdd(r2, c2)
		cosPoly = cosPoly.MulAdd(r2, c1)
		cosR := cosPoly.MulAdd(r2, one)
		octant := kInt.And(intThree)
		useCosMask := octant.And(intOne).Equal(intOne)
		negateMask := octant.And(intTwo).Equal(intTwo)
		sinRData := sinR.Data()
		cosRData := cosR.Data()
		resultData := make([]float64, len(sinRData))
		for i := range sinRData {
			if useCosMask.GetBit(i) {
				resultData[i] = cosRData[i]
			} else {
				resultData[i] = sinRData[i]
			}
		}
		result := archsimd.LoadFloat64x4Slice(resultData)
		negResult := archsimd.BroadcastFloat64x4(0).Sub(result)
		negResultData := negResult.Data()
		for i := range resultData {
			if negateMask.GetBit(i) {
				resultData[i] = negResultData[i]
			}
		}
		result = archsimd.LoadFloat64x4Slice(resultData)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseSinPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseCosPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	twoOverPi := BaseCosPoly_AVX2_twoOverPi_f32
	piOver2Hi := BaseCosPoly_AVX2_piOver2Hi_f32
	piOver2Lo := BaseCosPoly_AVX2_piOver2Lo_f32
	one := BaseCosPoly_AVX2_one_f32
	s1 := BaseCosPoly_AVX2_s1_f32
	s2 := BaseCosPoly_AVX2_s2_f32
	s3 := BaseCosPoly_AVX2_s3_f32
	s4 := BaseCosPoly_AVX2_s4_f32
	c1 := BaseCosPoly_AVX2_c1_f32
	c2 := BaseCosPoly_AVX2_c2_f32
	c3 := BaseCosPoly_AVX2_c3_f32
	c4 := BaseCosPoly_AVX2_c4_f32
	intOne := BaseCosPoly_AVX2_intOne_i32_f32
	intTwo := BaseCosPoly_AVX2_intTwo_i32_f32
	intThree := BaseCosPoly_AVX2_intThree_i32_f32
	lanes := 8
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		kFloat := x.Mul(twoOverPi).RoundToEven()
		kInt := kFloat.ConvertToInt32()
		r := x.Sub(kFloat.Mul(piOver2Hi))
		r = r.Sub(kFloat.Mul(piOver2Lo))
		r2 := r.Mul(r)
		sinPoly := s4.MulAdd(r2, s3)
		sinPoly = sinPoly.MulAdd(r2, s2)
		sinPoly = sinPoly.MulAdd(r2, s1)
		sinPoly = sinPoly.MulAdd(r2, one)
		sinR := r.Mul(sinPoly)
		cosPoly := c4.MulAdd(r2, c3)
		cosPoly = cosPoly.MulAdd(r2, c2)
		cosPoly = cosPoly.MulAdd(r2, c1)
		cosR := cosPoly.MulAdd(r2, one)
		cosOctant := kInt.Add(intOne).And(intThree)
		useCosMask := cosOctant.And(intOne).Equal(intOne)
		negateMask := cosOctant.And(intTwo).Equal(intTwo)
		sinRData := sinR.Data()
		cosRData := cosR.Data()
		resultData := make([]float32, len(sinRData))
		for i := range sinRData {
			if useCosMask.GetBit(i) {
				resultData[i] = cosRData[i]
			} else {
				resultData[i] = sinRData[i]
			}
		}
		result := archsimd.LoadFloat32x8Slice(resultData)
		negResult := archsimd.BroadcastFloat32x8(0).Sub(result)
		negResultData := negResult.Data()
		for i := range resultData {
			if negateMask.GetBit(i) {
				resultData[i] = negResultData[i]
			}
		}
		result = archsimd.LoadFloat32x8Slice(resultData)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseCosPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseCosPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	twoOverPi := BaseCosPoly_AVX2_twoOverPi_f64
	piOver2Hi := BaseCosPoly_AVX2_piOver2Hi_f64
	piOver2Lo := BaseCosPoly_AVX2_piOver2Lo_f64
	one := BaseCosPoly_AVX2_one_f64
	s1 := BaseCosPoly_AVX2_s1_f64
	s2 := BaseCosPoly_AVX2_s2_f64
	s3 := BaseCosPoly_AVX2_s3_f64
	s4 := BaseCosPoly_AVX2_s4_f64
	c1 := BaseCosPoly_AVX2_c1_f64
	c2 := BaseCosPoly_AVX2_c2_f64
	c3 := BaseCosPoly_AVX2_c3_f64
	c4 := BaseCosPoly_AVX2_c4_f64
	intOne := BaseCosPoly_AVX2_intOne_i32_f64
	intTwo := BaseCosPoly_AVX2_intTwo_i32_f64
	intThree := BaseCosPoly_AVX2_intThree_i32_f64
	lanes := 4
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		kFloat := x.Mul(twoOverPi).RoundToEven()
		kInt := kFloat.ConvertToInt32()
		r := x.Sub(kFloat.Mul(piOver2Hi))
		r = r.Sub(kFloat.Mul(piOver2Lo))
		r2 := r.Mul(r)
		sinPoly := s4.MulAdd(r2, s3)
		sinPoly = sinPoly.MulAdd(r2, s2)
		sinPoly = sinPoly.MulAdd(r2, s1)
		sinPoly = sinPoly.MulAdd(r2, one)
		sinR := r.Mul(sinPoly)
		cosPoly := c4.MulAdd(r2, c3)
		cosPoly = cosPoly.MulAdd(r2, c2)
		cosPoly = cosPoly.MulAdd(r2, c1)
		cosR := cosPoly.MulAdd(r2, one)
		cosOctant := kInt.Add(intOne).And(intThree)
		useCosMask := cosOctant.And(intOne).Equal(intOne)
		negateMask := cosOctant.And(intTwo).Equal(intTwo)
		sinRData := sinR.Data()
		cosRData := cosR.Data()
		resultData := make([]float64, len(sinRData))
		for i := range sinRData {
			if useCosMask.GetBit(i) {
				resultData[i] = cosRData[i]
			} else {
				resultData[i] = sinRData[i]
			}
		}
		result := archsimd.LoadFloat64x4Slice(resultData)
		negResult := archsimd.BroadcastFloat64x4(0).Sub(result)
		negResultData := negResult.Data()
		for i := range resultData {
			if negateMask.GetBit(i) {
				resultData[i] = negResultData[i]
			}
		}
		result = archsimd.LoadFloat64x4Slice(resultData)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseCosPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseAtanPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	piOver2 := BaseAtanPoly_AVX2_piOver2_f32
	piOver4 := BaseAtanPoly_AVX2_piOver4_f32
	tanPiOver8 := BaseAtanPoly_AVX2_tanPiOver8_f32
	one := BaseAtanPoly_AVX2_one_f32
	zero := BaseAtanPoly_AVX2_zero_f32
	c1 := BaseAtanPoly_AVX2_c1_f32
	c2 := BaseAtanPoly_AVX2_c2_f32
	c3 := BaseAtanPoly_AVX2_c3_f32
	c4 := BaseAtanPoly_AVX2_c4_f32
	c5 := BaseAtanPoly_AVX2_c5_f32
	lanes := 8
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat32x8(0).Sub(x))
		signMask := x.Less(zero)
		useReciprocalMask := absX.Greater(one)
		recipAbsX := one.Div(absX)
		reduced := recipAbsX.Merge(absX, useReciprocalMask)
		useIdentityMask := reduced.Greater(tanPiOver8)
		xMinus1 := reduced.Sub(one)
		xPlus1 := reduced.Add(one)
		transformed := xMinus1.Div(xPlus1)
		reduced = transformed.Merge(reduced, useIdentityMask)
		z2 := reduced.Mul(reduced)
		poly := c5.MulAdd(z2, c4)
		poly = poly.MulAdd(z2, c3)
		poly = poly.MulAdd(z2, c2)
		poly = poly.MulAdd(z2, c1)
		poly = poly.MulAdd(z2, one)
		atanCore := reduced.Mul(poly)
		atanWithIdentity := piOver4.Add(atanCore)
		atanReduced := atanWithIdentity.Merge(atanCore, useIdentityMask)
		atanWithReciprocal := piOver2.Sub(atanReduced)
		resultAbs := atanWithReciprocal.Merge(atanReduced, useReciprocalMask)
		negResult := archsimd.BroadcastFloat32x8(0).Sub(resultAbs)
		result := negResult.Merge(resultAbs, signMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAtanPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseAtanPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	piOver2 := BaseAtanPoly_AVX2_piOver2_f64
	piOver4 := BaseAtanPoly_AVX2_piOver4_f64
	tanPiOver8 := BaseAtanPoly_AVX2_tanPiOver8_f64
	one := BaseAtanPoly_AVX2_one_f64
	zero := BaseAtanPoly_AVX2_zero_f64
	c1 := BaseAtanPoly_AVX2_c1_f64
	c2 := BaseAtanPoly_AVX2_c2_f64
	c3 := BaseAtanPoly_AVX2_c3_f64
	c4 := BaseAtanPoly_AVX2_c4_f64
	c5 := BaseAtanPoly_AVX2_c5_f64
	lanes := 4
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat64x4(0).Sub(x))
		signMask := x.Less(zero)
		useReciprocalMask := absX.Greater(one)
		recipAbsX := one.Div(absX)
		reduced := recipAbsX.Merge(absX, useReciprocalMask)
		useIdentityMask := reduced.Greater(tanPiOver8)
		xMinus1 := reduced.Sub(one)
		xPlus1 := reduced.Add(one)
		transformed := xMinus1.Div(xPlus1)
		reduced = transformed.Merge(reduced, useIdentityMask)
		z2 := reduced.Mul(reduced)
		poly := c5.MulAdd(z2, c4)
		poly = poly.MulAdd(z2, c3)
		poly = poly.MulAdd(z2, c2)
		poly = poly.MulAdd(z2, c1)
		poly = poly.MulAdd(z2, one)
		atanCore := reduced.Mul(poly)
		atanWithIdentity := piOver4.Add(atanCore)
		atanReduced := atanWithIdentity.Merge(atanCore, useIdentityMask)
		atanWithReciprocal := piOver2.Sub(atanReduced)
		resultAbs := atanWithReciprocal.Merge(atanReduced, useReciprocalMask)
		negResult := archsimd.BroadcastFloat64x4(0).Sub(resultAbs)
		result := negResult.Merge(resultAbs, signMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAtanPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseAsinPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	piOver2 := BaseAsinPoly_AVX2_piOver2_f32
	half := BaseAsinPoly_AVX2_half_f32
	two := BaseAsinPoly_AVX2_two_f32
	one := BaseAsinPoly_AVX2_one_f32
	zero := BaseAsinPoly_AVX2_zero_f32
	p1 := BaseAsinPoly_AVX2_p1_f32
	p2 := BaseAsinPoly_AVX2_p2_f32
	p3 := BaseAsinPoly_AVX2_p3_f32
	p4 := BaseAsinPoly_AVX2_p4_f32
	p5 := BaseAsinPoly_AVX2_p5_f32
	p6 := BaseAsinPoly_AVX2_p6_f32
	lanes := 8
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat32x8(0).Sub(x))
		signMask := x.Less(zero)
		largeMask := absX.Greater(half)
		x2Small := x.Mul(x)
		poly := p6.MulAdd(x2Small, p5)
		poly = poly.MulAdd(x2Small, p4)
		poly = poly.MulAdd(x2Small, p3)
		poly = poly.MulAdd(x2Small, p2)
		poly = poly.MulAdd(x2Small, p1)
		smallResult := x.Add(x.Mul(x2Small).Mul(poly))
		oneMinusAbsX := one.Sub(absX)
		halfOneMinusAbsX := oneMinusAbsX.Mul(half)
		sqrtArg := halfOneMinusAbsX.Sqrt()
		sqrtArg2 := sqrtArg.Mul(sqrtArg)
		polyLarge := p6.MulAdd(sqrtArg2, p5)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p4)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p3)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p2)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p1)
		asinSqrtArg := sqrtArg.Add(sqrtArg.Mul(sqrtArg2).Mul(polyLarge))
		largeResultPos := piOver2.Sub(two.Mul(asinSqrtArg))
		largeResultNeg := archsimd.BroadcastFloat32x8(0).Sub(largeResultPos)
		largeResult := largeResultNeg.Merge(largeResultPos, signMask)
		result := largeResult.Merge(smallResult, largeMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAsinPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseAsinPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	piOver2 := BaseAsinPoly_AVX2_piOver2_f64
	half := BaseAsinPoly_AVX2_half_f64
	two := BaseAsinPoly_AVX2_two_f64
	one := BaseAsinPoly_AVX2_one_f64
	zero := BaseAsinPoly_AVX2_zero_f64
	p1 := BaseAsinPoly_AVX2_p1_f64
	p2 := BaseAsinPoly_AVX2_p2_f64
	p3 := BaseAsinPoly_AVX2_p3_f64
	p4 := BaseAsinPoly_AVX2_p4_f64
	p5 := BaseAsinPoly_AVX2_p5_f64
	p6 := BaseAsinPoly_AVX2_p6_f64
	lanes := 4
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat64x4(0).Sub(x))
		signMask := x.Less(zero)
		largeMask := absX.Greater(half)
		x2Small := x.Mul(x)
		poly := p6.MulAdd(x2Small, p5)
		poly = poly.MulAdd(x2Small, p4)
		poly = poly.MulAdd(x2Small, p3)
		poly = poly.MulAdd(x2Small, p2)
		poly = poly.MulAdd(x2Small, p1)
		smallResult := x.Add(x.Mul(x2Small).Mul(poly))
		oneMinusAbsX := one.Sub(absX)
		halfOneMinusAbsX := oneMinusAbsX.Mul(half)
		sqrtArg := halfOneMinusAbsX.Sqrt()
		sqrtArg2 := sqrtArg.Mul(sqrtArg)
		polyLarge := p6.MulAdd(sqrtArg2, p5)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p4)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p3)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p2)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p1)
		asinSqrtArg := sqrtArg.Add(sqrtArg.Mul(sqrtArg2).Mul(polyLarge))
		largeResultPos := piOver2.Sub(two.Mul(asinSqrtArg))
		largeResultNeg := archsimd.BroadcastFloat64x4(0).Sub(largeResultPos)
		largeResult := largeResultNeg.Merge(largeResultPos, signMask)
		result := largeResult.Merge(smallResult, largeMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAsinPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseAcosPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	piOver2 := BaseAcosPoly_AVX2_piOver2_f32
	half := BaseAcosPoly_AVX2_half_f32
	two := BaseAcosPoly_AVX2_two_f32
	one := BaseAcosPoly_AVX2_one_f32
	zero := BaseAcosPoly_AVX2_zero_f32
	p1 := BaseAcosPoly_AVX2_p1_f32
	p2 := BaseAcosPoly_AVX2_p2_f32
	p3 := BaseAcosPoly_AVX2_p3_f32
	p4 := BaseAcosPoly_AVX2_p4_f32
	p5 := BaseAcosPoly_AVX2_p5_f32
	p6 := BaseAcosPoly_AVX2_p6_f32
	lanes := 8
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat32x8(0).Sub(x))
		signMask := x.Less(zero)
		largeMask := absX.Greater(half)
		x2Small := x.Mul(x)
		poly := p6.MulAdd(x2Small, p5)
		poly = poly.MulAdd(x2Small, p4)
		poly = poly.MulAdd(x2Small, p3)
		poly = poly.MulAdd(x2Small, p2)
		poly = poly.MulAdd(x2Small, p1)
		asinSmall := x.Add(x.Mul(x2Small).Mul(poly))
		oneMinusAbsX := one.Sub(absX)
		halfOneMinusAbsX := oneMinusAbsX.Mul(half)
		sqrtArg := halfOneMinusAbsX.Sqrt()
		sqrtArg2 := sqrtArg.Mul(sqrtArg)
		polyLarge := p6.MulAdd(sqrtArg2, p5)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p4)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p3)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p2)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p1)
		asinSqrtArg := sqrtArg.Add(sqrtArg.Mul(sqrtArg2).Mul(polyLarge))
		largeResultPos := piOver2.Sub(two.Mul(asinSqrtArg))
		largeResultNeg := archsimd.BroadcastFloat32x8(0).Sub(largeResultPos)
		asinLarge := largeResultNeg.Merge(largeResultPos, signMask)
		asinX := asinLarge.Merge(asinSmall, largeMask)
		result := piOver2.Sub(asinX)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAcosPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseAcosPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	piOver2 := BaseAcosPoly_AVX2_piOver2_f64
	half := BaseAcosPoly_AVX2_half_f64
	two := BaseAcosPoly_AVX2_two_f64
	one := BaseAcosPoly_AVX2_one_f64
	zero := BaseAcosPoly_AVX2_zero_f64
	p1 := BaseAcosPoly_AVX2_p1_f64
	p2 := BaseAcosPoly_AVX2_p2_f64
	p3 := BaseAcosPoly_AVX2_p3_f64
	p4 := BaseAcosPoly_AVX2_p4_f64
	p5 := BaseAcosPoly_AVX2_p5_f64
	p6 := BaseAcosPoly_AVX2_p6_f64
	lanes := 4
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat64x4(0).Sub(x))
		signMask := x.Less(zero)
		largeMask := absX.Greater(half)
		x2Small := x.Mul(x)
		poly := p6.MulAdd(x2Small, p5)
		poly = poly.MulAdd(x2Small, p4)
		poly = poly.MulAdd(x2Small, p3)
		poly = poly.MulAdd(x2Small, p2)
		poly = poly.MulAdd(x2Small, p1)
		asinSmall := x.Add(x.Mul(x2Small).Mul(poly))
		oneMinusAbsX := one.Sub(absX)
		halfOneMinusAbsX := oneMinusAbsX.Mul(half)
		sqrtArg := halfOneMinusAbsX.Sqrt()
		sqrtArg2 := sqrtArg.Mul(sqrtArg)
		polyLarge := p6.MulAdd(sqrtArg2, p5)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p4)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p3)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p2)
		polyLarge = polyLarge.MulAdd(sqrtArg2, p1)
		asinSqrtArg := sqrtArg.Add(sqrtArg.Mul(sqrtArg2).Mul(polyLarge))
		largeResultPos := piOver2.Sub(two.Mul(asinSqrtArg))
		largeResultNeg := archsimd.BroadcastFloat64x4(0).Sub(largeResultPos)
		asinLarge := largeResultNeg.Merge(largeResultPos, signMask)
		asinX := asinLarge.Merge(asinSmall, largeMask)
		result := piOver2.Sub(asinX)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAcosPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseSinhPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	half := BaseSinhPoly_AVX2_half_f32
	lanes := 8
	expPos := [8]float32{}
	expNeg := [8]float32{}
	negInput := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		x.StoreSlice(expPos[:])
		BaseExpPoly_avx2(expPos[:], expPos[:])
		expX := archsimd.LoadFloat32x8Slice(expPos[:])
		negX := archsimd.BroadcastFloat32x8(0).Sub(x)
		negX.StoreSlice(negInput[:])
		BaseExpPoly_avx2(negInput[:], expNeg[:])
		expNegX := archsimd.LoadFloat32x8Slice(expNeg[:])
		diff := expX.Sub(expNegX)
		result := diff.Mul(half)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseSinhPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseSinhPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	half := BaseSinhPoly_AVX2_half_f64
	lanes := 4
	expPos := [4]float64{}
	expNeg := [4]float64{}
	negInput := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		x.StoreSlice(expPos[:])
		BaseExpPoly_avx2_Float64(expPos[:], expPos[:])
		expX := archsimd.LoadFloat64x4Slice(expPos[:])
		negX := archsimd.BroadcastFloat64x4(0).Sub(x)
		negX.StoreSlice(negInput[:])
		BaseExpPoly_avx2_Float64(negInput[:], expNeg[:])
		expNegX := archsimd.LoadFloat64x4Slice(expNeg[:])
		diff := expX.Sub(expNegX)
		result := diff.Mul(half)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseSinhPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseCoshPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	half := BaseCoshPoly_AVX2_half_f32
	lanes := 8
	expPos := [8]float32{}
	expNeg := [8]float32{}
	negInput := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		x.StoreSlice(expPos[:])
		BaseExpPoly_avx2(expPos[:], expPos[:])
		expX := archsimd.LoadFloat32x8Slice(expPos[:])
		negX := archsimd.BroadcastFloat32x8(0).Sub(x)
		negX.StoreSlice(negInput[:])
		BaseExpPoly_avx2(negInput[:], expNeg[:])
		expNegX := archsimd.LoadFloat32x8Slice(expNeg[:])
		sum := expX.Add(expNegX)
		result := sum.Mul(half)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseCoshPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseCoshPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	half := BaseCoshPoly_AVX2_half_f64
	lanes := 4
	expPos := [4]float64{}
	expNeg := [4]float64{}
	negInput := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		x.StoreSlice(expPos[:])
		BaseExpPoly_avx2_Float64(expPos[:], expPos[:])
		expX := archsimd.LoadFloat64x4Slice(expPos[:])
		negX := archsimd.BroadcastFloat64x4(0).Sub(x)
		negX.StoreSlice(negInput[:])
		BaseExpPoly_avx2_Float64(negInput[:], expNeg[:])
		expNegX := archsimd.LoadFloat64x4Slice(expNeg[:])
		sum := expX.Add(expNegX)
		result := sum.Mul(half)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseCoshPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseErfPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	a1 := BaseErfPoly_AVX2_a1_f32
	a2 := BaseErfPoly_AVX2_a2_f32
	a3 := BaseErfPoly_AVX2_a3_f32
	a4 := BaseErfPoly_AVX2_a4_f32
	a5 := BaseErfPoly_AVX2_a5_f32
	p := BaseErfPoly_AVX2_p_f32
	one := BaseErfPoly_AVX2_one_f32
	zero := BaseErfPoly_AVX2_zero_f32
	lanes := 8
	tempIn := [8]float32{}
	tempOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat32x8(0).Sub(x))
		signMask := x.Less(zero)
		t := one.Div(one.Add(p.Mul(absX)))
		poly := a5.MulAdd(t, a4)
		poly = poly.MulAdd(t, a3)
		poly = poly.MulAdd(t, a2)
		poly = poly.MulAdd(t, a1)
		poly = poly.Mul(t)
		x2 := absX.Mul(absX)
		negX2 := archsimd.BroadcastFloat32x8(0).Sub(x2)
		negX2.StoreSlice(tempIn[:])
		BaseExpPoly_avx2(tempIn[:], tempOut[:])
		expNegX2 := archsimd.LoadFloat32x8Slice(tempOut[:])
		erfAbs := one.Sub(poly.Mul(expNegX2))
		erfAbs = erfAbs.Min(one).Max(zero)
		negErfAbs := archsimd.BroadcastFloat32x8(0).Sub(erfAbs)
		result := negErfAbs.Merge(erfAbs, signMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseErfPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseErfPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	a1 := BaseErfPoly_AVX2_a1_f64
	a2 := BaseErfPoly_AVX2_a2_f64
	a3 := BaseErfPoly_AVX2_a3_f64
	a4 := BaseErfPoly_AVX2_a4_f64
	a5 := BaseErfPoly_AVX2_a5_f64
	p := BaseErfPoly_AVX2_p_f64
	one := BaseErfPoly_AVX2_one_f64
	zero := BaseErfPoly_AVX2_zero_f64
	lanes := 4
	tempIn := [4]float64{}
	tempOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat64x4(0).Sub(x))
		signMask := x.Less(zero)
		t := one.Div(one.Add(p.Mul(absX)))
		poly := a5.MulAdd(t, a4)
		poly = poly.MulAdd(t, a3)
		poly = poly.MulAdd(t, a2)
		poly = poly.MulAdd(t, a1)
		poly = poly.Mul(t)
		x2 := absX.Mul(absX)
		negX2 := archsimd.BroadcastFloat64x4(0).Sub(x2)
		negX2.StoreSlice(tempIn[:])
		BaseExpPoly_avx2_Float64(tempIn[:], tempOut[:])
		expNegX2 := archsimd.LoadFloat64x4Slice(tempOut[:])
		erfAbs := one.Sub(poly.Mul(expNegX2))
		erfAbs = erfAbs.Min(one).Max(zero)
		negErfAbs := archsimd.BroadcastFloat64x4(0).Sub(erfAbs)
		result := negErfAbs.Merge(erfAbs, signMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseErfPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseExpm1Poly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseExpm1Poly_AVX2_one_f32
	negOne := BaseExpm1Poly_AVX2_negOne_f32
	zero := BaseExpm1Poly_AVX2_zero_f32
	threshold := BaseExpm1Poly_AVX2_threshold_f32
	c2 := BaseExpm1Poly_AVX2_c2_f32
	c3 := BaseExpm1Poly_AVX2_c3_f32
	c4 := BaseExpm1Poly_AVX2_c4_f32
	c5 := BaseExpm1Poly_AVX2_c5_f32
	lanes := 8
	tempIn := [8]float32{}
	tempOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat32x8(0).Sub(x))
		smallMask := absX.Less(threshold)
		poly := c5.MulAdd(x, c4)
		poly = poly.MulAdd(x, c3)
		poly = poly.MulAdd(x, c2)
		poly = poly.MulAdd(x, one)
		taylorResult := x.Mul(poly)
		x.StoreSlice(tempIn[:])
		BaseExpPoly_avx2(tempIn[:], tempOut[:])
		expX := archsimd.LoadFloat32x8Slice(tempOut[:])
		expResult := expX.Sub(one)
		result := taylorResult.Merge(expResult, smallMask)
		veryNegMask := x.Less(archsimd.BroadcastFloat32x8(float32(-20.0)))
		result = negOne.Merge(result, veryNegMask)
		zeroMask := x.Equal(zero)
		result = zero.Merge(result, zeroMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseExpm1Poly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseExpm1Poly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseExpm1Poly_AVX2_one_f64
	negOne := BaseExpm1Poly_AVX2_negOne_f64
	zero := BaseExpm1Poly_AVX2_zero_f64
	threshold := BaseExpm1Poly_AVX2_threshold_f64
	c2 := BaseExpm1Poly_AVX2_c2_f64
	c3 := BaseExpm1Poly_AVX2_c3_f64
	c4 := BaseExpm1Poly_AVX2_c4_f64
	c5 := BaseExpm1Poly_AVX2_c5_f64
	lanes := 4
	tempIn := [4]float64{}
	tempOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat64x4(0).Sub(x))
		smallMask := absX.Less(threshold)
		poly := c5.MulAdd(x, c4)
		poly = poly.MulAdd(x, c3)
		poly = poly.MulAdd(x, c2)
		poly = poly.MulAdd(x, one)
		taylorResult := x.Mul(poly)
		x.StoreSlice(tempIn[:])
		BaseExpPoly_avx2_Float64(tempIn[:], tempOut[:])
		expX := archsimd.LoadFloat64x4Slice(tempOut[:])
		expResult := expX.Sub(one)
		result := taylorResult.Merge(expResult, smallMask)
		veryNegMask := x.Less(archsimd.BroadcastFloat64x4(float64(-20.0)))
		result = negOne.Merge(result, veryNegMask)
		zeroMask := x.Equal(zero)
		result = zero.Merge(result, zeroMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseExpm1Poly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseLog1pPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseLog1pPoly_AVX2_one_f32
	zero := BaseLog1pPoly_AVX2_zero_f32
	negOne := BaseLog1pPoly_AVX2_negOne_f32
	threshold := BaseLog1pPoly_AVX2_threshold_f32
	c2 := BaseLog1pPoly_AVX2_c2_f32
	c3 := BaseLog1pPoly_AVX2_c3_f32
	c4 := BaseLog1pPoly_AVX2_c4_f32
	c5 := BaseLog1pPoly_AVX2_c5_f32
	negInf := archsimd.BroadcastFloat32x8(float32(stdmath.Inf(-1)))
	nan := archsimd.BroadcastFloat32x8(float32(stdmath.NaN()))
	lanes := 8
	tempIn := [8]float32{}
	tempOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat32x8(0).Sub(x))
		smallMask := absX.Less(threshold)
		poly := c5.MulAdd(x, c4)
		poly = poly.MulAdd(x, c3)
		poly = poly.MulAdd(x, c2)
		poly = poly.MulAdd(x, one)
		taylorResult := x.Mul(poly)
		onePlusX := one.Add(x)
		onePlusX.StoreSlice(tempIn[:])
		BaseLogPoly_avx2(tempIn[:], tempOut[:])
		logResult := archsimd.LoadFloat32x8Slice(tempOut[:])
		result := taylorResult.Merge(logResult, smallMask)
		zeroMask := x.Equal(zero)
		result = zero.Merge(result, zeroMask)
		negOneMask := x.Equal(negOne)
		result = negInf.Merge(result, negOneMask)
		invalidMask := x.Less(negOne)
		result = nan.Merge(result, invalidMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseLog1pPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseLog1pPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseLog1pPoly_AVX2_one_f64
	zero := BaseLog1pPoly_AVX2_zero_f64
	negOne := BaseLog1pPoly_AVX2_negOne_f64
	threshold := BaseLog1pPoly_AVX2_threshold_f64
	c2 := BaseLog1pPoly_AVX2_c2_f64
	c3 := BaseLog1pPoly_AVX2_c3_f64
	c4 := BaseLog1pPoly_AVX2_c4_f64
	c5 := BaseLog1pPoly_AVX2_c5_f64
	negInf := archsimd.BroadcastFloat64x4(float64(stdmath.Inf(-1)))
	nan := archsimd.BroadcastFloat64x4(float64(stdmath.NaN()))
	lanes := 4
	tempIn := [4]float64{}
	tempOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		absX := x.Max(archsimd.BroadcastFloat64x4(0).Sub(x))
		smallMask := absX.Less(threshold)
		poly := c5.MulAdd(x, c4)
		poly = poly.MulAdd(x, c3)
		poly = poly.MulAdd(x, c2)
		poly = poly.MulAdd(x, one)
		taylorResult := x.Mul(poly)
		onePlusX := one.Add(x)
		onePlusX.StoreSlice(tempIn[:])
		BaseLogPoly_avx2_Float64(tempIn[:], tempOut[:])
		logResult := archsimd.LoadFloat64x4Slice(tempOut[:])
		result := taylorResult.Merge(logResult, smallMask)
		zeroMask := x.Equal(zero)
		result = zero.Merge(result, zeroMask)
		negOneMask := x.Equal(negOne)
		result = negInf.Merge(result, negOneMask)
		invalidMask := x.Less(negOne)
		result = nan.Merge(result, invalidMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseLog1pPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseLogPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseLogPoly_AVX2_one_f32
	two := BaseLogPoly_AVX2_two_f32
	zero := BaseLogPoly_AVX2_zero_f32
	ln2Hi := BaseLogPoly_AVX2_ln2Hi_f32
	ln2Lo := BaseLogPoly_AVX2_ln2Lo_f32
	negInf := archsimd.BroadcastFloat32x8(float32(stdmath.Inf(-1)))
	nan := archsimd.BroadcastFloat32x8(float32(stdmath.NaN()))
	c1 := BaseLogPoly_AVX2_c1_f32
	c2 := BaseLogPoly_AVX2_c2_f32
	c3 := BaseLogPoly_AVX2_c3_f32
	c4 := BaseLogPoly_AVX2_c4_f32
	c5 := BaseLogPoly_AVX2_c5_f32
	lanes := 8
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		zeroMask := x.Equal(zero)
		negMask := x.Less(zero)
		oneMask := x.Equal(one)
		e := x.GetExponent()
		m := x.GetMantissa()
		mLarge := m.Greater(archsimd.BroadcastFloat32x8(float32(1.414)))
		mAdjusted := m.Mul(archsimd.BroadcastFloat32x8(float32(0.5))).Merge(m, mLarge)
		eData := e.Data()
		eFloatData := make([]float32, len(eData))
		for i, v := range eData {
			eFloatData[i] = float32(v)
		}
		eFloat := archsimd.LoadFloat32x8Slice(eFloatData)
		eAdjusted := eFloat.Add(one).Merge(eFloat, mLarge)
		mMinus1 := mAdjusted.Sub(one)
		mPlus1 := mAdjusted.Add(one)
		y := mMinus1.Div(mPlus1)
		y2 := y.Mul(y)
		poly := c5.MulAdd(y2, c4)
		poly = poly.MulAdd(y2, c3)
		poly = poly.MulAdd(y2, c2)
		poly = poly.MulAdd(y2, c1)
		logM := two.Mul(y).Mul(poly)
		result := eAdjusted.MulAdd(ln2Hi, logM).Add(eAdjusted.Mul(ln2Lo))
		result = negInf.Merge(result, zeroMask)
		result = nan.Merge(result, negMask)
		result = zero.Merge(result, oneMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseLogPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseLogPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseLogPoly_AVX2_one_f64
	two := BaseLogPoly_AVX2_two_f64
	zero := BaseLogPoly_AVX2_zero_f64
	ln2Hi := BaseLogPoly_AVX2_ln2Hi_f64
	ln2Lo := BaseLogPoly_AVX2_ln2Lo_f64
	negInf := archsimd.BroadcastFloat64x4(float64(stdmath.Inf(-1)))
	nan := archsimd.BroadcastFloat64x4(float64(stdmath.NaN()))
	c1 := BaseLogPoly_AVX2_c1_f64
	c2 := BaseLogPoly_AVX2_c2_f64
	c3 := BaseLogPoly_AVX2_c3_f64
	c4 := BaseLogPoly_AVX2_c4_f64
	c5 := BaseLogPoly_AVX2_c5_f64
	lanes := 4
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		zeroMask := x.Equal(zero)
		negMask := x.Less(zero)
		oneMask := x.Equal(one)
		e := x.GetExponent()
		m := x.GetMantissa()
		mLarge := m.Greater(archsimd.BroadcastFloat64x4(float64(1.414)))
		mAdjusted := m.Mul(archsimd.BroadcastFloat64x4(float64(0.5))).Merge(m, mLarge)
		eData := e.Data()
		eFloatData := make([]float64, len(eData))
		for i, v := range eData {
			eFloatData[i] = float64(v)
		}
		eFloat := archsimd.LoadFloat64x4Slice(eFloatData)
		eAdjusted := eFloat.Add(one).Merge(eFloat, mLarge)
		mMinus1 := mAdjusted.Sub(one)
		mPlus1 := mAdjusted.Add(one)
		y := mMinus1.Div(mPlus1)
		y2 := y.Mul(y)
		poly := c5.MulAdd(y2, c4)
		poly = poly.MulAdd(y2, c3)
		poly = poly.MulAdd(y2, c2)
		poly = poly.MulAdd(y2, c1)
		logM := two.Mul(y).Mul(poly)
		result := eAdjusted.MulAdd(ln2Hi, logM).Add(eAdjusted.Mul(ln2Lo))
		result = negInf.Merge(result, zeroMask)
		result = nan.Merge(result, negMask)
		result = zero.Merge(result, oneMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseLogPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseLog2Poly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	log2E_f32 := BaseLog2Poly_AVX2_log2E_f32_f32
	lanes := 8
	tempOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		x.StoreSlice(output[ii:])
		BaseLogPoly_avx2(output[ii:ii+lanes], tempOut[:])
		lnX := archsimd.LoadFloat32x8Slice(tempOut[:])
		result := lnX.Mul(log2E_f32)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseLog2Poly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseLog2Poly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	log2E_f64 := BaseLog2Poly_AVX2_log2E_f64_f64
	lanes := 4
	tempOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		x.StoreSlice(output[ii:])
		BaseLogPoly_avx2_Float64(output[ii:ii+lanes], tempOut[:])
		lnX := archsimd.LoadFloat64x4Slice(tempOut[:])
		result := lnX.Mul(log2E_f64)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseLog2Poly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseLog10Poly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	log10E_f32 := BaseLog10Poly_AVX2_log10E_f32_f32
	lanes := 8
	tempOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		x.StoreSlice(output[ii:])
		BaseLogPoly_avx2(output[ii:ii+lanes], tempOut[:])
		lnX := archsimd.LoadFloat32x8Slice(tempOut[:])
		result := lnX.Mul(log10E_f32)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseLog10Poly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseLog10Poly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	log10E_f64 := BaseLog10Poly_AVX2_log10E_f64_f64
	lanes := 4
	tempOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		x.StoreSlice(output[ii:])
		BaseLogPoly_avx2_Float64(output[ii:ii+lanes], tempOut[:])
		lnX := archsimd.LoadFloat64x4Slice(tempOut[:])
		result := lnX.Mul(log10E_f64)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseLog10Poly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseTanPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	lanes := 8
	sinOut := [8]float32{}
	cosOut := [8]float32{}
	tempIn := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		x.StoreSlice(tempIn[:])
		BaseSinPoly_avx2(tempIn[:], sinOut[:])
		x.StoreSlice(tempIn[:])
		BaseCosPoly_avx2(tempIn[:], cosOut[:])
		sinX := archsimd.LoadFloat32x8Slice(sinOut[:])
		cosX := archsimd.LoadFloat32x8Slice(cosOut[:])
		result := sinX.Div(cosX)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseTanPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseTanPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	lanes := 4
	sinOut := [4]float64{}
	cosOut := [4]float64{}
	tempIn := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		x.StoreSlice(tempIn[:])
		BaseSinPoly_avx2_Float64(tempIn[:], sinOut[:])
		x.StoreSlice(tempIn[:])
		BaseCosPoly_avx2_Float64(tempIn[:], cosOut[:])
		sinX := archsimd.LoadFloat64x4Slice(sinOut[:])
		cosX := archsimd.LoadFloat64x4Slice(cosOut[:])
		result := sinX.Div(cosX)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseTanPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseAtan2Poly_avx2(inputY []float32, inputX []float32, output []float32) {
	size := len(inputY)
	if len(inputX) < size {
		size = len(inputX)
	}
	if len(output) < size {
		size = len(output)
	}
	pi := BaseAtan2Poly_AVX2_pi_f32
	piOver2 := BaseAtan2Poly_AVX2_piOver2_f32
	zero := BaseAtan2Poly_AVX2_zero_f32
	lanes := 8
	tempIn := [8]float32{}
	tempOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		y := archsimd.LoadFloat32x8Slice(inputY[ii:])
		x := archsimd.LoadFloat32x8Slice(inputX[ii:])
		ratio := y.Div(x)
		ratio.StoreSlice(tempIn[:])
		BaseAtanPoly_avx2(tempIn[:], tempOut[:])
		atanRatio := archsimd.LoadFloat32x8Slice(tempOut[:])
		xNegMask := x.Less(zero)
		yNonNegMask := y.GreaterEqual(zero)
		xZeroMask := x.Equal(zero)
		yPosZeroMask := y.Greater(zero)
		yNegMask := y.Less(zero)
		result := atanRatio
		addPiMask := xNegMask.And(yNonNegMask)
		result = result.Add(pi).Merge(result, addPiMask)
		subPiMask := xNegMask.And(yNegMask)
		result = result.Sub(pi).Merge(result, subPiMask)
		result = piOver2.Merge(result, xZeroMask.And(yPosZeroMask))
		result = archsimd.BroadcastFloat32x8(0).Sub(piOver2).Merge(result, xZeroMask.And(yNegMask))
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAtan2Poly_fallback(inputY[ii:size], inputX[ii:size], output[ii:size])
	}
}

func BaseAtan2Poly_avx2_Float64(inputY []float64, inputX []float64, output []float64) {
	size := len(inputY)
	if len(inputX) < size {
		size = len(inputX)
	}
	if len(output) < size {
		size = len(output)
	}
	pi := BaseAtan2Poly_AVX2_pi_f64
	piOver2 := BaseAtan2Poly_AVX2_piOver2_f64
	zero := BaseAtan2Poly_AVX2_zero_f64
	lanes := 4
	tempIn := [4]float64{}
	tempOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		y := archsimd.LoadFloat64x4Slice(inputY[ii:])
		x := archsimd.LoadFloat64x4Slice(inputX[ii:])
		ratio := y.Div(x)
		ratio.StoreSlice(tempIn[:])
		BaseAtanPoly_avx2_Float64(tempIn[:], tempOut[:])
		atanRatio := archsimd.LoadFloat64x4Slice(tempOut[:])
		xNegMask := x.Less(zero)
		yNonNegMask := y.GreaterEqual(zero)
		xZeroMask := x.Equal(zero)
		yPosZeroMask := y.Greater(zero)
		yNegMask := y.Less(zero)
		result := atanRatio
		addPiMask := xNegMask.And(yNonNegMask)
		result = result.Add(pi).Merge(result, addPiMask)
		subPiMask := xNegMask.And(yNegMask)
		result = result.Sub(pi).Merge(result, subPiMask)
		result = piOver2.Merge(result, xZeroMask.And(yPosZeroMask))
		result = archsimd.BroadcastFloat64x4(0).Sub(piOver2).Merge(result, xZeroMask.And(yNegMask))
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAtan2Poly_fallback_Float64(inputY[ii:size], inputX[ii:size], output[ii:size])
	}
}

func BaseSinCosPoly_avx2(input []float32, sinOutput []float32, cosOutput []float32) {
	size := len(input)
	if len(sinOutput) < size {
		size = len(sinOutput)
	}
	if len(cosOutput) < size {
		size = len(cosOutput)
	}
	twoOverPi := BaseSinCosPoly_AVX2_twoOverPi_f32
	piOver2Hi := BaseSinCosPoly_AVX2_piOver2Hi_f32
	piOver2Lo := BaseSinCosPoly_AVX2_piOver2Lo_f32
	one := BaseSinCosPoly_AVX2_one_f32
	s1 := BaseSinCosPoly_AVX2_s1_f32
	s2 := BaseSinCosPoly_AVX2_s2_f32
	s3 := BaseSinCosPoly_AVX2_s3_f32
	s4 := BaseSinCosPoly_AVX2_s4_f32
	c1 := BaseSinCosPoly_AVX2_c1_f32
	c2 := BaseSinCosPoly_AVX2_c2_f32
	c3 := BaseSinCosPoly_AVX2_c3_f32
	c4 := BaseSinCosPoly_AVX2_c4_f32
	intOne := BaseSinCosPoly_AVX2_intOne_i32_f32
	intTwo := BaseSinCosPoly_AVX2_intTwo_i32_f32
	intThree := BaseSinCosPoly_AVX2_intThree_i32_f32
	lanes := 8
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		kFloat := x.Mul(twoOverPi).RoundToEven()
		kInt := kFloat.ConvertToInt32()
		r := x.Sub(kFloat.Mul(piOver2Hi))
		r = r.Sub(kFloat.Mul(piOver2Lo))
		r2 := r.Mul(r)
		sinPoly := s4.MulAdd(r2, s3)
		sinPoly = sinPoly.MulAdd(r2, s2)
		sinPoly = sinPoly.MulAdd(r2, s1)
		sinPoly = sinPoly.MulAdd(r2, one)
		sinR := r.Mul(sinPoly)
		cosPoly := c4.MulAdd(r2, c3)
		cosPoly = cosPoly.MulAdd(r2, c2)
		cosPoly = cosPoly.MulAdd(r2, c1)
		cosR := cosPoly.MulAdd(r2, one)
		sinOctant := kInt.And(intThree)
		sinUseCosMask := sinOctant.And(intOne).Equal(intOne)
		sinNegateMask := sinOctant.And(intTwo).Equal(intTwo)
		cosOctant := kInt.Add(intOne).And(intThree)
		cosUseCosMask := cosOctant.And(intOne).Equal(intOne)
		cosNegateMask := cosOctant.And(intTwo).Equal(intTwo)
		sinRData := sinR.Data()
		cosRData := cosR.Data()
		sinResultData := make([]float32, len(sinRData))
		cosResultData := make([]float32, len(sinRData))
		for i := range sinRData {
			if sinUseCosMask.GetBit(i) {
				sinResultData[i] = cosRData[i]
			} else {
				sinResultData[i] = sinRData[i]
			}
			if cosUseCosMask.GetBit(i) {
				cosResultData[i] = cosRData[i]
			} else {
				cosResultData[i] = sinRData[i]
			}
		}
		for i := range sinResultData {
			if sinNegateMask.GetBit(i) {
				sinResultData[i] = -sinResultData[i]
			}
			if cosNegateMask.GetBit(i) {
				cosResultData[i] = -cosResultData[i]
			}
		}
		archsimd.LoadFloat32x8Slice(sinResultData).StoreSlice(sinOutput[ii:])
		archsimd.LoadFloat32x8Slice(cosResultData).StoreSlice(cosOutput[ii:])
	}
	if ii < size {
		BaseSinCosPoly_fallback(input[ii:size], sinOutput[ii:size], cosOutput[ii:size])
	}
}

func BaseSinCosPoly_avx2_Float64(input []float64, sinOutput []float64, cosOutput []float64) {
	size := len(input)
	if len(sinOutput) < size {
		size = len(sinOutput)
	}
	if len(cosOutput) < size {
		size = len(cosOutput)
	}
	twoOverPi := BaseSinCosPoly_AVX2_twoOverPi_f64
	piOver2Hi := BaseSinCosPoly_AVX2_piOver2Hi_f64
	piOver2Lo := BaseSinCosPoly_AVX2_piOver2Lo_f64
	one := BaseSinCosPoly_AVX2_one_f64
	s1 := BaseSinCosPoly_AVX2_s1_f64
	s2 := BaseSinCosPoly_AVX2_s2_f64
	s3 := BaseSinCosPoly_AVX2_s3_f64
	s4 := BaseSinCosPoly_AVX2_s4_f64
	c1 := BaseSinCosPoly_AVX2_c1_f64
	c2 := BaseSinCosPoly_AVX2_c2_f64
	c3 := BaseSinCosPoly_AVX2_c3_f64
	c4 := BaseSinCosPoly_AVX2_c4_f64
	intOne := BaseSinCosPoly_AVX2_intOne_i32_f64
	intTwo := BaseSinCosPoly_AVX2_intTwo_i32_f64
	intThree := BaseSinCosPoly_AVX2_intThree_i32_f64
	lanes := 4
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		kFloat := x.Mul(twoOverPi).RoundToEven()
		kInt := kFloat.ConvertToInt32()
		r := x.Sub(kFloat.Mul(piOver2Hi))
		r = r.Sub(kFloat.Mul(piOver2Lo))
		r2 := r.Mul(r)
		sinPoly := s4.MulAdd(r2, s3)
		sinPoly = sinPoly.MulAdd(r2, s2)
		sinPoly = sinPoly.MulAdd(r2, s1)
		sinPoly = sinPoly.MulAdd(r2, one)
		sinR := r.Mul(sinPoly)
		cosPoly := c4.MulAdd(r2, c3)
		cosPoly = cosPoly.MulAdd(r2, c2)
		cosPoly = cosPoly.MulAdd(r2, c1)
		cosR := cosPoly.MulAdd(r2, one)
		sinOctant := kInt.And(intThree)
		sinUseCosMask := sinOctant.And(intOne).Equal(intOne)
		sinNegateMask := sinOctant.And(intTwo).Equal(intTwo)
		cosOctant := kInt.Add(intOne).And(intThree)
		cosUseCosMask := cosOctant.And(intOne).Equal(intOne)
		cosNegateMask := cosOctant.And(intTwo).Equal(intTwo)
		sinRData := sinR.Data()
		cosRData := cosR.Data()
		sinResultData := make([]float64, len(sinRData))
		cosResultData := make([]float64, len(sinRData))
		for i := range sinRData {
			if sinUseCosMask.GetBit(i) {
				sinResultData[i] = cosRData[i]
			} else {
				sinResultData[i] = sinRData[i]
			}
			if cosUseCosMask.GetBit(i) {
				cosResultData[i] = cosRData[i]
			} else {
				cosResultData[i] = sinRData[i]
			}
		}
		for i := range sinResultData {
			if sinNegateMask.GetBit(i) {
				sinResultData[i] = -sinResultData[i]
			}
			if cosNegateMask.GetBit(i) {
				cosResultData[i] = -cosResultData[i]
			}
		}
		archsimd.LoadFloat64x4Slice(sinResultData).StoreSlice(sinOutput[ii:])
		archsimd.LoadFloat64x4Slice(cosResultData).StoreSlice(cosOutput[ii:])
	}
	if ii < size {
		BaseSinCosPoly_fallback_Float64(input[ii:size], sinOutput[ii:size], cosOutput[ii:size])
	}
}

func BasePowPoly_avx2(inputX []float32, inputY []float32, output []float32) {
	size := len(inputX)
	if len(inputY) < size {
		size = len(inputY)
	}
	if len(output) < size {
		size = len(output)
	}
	one := BasePowPoly_AVX2_one_f32
	zero := BasePowPoly_AVX2_zero_f32
	posInf := archsimd.BroadcastFloat32x8(float32(stdmath.Inf(1)))
	lanes := 8
	logIn := [8]float32{}
	logOut := [8]float32{}
	expIn := [8]float32{}
	expOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(inputX[ii:])
		y := archsimd.LoadFloat32x8Slice(inputY[ii:])
		x.StoreSlice(logIn[:])
		BaseLogPoly_avx2(logIn[:], logOut[:])
		logX := archsimd.LoadFloat32x8Slice(logOut[:])
		yLogX := y.Mul(logX)
		yLogX.StoreSlice(expIn[:])
		BaseExpPoly_avx2(expIn[:], expOut[:])
		result := archsimd.LoadFloat32x8Slice(expOut[:])
		xZeroMask := x.Equal(zero)
		xOneMask := x.Equal(one)
		xInfMask := x.Equal(archsimd.BroadcastFloat32x8(float32(stdmath.Inf(1)))).Or(x.Equal(archsimd.BroadcastFloat32x8(float32(stdmath.Inf(-1)))))
		yZeroMask := y.Equal(zero)
		yPosMask := y.Greater(zero)
		yNegMask := y.Less(zero)
		result = one.Merge(result, yZeroMask)
		result = zero.Merge(result, xZeroMask.And(yPosMask))
		result = posInf.Merge(result, xZeroMask.And(yNegMask))
		result = one.Merge(result, xOneMask)
		result = posInf.Merge(result, xInfMask.And(yPosMask))
		result = zero.Merge(result, xInfMask.And(yNegMask))
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BasePowPoly_fallback(inputX[ii:size], inputY[ii:size], output[ii:size])
	}
}

func BasePowPoly_avx2_Float64(inputX []float64, inputY []float64, output []float64) {
	size := len(inputX)
	if len(inputY) < size {
		size = len(inputY)
	}
	if len(output) < size {
		size = len(output)
	}
	one := BasePowPoly_AVX2_one_f64
	zero := BasePowPoly_AVX2_zero_f64
	posInf := archsimd.BroadcastFloat64x4(float64(stdmath.Inf(1)))
	lanes := 4
	logIn := [4]float64{}
	logOut := [4]float64{}
	expIn := [4]float64{}
	expOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(inputX[ii:])
		y := archsimd.LoadFloat64x4Slice(inputY[ii:])
		x.StoreSlice(logIn[:])
		BaseLogPoly_avx2_Float64(logIn[:], logOut[:])
		logX := archsimd.LoadFloat64x4Slice(logOut[:])
		yLogX := y.Mul(logX)
		yLogX.StoreSlice(expIn[:])
		BaseExpPoly_avx2_Float64(expIn[:], expOut[:])
		result := archsimd.LoadFloat64x4Slice(expOut[:])
		xZeroMask := x.Equal(zero)
		xOneMask := x.Equal(one)
		xInfMask := x.Equal(archsimd.BroadcastFloat64x4(stdmath.Inf(1))).Or(x.Equal(archsimd.BroadcastFloat64x4(stdmath.Inf(-1))))
		yZeroMask := y.Equal(zero)
		yPosMask := y.Greater(zero)
		yNegMask := y.Less(zero)
		result = one.Merge(result, yZeroMask)
		result = zero.Merge(result, xZeroMask.And(yPosMask))
		result = posInf.Merge(result, xZeroMask.And(yNegMask))
		result = one.Merge(result, xOneMask)
		result = posInf.Merge(result, xInfMask.And(yPosMask))
		result = zero.Merge(result, xInfMask.And(yNegMask))
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BasePowPoly_fallback_Float64(inputX[ii:size], inputY[ii:size], output[ii:size])
	}
}

func BaseExp2Poly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	ln2_f32 := BaseExp2Poly_AVX2_ln2_f32_f32
	lanes := 8
	expIn := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		xLn2 := x.Mul(ln2_f32)
		xLn2.StoreSlice(expIn[:])
		BaseExpPoly_avx2(expIn[:], output[ii:ii+lanes])
	}
	if ii < size {
		BaseExp2Poly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseExp2Poly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	ln2_f64 := BaseExp2Poly_AVX2_ln2_f64_f64
	lanes := 4
	expIn := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		xLn2 := x.Mul(ln2_f64)
		xLn2.StoreSlice(expIn[:])
		BaseExpPoly_avx2_Float64(expIn[:], output[ii:ii+lanes])
	}
	if ii < size {
		BaseExp2Poly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseExp10Poly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	ln10_f32 := BaseExp10Poly_AVX2_ln10_f32_f32
	lanes := 8
	expIn := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		xLn10 := x.Mul(ln10_f32)
		xLn10.StoreSlice(expIn[:])
		BaseExpPoly_avx2(expIn[:], output[ii:ii+lanes])
	}
	if ii < size {
		BaseExp10Poly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseExp10Poly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	ln10_f64 := BaseExp10Poly_AVX2_ln10_f64_f64
	lanes := 4
	expIn := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		xLn10 := x.Mul(ln10_f64)
		xLn10.StoreSlice(expIn[:])
		BaseExpPoly_avx2_Float64(expIn[:], output[ii:ii+lanes])
	}
	if ii < size {
		BaseExp10Poly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseAsinhPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseAsinhPoly_AVX2_one_f32
	lanes := 8
	logIn := [8]float32{}
	logOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		x2 := x.Mul(x)
		x2Plus1 := x2.Add(one)
		sqrtPart := x2Plus1.Sqrt()
		arg := x.Add(sqrtPart)
		arg.StoreSlice(logIn[:])
		BaseLogPoly_avx2(logIn[:], logOut[:])
		result := archsimd.LoadFloat32x8Slice(logOut[:])
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAsinhPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseAsinhPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseAsinhPoly_AVX2_one_f64
	lanes := 4
	logIn := [4]float64{}
	logOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		x2 := x.Mul(x)
		x2Plus1 := x2.Add(one)
		sqrtPart := x2Plus1.Sqrt()
		arg := x.Add(sqrtPart)
		arg.StoreSlice(logIn[:])
		BaseLogPoly_avx2_Float64(logIn[:], logOut[:])
		result := archsimd.LoadFloat64x4Slice(logOut[:])
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAsinhPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseAcoshPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseAcoshPoly_AVX2_one_f32
	zero := BaseAcoshPoly_AVX2_zero_f32
	lanes := 8
	logIn := [8]float32{}
	logOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		x2 := x.Mul(x)
		x2Minus1 := x2.Sub(one)
		sqrtPart := x2Minus1.Sqrt()
		arg := x.Add(sqrtPart)
		arg.StoreSlice(logIn[:])
		BaseLogPoly_avx2(logIn[:], logOut[:])
		result := archsimd.LoadFloat32x8Slice(logOut[:])
		oneMask := x.Equal(one)
		result = zero.Merge(result, oneMask)
		invalidMask := x.Less(one)
		nan := zero.Div(zero)
		result = nan.Merge(result, invalidMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAcoshPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseAcoshPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseAcoshPoly_AVX2_one_f64
	zero := BaseAcoshPoly_AVX2_zero_f64
	lanes := 4
	logIn := [4]float64{}
	logOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		x2 := x.Mul(x)
		x2Minus1 := x2.Sub(one)
		sqrtPart := x2Minus1.Sqrt()
		arg := x.Add(sqrtPart)
		arg.StoreSlice(logIn[:])
		BaseLogPoly_avx2_Float64(logIn[:], logOut[:])
		result := archsimd.LoadFloat64x4Slice(logOut[:])
		oneMask := x.Equal(one)
		result = zero.Merge(result, oneMask)
		invalidMask := x.Less(one)
		nan := zero.Div(zero)
		result = nan.Merge(result, invalidMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAcoshPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseAtanhPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseAtanhPoly_AVX2_one_f32
	half := BaseAtanhPoly_AVX2_half_f32
	zero := BaseAtanhPoly_AVX2_zero_f32
	lanes := 8
	logIn := [8]float32{}
	logOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		onePlusX := one.Add(x)
		oneMinusX := one.Sub(x)
		ratio := onePlusX.Div(oneMinusX)
		ratio.StoreSlice(logIn[:])
		BaseLogPoly_avx2(logIn[:], logOut[:])
		logRatio := archsimd.LoadFloat32x8Slice(logOut[:])
		result := half.Mul(logRatio)
		zeroMask := x.Equal(zero)
		result = zero.Merge(result, zeroMask)
		inf := one.Div(zero)
		negInf := archsimd.BroadcastFloat32x8(0).Sub(inf)
		oneMask := x.Equal(one)
		negOneMask := x.Equal(archsimd.BroadcastFloat32x8(0).Sub(one))
		result = inf.Merge(result, oneMask)
		result = negInf.Merge(result, negOneMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAtanhPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseAtanhPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	one := BaseAtanhPoly_AVX2_one_f64
	half := BaseAtanhPoly_AVX2_half_f64
	zero := BaseAtanhPoly_AVX2_zero_f64
	lanes := 4
	logIn := [4]float64{}
	logOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		onePlusX := one.Add(x)
		oneMinusX := one.Sub(x)
		ratio := onePlusX.Div(oneMinusX)
		ratio.StoreSlice(logIn[:])
		BaseLogPoly_avx2_Float64(logIn[:], logOut[:])
		logRatio := archsimd.LoadFloat64x4Slice(logOut[:])
		result := half.Mul(logRatio)
		zeroMask := x.Equal(zero)
		result = zero.Merge(result, zeroMask)
		inf := one.Div(zero)
		negInf := archsimd.BroadcastFloat64x4(0).Sub(inf)
		oneMask := x.Equal(one)
		negOneMask := x.Equal(archsimd.BroadcastFloat64x4(0).Sub(one))
		result = inf.Merge(result, oneMask)
		result = negInf.Merge(result, negOneMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseAtanhPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseCbrtPoly_avx2(input []float32, output []float32) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	zero := BaseCbrtPoly_AVX2_zero_f32
	third := archsimd.BroadcastFloat32x8(float32(1.0 / 3.0))
	posInf := archsimd.BroadcastFloat32x8(float32(stdmath.Inf(1)))
	negInf := archsimd.BroadcastFloat32x8(float32(stdmath.Inf(-1)))
	nan := archsimd.BroadcastFloat32x8(float32(stdmath.NaN()))
	lanes := 8
	logIn := [8]float32{}
	logOut := [8]float32{}
	expIn := [8]float32{}
	expOut := [8]float32{}
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(input[ii:])
		posInfMask := x.Equal(posInf)
		negInfMask := x.Equal(negInf)
		nanMask := x.Equal(x).Xor(archsimd.BroadcastFloat32x8(1.0).Equal(archsimd.BroadcastFloat32x8(1.0)))
		signMask := x.Less(zero)
		absX := x.Max(archsimd.BroadcastFloat32x8(0).Sub(x))
		absX.StoreSlice(logIn[:])
		BaseLogPoly_avx2(logIn[:], logOut[:])
		logAbsX := archsimd.LoadFloat32x8Slice(logOut[:])
		logAbsXOver3 := logAbsX.Mul(third)
		logAbsXOver3.StoreSlice(expIn[:])
		BaseExpPoly_avx2(expIn[:], expOut[:])
		cbrtAbsX := archsimd.LoadFloat32x8Slice(expOut[:])
		negCbrt := archsimd.BroadcastFloat32x8(0).Sub(cbrtAbsX)
		result := negCbrt.Merge(cbrtAbsX, signMask)
		zeroMask := x.Equal(zero)
		result = zero.Merge(result, zeroMask)
		result = posInf.Merge(result, posInfMask)
		result = negInf.Merge(result, negInfMask)
		result = nan.Merge(result, nanMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseCbrtPoly_fallback(input[ii:size], output[ii:size])
	}
}

func BaseCbrtPoly_avx2_Float64(input []float64, output []float64) {
	size := len(input)
	if len(output) < size {
		size = len(output)
	}
	zero := BaseCbrtPoly_AVX2_zero_f64
	third := archsimd.BroadcastFloat64x4(float64(1.0 / 3.0))
	posInf := archsimd.BroadcastFloat64x4(float64(stdmath.Inf(1)))
	negInf := archsimd.BroadcastFloat64x4(float64(stdmath.Inf(-1)))
	nan := archsimd.BroadcastFloat64x4(float64(stdmath.NaN()))
	lanes := 4
	logIn := [4]float64{}
	logOut := [4]float64{}
	expIn := [4]float64{}
	expOut := [4]float64{}
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(input[ii:])
		posInfMask := x.Equal(posInf)
		negInfMask := x.Equal(negInf)
		nanMask := x.Equal(x).Xor(archsimd.BroadcastFloat64x4(1.0).Equal(archsimd.BroadcastFloat64x4(1.0)))
		signMask := x.Less(zero)
		absX := x.Max(archsimd.BroadcastFloat64x4(0).Sub(x))
		absX.StoreSlice(logIn[:])
		BaseLogPoly_avx2_Float64(logIn[:], logOut[:])
		logAbsX := archsimd.LoadFloat64x4Slice(logOut[:])
		logAbsXOver3 := logAbsX.Mul(third)
		logAbsXOver3.StoreSlice(expIn[:])
		BaseExpPoly_avx2_Float64(expIn[:], expOut[:])
		cbrtAbsX := archsimd.LoadFloat64x4Slice(expOut[:])
		negCbrt := archsimd.BroadcastFloat64x4(0).Sub(cbrtAbsX)
		result := negCbrt.Merge(cbrtAbsX, signMask)
		zeroMask := x.Equal(zero)
		result = zero.Merge(result, zeroMask)
		result = posInf.Merge(result, posInfMask)
		result = negInf.Merge(result, negInfMask)
		result = nan.Merge(result, nanMask)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseCbrtPoly_fallback_Float64(input[ii:size], output[ii:size])
	}
}

func BaseHypotPoly_avx2(inputX []float32, inputY []float32, output []float32) {
	size := len(inputX)
	if len(inputY) < size {
		size = len(inputY)
	}
	if len(output) < size {
		size = len(output)
	}
	one := BaseHypotPoly_AVX2_one_f32
	zero := BaseHypotPoly_AVX2_zero_f32
	posInf := archsimd.BroadcastFloat32x8(float32(stdmath.Inf(1)))
	nan := archsimd.BroadcastFloat32x8(float32(stdmath.NaN()))
	lanes := 8
	ii := 0
	for ; ii+8 <= size; ii += lanes {
		x := archsimd.LoadFloat32x8Slice(inputX[ii:])
		y := archsimd.LoadFloat32x8Slice(inputY[ii:])
		xIsInf := x.Equal(archsimd.BroadcastFloat32x8(float32(stdmath.Inf(1)))).Or(x.Equal(archsimd.BroadcastFloat32x8(float32(stdmath.Inf(-1)))))
		yIsInf := y.Equal(archsimd.BroadcastFloat32x8(float32(stdmath.Inf(1)))).Or(y.Equal(archsimd.BroadcastFloat32x8(float32(stdmath.Inf(-1)))))
		xIsNaN := x.Equal(x).Xor(archsimd.BroadcastFloat32x8(1.0).Equal(archsimd.BroadcastFloat32x8(1.0)))
		yIsNaN := y.Equal(y).Xor(archsimd.BroadcastFloat32x8(1.0).Equal(archsimd.BroadcastFloat32x8(1.0)))
		eitherInf := xIsInf.Or(yIsInf)
		eitherNaN := xIsNaN.Or(yIsNaN)
		absX := x.Max(archsimd.BroadcastFloat32x8(0).Sub(x))
		absY := y.Max(archsimd.BroadcastFloat32x8(0).Sub(y))
		maxXY := absX.Max(absY)
		minXY := absX.Min(absY)
		ratio := minXY.Div(maxXY)
		ratio2 := ratio.Mul(ratio)
		sqrtArg := one.Add(ratio2)
		sqrtPart := sqrtArg.Sqrt()
		result := maxXY.Mul(sqrtPart)
		zeroMask := maxXY.Equal(zero)
		result = zero.Merge(result, zeroMask)
		result = nan.Merge(result, eitherNaN)
		result = posInf.Merge(result, eitherInf)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseHypotPoly_fallback(inputX[ii:size], inputY[ii:size], output[ii:size])
	}
}

func BaseHypotPoly_avx2_Float64(inputX []float64, inputY []float64, output []float64) {
	size := len(inputX)
	if len(inputY) < size {
		size = len(inputY)
	}
	if len(output) < size {
		size = len(output)
	}
	one := BaseHypotPoly_AVX2_one_f64
	zero := BaseHypotPoly_AVX2_zero_f64
	posInf := archsimd.BroadcastFloat64x4(float64(stdmath.Inf(1)))
	nan := archsimd.BroadcastFloat64x4(float64(stdmath.NaN()))
	lanes := 4
	ii := 0
	for ; ii+4 <= size; ii += lanes {
		x := archsimd.LoadFloat64x4Slice(inputX[ii:])
		y := archsimd.LoadFloat64x4Slice(inputY[ii:])
		xIsInf := x.Equal(archsimd.BroadcastFloat64x4(stdmath.Inf(1))).Or(x.Equal(archsimd.BroadcastFloat64x4(stdmath.Inf(-1))))
		yIsInf := y.Equal(archsimd.BroadcastFloat64x4(stdmath.Inf(1))).Or(y.Equal(archsimd.BroadcastFloat64x4(stdmath.Inf(-1))))
		xIsNaN := x.Equal(x).Xor(archsimd.BroadcastFloat64x4(1.0).Equal(archsimd.BroadcastFloat64x4(1.0)))
		yIsNaN := y.Equal(y).Xor(archsimd.BroadcastFloat64x4(1.0).Equal(archsimd.BroadcastFloat64x4(1.0)))
		eitherInf := xIsInf.Or(yIsInf)
		eitherNaN := xIsNaN.Or(yIsNaN)
		absX := x.Max(archsimd.BroadcastFloat64x4(0).Sub(x))
		absY := y.Max(archsimd.BroadcastFloat64x4(0).Sub(y))
		maxXY := absX.Max(absY)
		minXY := absX.Min(absY)
		ratio := minXY.Div(maxXY)
		ratio2 := ratio.Mul(ratio)
		sqrtArg := one.Add(ratio2)
		sqrtPart := sqrtArg.Sqrt()
		result := maxXY.Mul(sqrtPart)
		zeroMask := maxXY.Equal(zero)
		result = zero.Merge(result, zeroMask)
		result = nan.Merge(result, eitherNaN)
		result = posInf.Merge(result, eitherInf)
		result.StoreSlice(output[ii:])
	}
	if ii < size {
		BaseHypotPoly_fallback_Float64(inputX[ii:size], inputY[ii:size], output[ii:size])
	}
}
