// Code generated by hwygen. DO NOT EDIT.
//go:build amd64 && goexperiment.simd

package math

import (
	"github.com/ajroetker/go-highway/hwy"
	"simd/archsimd"
)

// Hoisted constants - pre-broadcasted at package init time
var (
	BaseSinVec_AVX2_c3_f32           = archsimd.BroadcastFloat32x8(float32(trigC3_f32))
	BaseCosVec_AVX2_one_f64          = archsimd.BroadcastFloat64x4(float64(trigOne_f64))
	BaseCoshVec_AVX2_one_f32         = archsimd.BroadcastFloat32x8(float32(1.0))
	BaseExpVec_AVX2_c3_f32           = archsimd.BroadcastFloat32x8(float32(expC3_f32))
	BaseSigmoidVec_AVX2_zero_f64     = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseSinVec_AVX2_twoOverPi_f64    = archsimd.BroadcastFloat64x4(float64(trig2OverPi_f64))
	BaseCosVec_AVX2_s2_f64           = archsimd.BroadcastFloat64x4(float64(trigS2_f64))
	BaseAtanhVec_AVX2_zero_f64       = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseExpVec_AVX2_invLn2_f32       = archsimd.BroadcastFloat32x8(float32(expInvLn2_f32))
	BaseExpVec_AVX2_ln2Hi_f32        = archsimd.BroadcastFloat32x8(float32(expLn2Hi_f32))
	BaseExpVec_AVX2_c4_f32           = archsimd.BroadcastFloat32x8(float32(expC4_f32))
	BaseExpVec_AVX2_c1_f32           = archsimd.BroadcastFloat32x8(float32(expC1_f32))
	BaseTanhVec_AVX2_threshold_f64   = archsimd.BroadcastFloat64x4(float64(tanhClamp_f64))
	BaseLogVec_AVX2_ln2Hi_f64        = archsimd.BroadcastFloat64x4(float64(logLn2Hi_f64))
	BaseSinVec_AVX2_s1_f64           = archsimd.BroadcastFloat64x4(float64(trigS1_f64))
	BaseSinVec_AVX2_one_f64          = archsimd.BroadcastFloat64x4(float64(trigOne_f64))
	BaseLogVec_AVX2_zero_f32         = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseSinVec_AVX2_intOne_i32_f64   = archsimd.BroadcastInt32x4(1)
	BaseCosVec_AVX2_c3_f64           = archsimd.BroadcastFloat64x4(float64(trigC3_f64))
	BaseErfVec_AVX2_a4_f32           = archsimd.BroadcastFloat32x8(float32(erfA4_f32))
	BaseErfVec_AVX2_zero_f32         = archsimd.BroadcastFloat32x8(float32(erfZero_f32))
	BaseSinhVec_AVX2_c5_f64          = archsimd.BroadcastFloat64x4(float64(sinhC5_f64))
	BaseCoshVec_AVX2_c6_f32          = archsimd.BroadcastFloat32x8(float32(0.001388888888888889))
	BaseExpVec_AVX2_c5_f32           = archsimd.BroadcastFloat32x8(float32(expC5_f32))
	BaseExpVec_AVX2_c3_f64           = archsimd.BroadcastFloat64x4(float64(expC3_f64))
	BaseSinVec_AVX2_c2_f32           = archsimd.BroadcastFloat32x8(float32(trigC2_f32))
	BaseCosVec_AVX2_piOver2Hi_f32    = archsimd.BroadcastFloat32x8(float32(trigPiOver2Hi_f32))
	BaseCosVec_AVX2_c4_f64           = archsimd.BroadcastFloat64x4(float64(trigC4_f64))
	BaseLog2Vec_AVX2_log2E_f32       = archsimd.BroadcastFloat32x8(float32(log2E_f32))
	BaseCoshVec_AVX2_c6_f64          = archsimd.BroadcastFloat64x4(float64(0.001388888888888889))
	BaseAcoshVec_AVX2_one_f32        = archsimd.BroadcastFloat32x8(float32(1.0))
	BaseSinVec_AVX2_intTwo_i32_f32   = archsimd.BroadcastInt32x8(2)
	BaseSinVec_AVX2_one_f32          = archsimd.BroadcastFloat32x8(float32(trigOne_f32))
	BaseErfVec_AVX2_a4_f64           = archsimd.BroadcastFloat64x4(float64(erfA4_f64))
	BaseLog10Vec_AVX2_log10E_f64     = archsimd.BroadcastFloat64x4(float64(log10E_f64))
	BaseSinhVec_AVX2_one_f32         = archsimd.BroadcastFloat32x8(float32(sinhOne_f32))
	BaseTanhVec_AVX2_negOne_f64      = archsimd.BroadcastFloat64x4(float64(tanhNegOne_f64))
	BaseLogVec_AVX2_two_f64          = archsimd.BroadcastFloat64x4(float64(logTwo_f64))
	BaseCosVec_AVX2_c1_f32           = archsimd.BroadcastFloat32x8(float32(trigC1_f32))
	BaseSinhVec_AVX2_c3_f64          = archsimd.BroadcastFloat64x4(float64(sinhC3_f64))
	BaseAcoshVec_AVX2_zero_f32       = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseCosVec_AVX2_s2_f32           = archsimd.BroadcastFloat32x8(float32(trigS2_f32))
	BaseCosVec_AVX2_intThree_i32_f32 = archsimd.BroadcastInt32x8(3)
	BaseTanhVec_AVX2_one_f32         = archsimd.BroadcastFloat32x8(float32(tanhOne_f32))
	BaseExpVec_AVX2_invLn2_f64       = archsimd.BroadcastFloat64x4(float64(expInvLn2_f64))
	BaseTanhVec_AVX2_two_f32         = archsimd.BroadcastFloat32x8(float32(2.0))
	BaseCosVec_AVX2_one_f32          = archsimd.BroadcastFloat32x8(float32(trigOne_f32))
	BaseCosVec_AVX2_s3_f32           = archsimd.BroadcastFloat32x8(float32(trigS3_f32))
	BaseErfVec_AVX2_one_f64          = archsimd.BroadcastFloat64x4(float64(erfOne_f64))
	BaseAtanhVec_AVX2_one_f32        = archsimd.BroadcastFloat32x8(float32(1.0))
	BaseLogVec_AVX2_c2_f32           = archsimd.BroadcastFloat32x8(float32(logC2_f32))
	BaseErfVec_AVX2_a1_f64           = archsimd.BroadcastFloat64x4(float64(erfA1_f64))
	BaseExpVec_AVX2_ln2Lo_f32        = archsimd.BroadcastFloat32x8(float32(expLn2Lo_f32))
	BaseExpVec_AVX2_zero_f64         = archsimd.BroadcastFloat64x4(float64(expZero_f64))
	BaseSinVec_AVX2_intThree_i32_f32 = archsimd.BroadcastInt32x8(3)
	BaseSinVec_AVX2_piOver2Hi_f64    = archsimd.BroadcastFloat64x4(float64(trigPiOver2Hi_f64))
	BaseCosVec_AVX2_c4_f32           = archsimd.BroadcastFloat32x8(float32(trigC4_f32))
	BaseErfVec_AVX2_p_f32            = archsimd.BroadcastFloat32x8(float32(erfP_f32))
	BaseErfVec_AVX2_a2_f64           = archsimd.BroadcastFloat64x4(float64(erfA2_f64))
	BaseLogVec_AVX2_c5_f64           = archsimd.BroadcastFloat64x4(float64(logC5_f64))
	BaseCosVec_AVX2_s4_f32           = archsimd.BroadcastFloat32x8(float32(trigS4_f32))
	BaseCosVec_AVX2_intTwo_i32_f32   = archsimd.BroadcastInt32x8(2)
	BaseCosVec_AVX2_s4_f64           = archsimd.BroadcastFloat64x4(float64(trigS4_f64))
	BaseErfVec_AVX2_a5_f32           = archsimd.BroadcastFloat32x8(float32(erfA5_f32))
	BaseExp2Vec_AVX2_ln2_f64         = archsimd.BroadcastFloat64x4(float64(ln2_f64))
	BaseAsinhVec_AVX2_one_f64        = archsimd.BroadcastFloat64x4(float64(1.0))
	BaseAtanhVec_AVX2_zero_f32       = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseLogVec_AVX2_c1_f64           = archsimd.BroadcastFloat64x4(float64(logC1_f64))
	BaseSinVec_AVX2_twoOverPi_f32    = archsimd.BroadcastFloat32x8(float32(trig2OverPi_f32))
	BaseExp2Vec_AVX2_ln2_f32         = archsimd.BroadcastFloat32x8(float32(ln2_f32))
	BaseSinhVec_AVX2_c7_f64          = archsimd.BroadcastFloat64x4(float64(sinhC7_f64))
	BaseAcoshVec_AVX2_zero_f64       = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseSigmoidVec_AVX2_zero_f32     = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseLogVec_AVX2_nan_f32          = archsimd.BroadcastFloat32x8(float32(0.0))
	BaseCosVec_AVX2_s3_f64           = archsimd.BroadcastFloat64x4(float64(trigS3_f64))
	BaseCosVec_AVX2_intOne_i32_f64   = archsimd.BroadcastInt32x4(1)
	BaseCosVec_AVX2_s1_f64           = archsimd.BroadcastFloat64x4(float64(trigS1_f64))
	BaseCoshVec_AVX2_c4_f64          = archsimd.BroadcastFloat64x4(float64(0.041666666666666664))
	BaseExpVec_AVX2_c6_f32           = archsimd.BroadcastFloat32x8(float32(expC6_f32))
	BaseSinVec_AVX2_piOver2Lo_f32    = archsimd.BroadcastFloat32x8(float32(trigPiOver2Lo_f32))
	BaseSinVec_AVX2_s2_f32           = archsimd.BroadcastFloat32x8(float32(trigS2_f32))
	BaseLogVec_AVX2_negInf_f32       = archsimd.BroadcastFloat32x8(float32(-1e38))
	BaseSigmoidVec_AVX2_one_f32      = archsimd.BroadcastFloat32x8(float32(sigmoidOne_f32))
	BaseExpVec_AVX2_ln2Lo_f64        = archsimd.BroadcastFloat64x4(float64(expLn2Lo_f64))
	BaseExpVec_AVX2_c6_f64           = archsimd.BroadcastFloat64x4(float64(expC6_f64))
	BaseSigmoidVec_AVX2_satHi_f64    = archsimd.BroadcastFloat64x4(float64(20.0))
	BaseSinVec_AVX2_s1_f32           = archsimd.BroadcastFloat32x8(float32(trigS1_f32))
	BaseSinVec_AVX2_c1_f32           = archsimd.BroadcastFloat32x8(float32(trigC1_f32))
	BaseSinVec_AVX2_c3_f64           = archsimd.BroadcastFloat64x4(float64(trigC3_f64))
	BaseLogVec_AVX2_c5_f32           = archsimd.BroadcastFloat32x8(float32(logC5_f32))
	BaseLogVec_AVX2_c1_f32           = archsimd.BroadcastFloat32x8(float32(logC1_f32))
	BaseCosVec_AVX2_twoOverPi_f32    = archsimd.BroadcastFloat32x8(float32(trig2OverPi_f32))
	BaseCosVec_AVX2_piOver2Lo_f64    = archsimd.BroadcastFloat64x4(float64(trigPiOver2Lo_f64))
	BaseCosVec_AVX2_c2_f64           = archsimd.BroadcastFloat64x4(float64(trigC2_f64))
	BaseErfVec_AVX2_one_f32          = archsimd.BroadcastFloat32x8(float32(erfOne_f32))
	BaseSinhVec_AVX2_c5_f32          = archsimd.BroadcastFloat32x8(float32(sinhC5_f32))
	BaseExpVec_AVX2_overflow_f32     = archsimd.BroadcastFloat32x8(float32(expOverflow_f32))
	BaseLogVec_AVX2_c3_f32           = archsimd.BroadcastFloat32x8(float32(logC3_f32))
	BaseLogVec_AVX2_c4_f64           = archsimd.BroadcastFloat64x4(float64(logC4_f64))
	BaseLogVec_AVX2_one_f64          = archsimd.BroadcastFloat64x4(float64(logOne_f64))
	BaseSinVec_AVX2_s4_f64           = archsimd.BroadcastFloat64x4(float64(trigS4_f64))
	BaseCosVec_AVX2_c1_f64           = archsimd.BroadcastFloat64x4(float64(trigC1_f64))
	BaseSinhVec_AVX2_one_f64         = archsimd.BroadcastFloat64x4(float64(sinhOne_f64))
	BaseCosVec_AVX2_piOver2Lo_f32    = archsimd.BroadcastFloat32x8(float32(trigPiOver2Lo_f32))
	BaseCosVec_AVX2_intOne_i32_f32   = archsimd.BroadcastInt32x8(1)
	BaseExpVec_AVX2_c5_f64           = archsimd.BroadcastFloat64x4(float64(expC5_f64))
	BaseSinVec_AVX2_c4_f32           = archsimd.BroadcastFloat32x8(float32(trigC4_f32))
	BaseSinVec_AVX2_intTwo_i32_f64   = archsimd.BroadcastInt32x4(2)
	BaseErfVec_AVX2_a1_f32           = archsimd.BroadcastFloat32x8(float32(erfA1_f32))
	BaseExpVec_AVX2_one_f64          = archsimd.BroadcastFloat64x4(float64(expOne_f64))
	BaseExpVec_AVX2_underflow_f32    = archsimd.BroadcastFloat32x8(float32(expUnderflow_f32))
	BaseExpVec_AVX2_c2_f64           = archsimd.BroadcastFloat64x4(float64(expC2_f64))
	BaseTanhVec_AVX2_one_f64         = archsimd.BroadcastFloat64x4(float64(tanhOne_f64))
	BaseSinVec_AVX2_intThree_i32_f64 = archsimd.BroadcastInt32x4(3)
	BaseSinVec_AVX2_piOver2Lo_f64    = archsimd.BroadcastFloat64x4(float64(trigPiOver2Lo_f64))
	BaseCoshVec_AVX2_c2_f32          = archsimd.BroadcastFloat32x8(float32(0.5))
	BaseExpVec_AVX2_one_f32          = archsimd.BroadcastFloat32x8(float32(expOne_f32))
	BaseExpVec_AVX2_ln2Hi_f64        = archsimd.BroadcastFloat64x4(float64(expLn2Hi_f64))
	BaseLogVec_AVX2_ln2Hi_f32        = archsimd.BroadcastFloat32x8(float32(logLn2Hi_f32))
	BaseLogVec_AVX2_c3_f64           = archsimd.BroadcastFloat64x4(float64(logC3_f64))
	BaseLogVec_AVX2_negInf_f64       = archsimd.BroadcastFloat64x4(float64(-1e38))
	BaseSinVec_AVX2_c2_f64           = archsimd.BroadcastFloat64x4(float64(trigC2_f64))
	BaseCosVec_AVX2_s1_f32           = archsimd.BroadcastFloat32x8(float32(trigS1_f32))
	BaseCosVec_AVX2_c2_f32           = archsimd.BroadcastFloat32x8(float32(trigC2_f32))
	BaseSigmoidVec_AVX2_one_f64      = archsimd.BroadcastFloat64x4(float64(sigmoidOne_f64))
	BaseTanhVec_AVX2_negOne_f32      = archsimd.BroadcastFloat32x8(float32(tanhNegOne_f32))
	BaseErfVec_AVX2_a5_f64           = archsimd.BroadcastFloat64x4(float64(erfA5_f64))
	BaseCoshVec_AVX2_one_f64         = archsimd.BroadcastFloat64x4(float64(1.0))
	BaseAtanhVec_AVX2_half_f64       = archsimd.BroadcastFloat64x4(float64(0.5))
	BaseSigmoidVec_AVX2_satLo_f32    = archsimd.BroadcastFloat32x8(float32(-20.0))
	BaseLogVec_AVX2_one_f32          = archsimd.BroadcastFloat32x8(float32(logOne_f32))
	BaseLogVec_AVX2_zero_f64         = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseCosVec_AVX2_piOver2Hi_f64    = archsimd.BroadcastFloat64x4(float64(trigPiOver2Hi_f64))
	BaseErfVec_AVX2_p_f64            = archsimd.BroadcastFloat64x4(float64(erfP_f64))
	BaseLog10Vec_AVX2_log10E_f32     = archsimd.BroadcastFloat32x8(float32(log10E_f32))
	BaseSinhVec_AVX2_c7_f32          = archsimd.BroadcastFloat32x8(float32(sinhC7_f32))
	BaseAcoshVec_AVX2_one_f64        = archsimd.BroadcastFloat64x4(float64(1.0))
	BaseLogVec_AVX2_c4_f32           = archsimd.BroadcastFloat32x8(float32(logC4_f32))
	BaseSinVec_AVX2_s3_f32           = archsimd.BroadcastFloat32x8(float32(trigS3_f32))
	BaseErfVec_AVX2_zero_f64         = archsimd.BroadcastFloat64x4(float64(erfZero_f64))
	BaseExpVec_AVX2_c1_f64           = archsimd.BroadcastFloat64x4(float64(expC1_f64))
	BaseTanhVec_AVX2_threshold_f32   = archsimd.BroadcastFloat32x8(float32(tanhClamp_f32))
	BaseSinVec_AVX2_s4_f32           = archsimd.BroadcastFloat32x8(float32(trigS4_f32))
	BaseSinVec_AVX2_piOver2Hi_f32    = archsimd.BroadcastFloat32x8(float32(trigPiOver2Hi_f32))
	BaseErfVec_AVX2_a3_f32           = archsimd.BroadcastFloat32x8(float32(erfA3_f32))
	BaseLog2Vec_AVX2_log2E_f64       = archsimd.BroadcastFloat64x4(float64(log2E_f64))
	BaseLogVec_AVX2_ln2Lo_f64        = archsimd.BroadcastFloat64x4(float64(logLn2Lo_f64))
	BaseCoshVec_AVX2_c2_f64          = archsimd.BroadcastFloat64x4(float64(0.5))
	BaseAsinhVec_AVX2_one_f32        = archsimd.BroadcastFloat32x8(float32(1.0))
	BaseSigmoidVec_AVX2_satLo_f64    = archsimd.BroadcastFloat64x4(float64(-20.0))
	BaseTanhVec_AVX2_two_f64         = archsimd.BroadcastFloat64x4(float64(2.0))
	BaseCosVec_AVX2_c3_f32           = archsimd.BroadcastFloat32x8(float32(trigC3_f32))
	BaseErfVec_AVX2_a2_f32           = archsimd.BroadcastFloat32x8(float32(erfA2_f32))
	BaseErfVec_AVX2_a3_f64           = archsimd.BroadcastFloat64x4(float64(erfA3_f64))
	BaseCoshVec_AVX2_c4_f32          = archsimd.BroadcastFloat32x8(float32(0.041666666666666664))
	BaseAtanhVec_AVX2_one_f64        = archsimd.BroadcastFloat64x4(float64(1.0))
	BaseLogVec_AVX2_ln2Lo_f32        = archsimd.BroadcastFloat32x8(float32(logLn2Lo_f32))
	BaseSinVec_AVX2_intOne_i32_f32   = archsimd.BroadcastInt32x8(1)
	BaseSinVec_AVX2_c4_f64           = archsimd.BroadcastFloat64x4(float64(trigC4_f64))
	BaseCosVec_AVX2_intTwo_i32_f64   = archsimd.BroadcastInt32x4(2)
	BaseExpVec_AVX2_underflow_f64    = archsimd.BroadcastFloat64x4(float64(expUnderflow_f64))
	BaseCosVec_AVX2_twoOverPi_f64    = archsimd.BroadcastFloat64x4(float64(trig2OverPi_f64))
	BaseSinVec_AVX2_s3_f64           = archsimd.BroadcastFloat64x4(float64(trigS3_f64))
	BaseExpVec_AVX2_overflow_f64     = archsimd.BroadcastFloat64x4(float64(expOverflow_f64))
	BaseLogVec_AVX2_two_f32          = archsimd.BroadcastFloat32x8(float32(logTwo_f32))
	BaseLogVec_AVX2_nan_f64          = archsimd.BroadcastFloat64x4(float64(0.0))
	BaseSinVec_AVX2_s2_f64           = archsimd.BroadcastFloat64x4(float64(trigS2_f64))
	BaseSinVec_AVX2_c1_f64           = archsimd.BroadcastFloat64x4(float64(trigC1_f64))
	BaseCosVec_AVX2_intThree_i32_f64 = archsimd.BroadcastInt32x4(3)
	BaseSinhVec_AVX2_c3_f32          = archsimd.BroadcastFloat32x8(float32(sinhC3_f32))
	BaseExpVec_AVX2_zero_f32         = archsimd.BroadcastFloat32x8(float32(expZero_f32))
	BaseExpVec_AVX2_c4_f64           = archsimd.BroadcastFloat64x4(float64(expC4_f64))
	BaseAtanhVec_AVX2_half_f32       = archsimd.BroadcastFloat32x8(float32(0.5))
	BaseExpVec_AVX2_c2_f32           = archsimd.BroadcastFloat32x8(float32(expC2_f32))
	BaseSigmoidVec_AVX2_satHi_f32    = archsimd.BroadcastFloat32x8(float32(20.0))
	BaseLogVec_AVX2_c2_f64           = archsimd.BroadcastFloat64x4(float64(logC2_f64))
)

func BaseExpVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	overflow := BaseExpVec_AVX2_overflow_f32
	underflow := BaseExpVec_AVX2_underflow_f32
	one := BaseExpVec_AVX2_one_f32
	zero := BaseExpVec_AVX2_zero_f32
	inf := archsimd.BroadcastFloat32x8(float32(expOverflow_f32 * 2))
	invLn2 := BaseExpVec_AVX2_invLn2_f32
	ln2Hi := BaseExpVec_AVX2_ln2Hi_f32
	ln2Lo := BaseExpVec_AVX2_ln2Lo_f32
	c1 := BaseExpVec_AVX2_c1_f32
	c2 := BaseExpVec_AVX2_c2_f32
	c3 := BaseExpVec_AVX2_c3_f32
	c4 := BaseExpVec_AVX2_c4_f32
	c5 := BaseExpVec_AVX2_c5_f32
	c6 := BaseExpVec_AVX2_c6_f32
	overflowMask := x.Greater(overflow)
	underflowMask := x.Less(underflow)
	kFloat := x.Mul(invLn2).RoundToEven()
	r := x.Sub(kFloat.Mul(ln2Hi))
	r = r.Sub(kFloat.Mul(ln2Lo))
	p := c6.MulAdd(r, c5)
	p = p.MulAdd(r, c4)
	p = p.MulAdd(r, c3)
	p = p.MulAdd(r, c2)
	p = p.MulAdd(r, c1)
	p = p.MulAdd(r, one)
	kInt := kFloat.ConvertToInt32()
	scale := hwy.Pow2_AVX2_F32x8(kInt)
	result := p.Mul(scale)
	result = inf.Merge(result, overflowMask)
	result = zero.Merge(result, underflowMask)
	return result
}

func BaseExpVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	overflow := BaseExpVec_AVX2_overflow_f64
	underflow := BaseExpVec_AVX2_underflow_f64
	one := BaseExpVec_AVX2_one_f64
	zero := BaseExpVec_AVX2_zero_f64
	inf := archsimd.BroadcastFloat64x4(float64(expOverflow_f64 * 2))
	invLn2 := BaseExpVec_AVX2_invLn2_f64
	ln2Hi := BaseExpVec_AVX2_ln2Hi_f64
	ln2Lo := BaseExpVec_AVX2_ln2Lo_f64
	c1 := BaseExpVec_AVX2_c1_f64
	c2 := BaseExpVec_AVX2_c2_f64
	c3 := BaseExpVec_AVX2_c3_f64
	c4 := BaseExpVec_AVX2_c4_f64
	c5 := BaseExpVec_AVX2_c5_f64
	c6 := BaseExpVec_AVX2_c6_f64
	overflowMask := x.Greater(overflow)
	underflowMask := x.Less(underflow)
	kFloat := x.Mul(invLn2).RoundToEven()
	r := x.Sub(kFloat.Mul(ln2Hi))
	r = r.Sub(kFloat.Mul(ln2Lo))
	p := c6.MulAdd(r, c5)
	p = p.MulAdd(r, c4)
	p = p.MulAdd(r, c3)
	p = p.MulAdd(r, c2)
	p = p.MulAdd(r, c1)
	p = p.MulAdd(r, one)
	kInt := kFloat.ConvertToInt32()
	scale := hwy.Pow2_AVX2_F64x4(kInt)
	result := p.Mul(scale)
	result = inf.Merge(result, overflowMask)
	result = zero.Merge(result, underflowMask)
	return result
}

func BaseSigmoidVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	one := BaseSigmoidVec_AVX2_one_f32
	zero := BaseSigmoidVec_AVX2_zero_f32
	satHi := BaseSigmoidVec_AVX2_satHi_f32
	satLo := BaseSigmoidVec_AVX2_satLo_f32
	clampedX := x.Min(satHi).Max(satLo)
	negX := archsimd.BroadcastFloat32x8(0).Sub(clampedX)
	expNegX := BaseExpVec_avx2(negX)
	result := one.Div(one.Add(expNegX))
	result = one.Merge(result, x.Greater(satHi))
	result = zero.Merge(result, x.Less(satLo))
	return result
}

func BaseSigmoidVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	one := BaseSigmoidVec_AVX2_one_f64
	zero := BaseSigmoidVec_AVX2_zero_f64
	satHi := BaseSigmoidVec_AVX2_satHi_f64
	satLo := BaseSigmoidVec_AVX2_satLo_f64
	clampedX := x.Min(satHi).Max(satLo)
	negX := archsimd.BroadcastFloat64x4(0).Sub(clampedX)
	expNegX := BaseExpVec_avx2_Float64(negX)
	result := one.Div(one.Add(expNegX))
	result = one.Merge(result, x.Greater(satHi))
	result = zero.Merge(result, x.Less(satLo))
	return result
}

func BaseTanhVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	two := BaseTanhVec_AVX2_two_f32
	one := BaseTanhVec_AVX2_one_f32
	negOne := BaseTanhVec_AVX2_negOne_f32
	threshold := BaseTanhVec_AVX2_threshold_f32
	negThreshold := archsimd.BroadcastFloat32x8(0).Sub(threshold)
	twoX := two.Mul(x)
	sigTwoX := BaseSigmoidVec_avx2(twoX)
	result := two.Mul(sigTwoX).Sub(one)
	result = one.Merge(result, x.Greater(threshold))
	result = negOne.Merge(result, x.Less(negThreshold))
	return result
}

func BaseTanhVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	two := BaseTanhVec_AVX2_two_f64
	one := BaseTanhVec_AVX2_one_f64
	negOne := BaseTanhVec_AVX2_negOne_f64
	threshold := BaseTanhVec_AVX2_threshold_f64
	negThreshold := archsimd.BroadcastFloat64x4(0).Sub(threshold)
	twoX := two.Mul(x)
	sigTwoX := BaseSigmoidVec_avx2_Float64(twoX)
	result := two.Mul(sigTwoX).Sub(one)
	result = one.Merge(result, x.Greater(threshold))
	result = negOne.Merge(result, x.Less(negThreshold))
	return result
}

func BaseLogVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	one := BaseLogVec_AVX2_one_f32
	two := BaseLogVec_AVX2_two_f32
	zero := BaseLogVec_AVX2_zero_f32
	ln2Hi := BaseLogVec_AVX2_ln2Hi_f32
	ln2Lo := BaseLogVec_AVX2_ln2Lo_f32
	negInf := BaseLogVec_AVX2_negInf_f32
	nan := BaseLogVec_AVX2_nan_f32
	c1 := BaseLogVec_AVX2_c1_f32
	c2 := BaseLogVec_AVX2_c2_f32
	c3 := BaseLogVec_AVX2_c3_f32
	c4 := BaseLogVec_AVX2_c4_f32
	c5 := BaseLogVec_AVX2_c5_f32
	zeroMask := x.Equal(zero)
	negMask := x.Less(zero)
	oneMask := x.Equal(one)
	e := x.GetExponent()
	m := x.GetMantissa()
	mLarge := m.Greater(archsimd.BroadcastFloat32x8(float32(1.414)))
	mAdjusted := m.Mul(archsimd.BroadcastFloat32x8(float32(0.5))).Merge(m, mLarge)
	eData := e.Data()
	eFloatData := make([]float32, len(eData))
	for i, v := range eData {
		eFloatData[i] = float32(v)
	}
	eFloat := archsimd.LoadFloat32x8Slice(eFloatData)
	eAdjusted := eFloat.Add(one).Merge(eFloat, mLarge)
	mMinus1 := mAdjusted.Sub(one)
	mPlus1 := mAdjusted.Add(one)
	y := mMinus1.Div(mPlus1)
	y2 := y.Mul(y)
	poly := c5.MulAdd(y2, c4)
	poly = poly.MulAdd(y2, c3)
	poly = poly.MulAdd(y2, c2)
	poly = poly.MulAdd(y2, c1)
	logM := two.Mul(y).Mul(poly)
	result := eAdjusted.MulAdd(ln2Hi, logM).Add(eAdjusted.Mul(ln2Lo))
	result = negInf.Merge(result, zeroMask)
	result = nan.Merge(result, negMask)
	result = zero.Merge(result, oneMask)
	return result
}

func BaseLogVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	one := BaseLogVec_AVX2_one_f64
	two := BaseLogVec_AVX2_two_f64
	zero := BaseLogVec_AVX2_zero_f64
	ln2Hi := BaseLogVec_AVX2_ln2Hi_f64
	ln2Lo := BaseLogVec_AVX2_ln2Lo_f64
	negInf := BaseLogVec_AVX2_negInf_f64
	nan := BaseLogVec_AVX2_nan_f64
	c1 := BaseLogVec_AVX2_c1_f64
	c2 := BaseLogVec_AVX2_c2_f64
	c3 := BaseLogVec_AVX2_c3_f64
	c4 := BaseLogVec_AVX2_c4_f64
	c5 := BaseLogVec_AVX2_c5_f64
	zeroMask := x.Equal(zero)
	negMask := x.Less(zero)
	oneMask := x.Equal(one)
	e := x.GetExponent()
	m := x.GetMantissa()
	mLarge := m.Greater(archsimd.BroadcastFloat64x4(float64(1.414)))
	mAdjusted := m.Mul(archsimd.BroadcastFloat64x4(float64(0.5))).Merge(m, mLarge)
	eData := e.Data()
	eFloatData := make([]float64, len(eData))
	for i, v := range eData {
		eFloatData[i] = float64(v)
	}
	eFloat := archsimd.LoadFloat64x4Slice(eFloatData)
	eAdjusted := eFloat.Add(one).Merge(eFloat, mLarge)
	mMinus1 := mAdjusted.Sub(one)
	mPlus1 := mAdjusted.Add(one)
	y := mMinus1.Div(mPlus1)
	y2 := y.Mul(y)
	poly := c5.MulAdd(y2, c4)
	poly = poly.MulAdd(y2, c3)
	poly = poly.MulAdd(y2, c2)
	poly = poly.MulAdd(y2, c1)
	logM := two.Mul(y).Mul(poly)
	result := eAdjusted.MulAdd(ln2Hi, logM).Add(eAdjusted.Mul(ln2Lo))
	result = negInf.Merge(result, zeroMask)
	result = nan.Merge(result, negMask)
	result = zero.Merge(result, oneMask)
	return result
}

func BaseSinVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	twoOverPi := BaseSinVec_AVX2_twoOverPi_f32
	piOver2Hi := BaseSinVec_AVX2_piOver2Hi_f32
	piOver2Lo := BaseSinVec_AVX2_piOver2Lo_f32
	one := BaseSinVec_AVX2_one_f32
	s1 := BaseSinVec_AVX2_s1_f32
	s2 := BaseSinVec_AVX2_s2_f32
	s3 := BaseSinVec_AVX2_s3_f32
	s4 := BaseSinVec_AVX2_s4_f32
	c1 := BaseSinVec_AVX2_c1_f32
	c2 := BaseSinVec_AVX2_c2_f32
	c3 := BaseSinVec_AVX2_c3_f32
	c4 := BaseSinVec_AVX2_c4_f32
	intOne := BaseSinVec_AVX2_intOne_i32_f32
	intTwo := BaseSinVec_AVX2_intTwo_i32_f32
	intThree := BaseSinVec_AVX2_intThree_i32_f32
	kFloat := x.Mul(twoOverPi).RoundToEven()
	kInt := kFloat.ConvertToInt32()
	r := x.Sub(kFloat.Mul(piOver2Hi))
	r = r.Sub(kFloat.Mul(piOver2Lo))
	r2 := r.Mul(r)
	sinPoly := s4.MulAdd(r2, s3)
	sinPoly = sinPoly.MulAdd(r2, s2)
	sinPoly = sinPoly.MulAdd(r2, s1)
	sinPoly = sinPoly.MulAdd(r2, one)
	sinR := r.Mul(sinPoly)
	cosPoly := c4.MulAdd(r2, c3)
	cosPoly = cosPoly.MulAdd(r2, c2)
	cosPoly = cosPoly.MulAdd(r2, c1)
	cosR := cosPoly.MulAdd(r2, one)
	octant := kInt.And(intThree)
	useCosMask := octant.And(intOne).Equal(intOne)
	negateMask := octant.And(intTwo).Equal(intTwo)
	sinRData := sinR.Data()
	cosRData := cosR.Data()
	resultData := make([]float32, len(sinRData))
	for i := range sinRData {
		if useCosMask.GetBit(i) {
			resultData[i] = cosRData[i]
		} else {
			resultData[i] = sinRData[i]
		}
	}
	result := archsimd.LoadFloat32x8Slice(resultData)
	negResult := archsimd.BroadcastFloat32x8(0).Sub(result)
	negResultData := negResult.Data()
	for i := range resultData {
		if negateMask.GetBit(i) {
			resultData[i] = negResultData[i]
		}
	}
	return archsimd.LoadFloat32x8Slice(resultData)
}

func BaseSinVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	twoOverPi := BaseSinVec_AVX2_twoOverPi_f64
	piOver2Hi := BaseSinVec_AVX2_piOver2Hi_f64
	piOver2Lo := BaseSinVec_AVX2_piOver2Lo_f64
	one := BaseSinVec_AVX2_one_f64
	s1 := BaseSinVec_AVX2_s1_f64
	s2 := BaseSinVec_AVX2_s2_f64
	s3 := BaseSinVec_AVX2_s3_f64
	s4 := BaseSinVec_AVX2_s4_f64
	c1 := BaseSinVec_AVX2_c1_f64
	c2 := BaseSinVec_AVX2_c2_f64
	c3 := BaseSinVec_AVX2_c3_f64
	c4 := BaseSinVec_AVX2_c4_f64
	intOne := BaseSinVec_AVX2_intOne_i32_f64
	intTwo := BaseSinVec_AVX2_intTwo_i32_f64
	intThree := BaseSinVec_AVX2_intThree_i32_f64
	kFloat := x.Mul(twoOverPi).RoundToEven()
	kInt := kFloat.ConvertToInt32()
	r := x.Sub(kFloat.Mul(piOver2Hi))
	r = r.Sub(kFloat.Mul(piOver2Lo))
	r2 := r.Mul(r)
	sinPoly := s4.MulAdd(r2, s3)
	sinPoly = sinPoly.MulAdd(r2, s2)
	sinPoly = sinPoly.MulAdd(r2, s1)
	sinPoly = sinPoly.MulAdd(r2, one)
	sinR := r.Mul(sinPoly)
	cosPoly := c4.MulAdd(r2, c3)
	cosPoly = cosPoly.MulAdd(r2, c2)
	cosPoly = cosPoly.MulAdd(r2, c1)
	cosR := cosPoly.MulAdd(r2, one)
	octant := kInt.And(intThree)
	useCosMask := octant.And(intOne).Equal(intOne)
	negateMask := octant.And(intTwo).Equal(intTwo)
	sinRData := sinR.Data()
	cosRData := cosR.Data()
	resultData := make([]float64, len(sinRData))
	for i := range sinRData {
		if useCosMask.GetBit(i) {
			resultData[i] = cosRData[i]
		} else {
			resultData[i] = sinRData[i]
		}
	}
	result := archsimd.LoadFloat64x4Slice(resultData)
	negResult := archsimd.BroadcastFloat64x4(0).Sub(result)
	negResultData := negResult.Data()
	for i := range resultData {
		if negateMask.GetBit(i) {
			resultData[i] = negResultData[i]
		}
	}
	return archsimd.LoadFloat64x4Slice(resultData)
}

func BaseCosVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	twoOverPi := BaseCosVec_AVX2_twoOverPi_f32
	piOver2Hi := BaseCosVec_AVX2_piOver2Hi_f32
	piOver2Lo := BaseCosVec_AVX2_piOver2Lo_f32
	one := BaseCosVec_AVX2_one_f32
	s1 := BaseCosVec_AVX2_s1_f32
	s2 := BaseCosVec_AVX2_s2_f32
	s3 := BaseCosVec_AVX2_s3_f32
	s4 := BaseCosVec_AVX2_s4_f32
	c1 := BaseCosVec_AVX2_c1_f32
	c2 := BaseCosVec_AVX2_c2_f32
	c3 := BaseCosVec_AVX2_c3_f32
	c4 := BaseCosVec_AVX2_c4_f32
	intOne := BaseCosVec_AVX2_intOne_i32_f32
	intTwo := BaseCosVec_AVX2_intTwo_i32_f32
	intThree := BaseCosVec_AVX2_intThree_i32_f32
	kFloat := x.Mul(twoOverPi).RoundToEven()
	kInt := kFloat.ConvertToInt32()
	r := x.Sub(kFloat.Mul(piOver2Hi))
	r = r.Sub(kFloat.Mul(piOver2Lo))
	r2 := r.Mul(r)
	sinPoly := s4.MulAdd(r2, s3)
	sinPoly = sinPoly.MulAdd(r2, s2)
	sinPoly = sinPoly.MulAdd(r2, s1)
	sinPoly = sinPoly.MulAdd(r2, one)
	sinR := r.Mul(sinPoly)
	cosPoly := c4.MulAdd(r2, c3)
	cosPoly = cosPoly.MulAdd(r2, c2)
	cosPoly = cosPoly.MulAdd(r2, c1)
	cosR := cosPoly.MulAdd(r2, one)
	cosOctant := kInt.Add(intOne).And(intThree)
	useCosMask := cosOctant.And(intOne).Equal(intOne)
	negateMask := cosOctant.And(intTwo).Equal(intTwo)
	sinRData := sinR.Data()
	cosRData := cosR.Data()
	resultData := make([]float32, len(sinRData))
	for i := range sinRData {
		if useCosMask.GetBit(i) {
			resultData[i] = cosRData[i]
		} else {
			resultData[i] = sinRData[i]
		}
	}
	result := archsimd.LoadFloat32x8Slice(resultData)
	negResult := archsimd.BroadcastFloat32x8(0).Sub(result)
	negResultData := negResult.Data()
	for i := range resultData {
		if negateMask.GetBit(i) {
			resultData[i] = negResultData[i]
		}
	}
	return archsimd.LoadFloat32x8Slice(resultData)
}

func BaseCosVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	twoOverPi := BaseCosVec_AVX2_twoOverPi_f64
	piOver2Hi := BaseCosVec_AVX2_piOver2Hi_f64
	piOver2Lo := BaseCosVec_AVX2_piOver2Lo_f64
	one := BaseCosVec_AVX2_one_f64
	s1 := BaseCosVec_AVX2_s1_f64
	s2 := BaseCosVec_AVX2_s2_f64
	s3 := BaseCosVec_AVX2_s3_f64
	s4 := BaseCosVec_AVX2_s4_f64
	c1 := BaseCosVec_AVX2_c1_f64
	c2 := BaseCosVec_AVX2_c2_f64
	c3 := BaseCosVec_AVX2_c3_f64
	c4 := BaseCosVec_AVX2_c4_f64
	intOne := BaseCosVec_AVX2_intOne_i32_f64
	intTwo := BaseCosVec_AVX2_intTwo_i32_f64
	intThree := BaseCosVec_AVX2_intThree_i32_f64
	kFloat := x.Mul(twoOverPi).RoundToEven()
	kInt := kFloat.ConvertToInt32()
	r := x.Sub(kFloat.Mul(piOver2Hi))
	r = r.Sub(kFloat.Mul(piOver2Lo))
	r2 := r.Mul(r)
	sinPoly := s4.MulAdd(r2, s3)
	sinPoly = sinPoly.MulAdd(r2, s2)
	sinPoly = sinPoly.MulAdd(r2, s1)
	sinPoly = sinPoly.MulAdd(r2, one)
	sinR := r.Mul(sinPoly)
	cosPoly := c4.MulAdd(r2, c3)
	cosPoly = cosPoly.MulAdd(r2, c2)
	cosPoly = cosPoly.MulAdd(r2, c1)
	cosR := cosPoly.MulAdd(r2, one)
	cosOctant := kInt.Add(intOne).And(intThree)
	useCosMask := cosOctant.And(intOne).Equal(intOne)
	negateMask := cosOctant.And(intTwo).Equal(intTwo)
	sinRData := sinR.Data()
	cosRData := cosR.Data()
	resultData := make([]float64, len(sinRData))
	for i := range sinRData {
		if useCosMask.GetBit(i) {
			resultData[i] = cosRData[i]
		} else {
			resultData[i] = sinRData[i]
		}
	}
	result := archsimd.LoadFloat64x4Slice(resultData)
	negResult := archsimd.BroadcastFloat64x4(0).Sub(result)
	negResultData := negResult.Data()
	for i := range resultData {
		if negateMask.GetBit(i) {
			resultData[i] = negResultData[i]
		}
	}
	return archsimd.LoadFloat64x4Slice(resultData)
}

func BaseErfVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	a1 := BaseErfVec_AVX2_a1_f32
	a2 := BaseErfVec_AVX2_a2_f32
	a3 := BaseErfVec_AVX2_a3_f32
	a4 := BaseErfVec_AVX2_a4_f32
	a5 := BaseErfVec_AVX2_a5_f32
	p := BaseErfVec_AVX2_p_f32
	one := BaseErfVec_AVX2_one_f32
	zero := BaseErfVec_AVX2_zero_f32
	absX := x.Max(archsimd.BroadcastFloat32x8(0).Sub(x))
	signMask := x.Less(zero)
	t := one.Div(one.Add(p.Mul(absX)))
	poly := a5.MulAdd(t, a4)
	poly = poly.MulAdd(t, a3)
	poly = poly.MulAdd(t, a2)
	poly = poly.MulAdd(t, a1)
	poly = poly.Mul(t)
	x2 := absX.Mul(absX)
	negX2 := archsimd.BroadcastFloat32x8(0).Sub(x2)
	expNegX2 := BaseExpVec_avx2(negX2)
	erfAbs := one.Sub(poly.Mul(expNegX2))
	erfAbs = erfAbs.Min(one).Max(zero)
	negErfAbs := archsimd.BroadcastFloat32x8(0).Sub(erfAbs)
	result := negErfAbs.Merge(erfAbs, signMask)
	return result
}

func BaseErfVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	a1 := BaseErfVec_AVX2_a1_f64
	a2 := BaseErfVec_AVX2_a2_f64
	a3 := BaseErfVec_AVX2_a3_f64
	a4 := BaseErfVec_AVX2_a4_f64
	a5 := BaseErfVec_AVX2_a5_f64
	p := BaseErfVec_AVX2_p_f64
	one := BaseErfVec_AVX2_one_f64
	zero := BaseErfVec_AVX2_zero_f64
	absX := x.Max(archsimd.BroadcastFloat64x4(0).Sub(x))
	signMask := x.Less(zero)
	t := one.Div(one.Add(p.Mul(absX)))
	poly := a5.MulAdd(t, a4)
	poly = poly.MulAdd(t, a3)
	poly = poly.MulAdd(t, a2)
	poly = poly.MulAdd(t, a1)
	poly = poly.Mul(t)
	x2 := absX.Mul(absX)
	negX2 := archsimd.BroadcastFloat64x4(0).Sub(x2)
	expNegX2 := BaseExpVec_avx2_Float64(negX2)
	erfAbs := one.Sub(poly.Mul(expNegX2))
	erfAbs = erfAbs.Min(one).Max(zero)
	negErfAbs := archsimd.BroadcastFloat64x4(0).Sub(erfAbs)
	result := negErfAbs.Merge(erfAbs, signMask)
	return result
}

func BaseLog2Vec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	log2E := BaseLog2Vec_AVX2_log2E_f32
	lnX := BaseLogVec_avx2(x)
	return lnX.Mul(log2E)
}

func BaseLog2Vec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	log2E := BaseLog2Vec_AVX2_log2E_f64
	lnX := BaseLogVec_avx2_Float64(x)
	return lnX.Mul(log2E)
}

func BaseLog10Vec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	log10E := BaseLog10Vec_AVX2_log10E_f32
	lnX := BaseLogVec_avx2(x)
	return lnX.Mul(log10E)
}

func BaseLog10Vec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	log10E := BaseLog10Vec_AVX2_log10E_f64
	lnX := BaseLogVec_avx2_Float64(x)
	return lnX.Mul(log10E)
}

func BaseExp2Vec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	ln2 := BaseExp2Vec_AVX2_ln2_f32
	xLn2 := x.Mul(ln2)
	return BaseExpVec_avx2(xLn2)
}

func BaseExp2Vec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	ln2 := BaseExp2Vec_AVX2_ln2_f64
	xLn2 := x.Mul(ln2)
	return BaseExpVec_avx2_Float64(xLn2)
}

func BaseSinhVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	one := BaseSinhVec_AVX2_one_f32
	c3 := BaseSinhVec_AVX2_c3_f32
	c5 := BaseSinhVec_AVX2_c5_f32
	c7 := BaseSinhVec_AVX2_c7_f32
	x2 := x.Mul(x)
	poly := c7.MulAdd(x2, c5)
	poly = poly.MulAdd(x2, c3)
	poly = poly.MulAdd(x2, one)
	return x.Mul(poly)
}

func BaseSinhVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	one := BaseSinhVec_AVX2_one_f64
	c3 := BaseSinhVec_AVX2_c3_f64
	c5 := BaseSinhVec_AVX2_c5_f64
	c7 := BaseSinhVec_AVX2_c7_f64
	x2 := x.Mul(x)
	poly := c7.MulAdd(x2, c5)
	poly = poly.MulAdd(x2, c3)
	poly = poly.MulAdd(x2, one)
	return x.Mul(poly)
}

func BaseCoshVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	one := BaseCoshVec_AVX2_one_f32
	c2 := BaseCoshVec_AVX2_c2_f32
	c4 := BaseCoshVec_AVX2_c4_f32
	c6 := BaseCoshVec_AVX2_c6_f32
	x2 := x.Mul(x)
	poly := c6.MulAdd(x2, c4)
	poly = poly.MulAdd(x2, c2)
	return poly.MulAdd(x2, one)
}

func BaseCoshVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	one := BaseCoshVec_AVX2_one_f64
	c2 := BaseCoshVec_AVX2_c2_f64
	c4 := BaseCoshVec_AVX2_c4_f64
	c6 := BaseCoshVec_AVX2_c6_f64
	x2 := x.Mul(x)
	poly := c6.MulAdd(x2, c4)
	poly = poly.MulAdd(x2, c2)
	return poly.MulAdd(x2, one)
}

func BaseAsinhVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	one := BaseAsinhVec_AVX2_one_f32
	x2 := x.Mul(x)
	x2Plus1 := x2.Add(one)
	sqrtPart := x2Plus1.Sqrt()
	arg := x.Add(sqrtPart)
	return BaseLogVec_avx2(arg)
}

func BaseAsinhVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	one := BaseAsinhVec_AVX2_one_f64
	x2 := x.Mul(x)
	x2Plus1 := x2.Add(one)
	sqrtPart := x2Plus1.Sqrt()
	arg := x.Add(sqrtPart)
	return BaseLogVec_avx2_Float64(arg)
}

func BaseAcoshVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	one := BaseAcoshVec_AVX2_one_f32
	zero := BaseAcoshVec_AVX2_zero_f32
	x2 := x.Mul(x)
	x2Minus1 := x2.Sub(one)
	sqrtPart := x2Minus1.Sqrt()
	arg := x.Add(sqrtPart)
	result := BaseLogVec_avx2(arg)
	oneMask := x.Equal(one)
	result = zero.Merge(result, oneMask)
	return result
}

func BaseAcoshVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	one := BaseAcoshVec_AVX2_one_f64
	zero := BaseAcoshVec_AVX2_zero_f64
	x2 := x.Mul(x)
	x2Minus1 := x2.Sub(one)
	sqrtPart := x2Minus1.Sqrt()
	arg := x.Add(sqrtPart)
	result := BaseLogVec_avx2_Float64(arg)
	oneMask := x.Equal(one)
	result = zero.Merge(result, oneMask)
	return result
}

func BaseAtanhVec_avx2(x archsimd.Float32x8) archsimd.Float32x8 {
	one := BaseAtanhVec_AVX2_one_f32
	half := BaseAtanhVec_AVX2_half_f32
	zero := BaseAtanhVec_AVX2_zero_f32
	onePlusX := one.Add(x)
	oneMinusX := one.Sub(x)
	ratio := onePlusX.Div(oneMinusX)
	logRatio := BaseLogVec_avx2(ratio)
	result := half.Mul(logRatio)
	zeroMask := x.Equal(zero)
	result = zero.Merge(result, zeroMask)
	return result
}

func BaseAtanhVec_avx2_Float64(x archsimd.Float64x4) archsimd.Float64x4 {
	one := BaseAtanhVec_AVX2_one_f64
	half := BaseAtanhVec_AVX2_half_f64
	zero := BaseAtanhVec_AVX2_zero_f64
	onePlusX := one.Add(x)
	oneMinusX := one.Sub(x)
	ratio := onePlusX.Div(oneMinusX)
	logRatio := BaseLogVec_avx2_Float64(ratio)
	result := half.Mul(logRatio)
	zeroMask := x.Equal(zero)
	result = zero.Merge(result, zeroMask)
	return result
}
