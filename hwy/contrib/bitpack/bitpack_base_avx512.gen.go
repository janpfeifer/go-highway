// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package bitpack

import (
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
)

func BasePack32_avx512(src []uint32, bitWidth int, dst []byte) int {
	if len(src) == 0 || bitWidth == 0 {
		return 0
	}
	if bitWidth > 32 {
		bitWidth = 32
	}
	lanes := 16
	mask := uint32((1 << bitWidth) - 1)
	maskVec := archsimd.BroadcastUint32x16(mask)
	bitPos := 0
	bytePos := 0
	var i int
	for i = 0; i+lanes*3 <= len(src); i += lanes * 3 {
		v := archsimd.LoadUint32x16((*[16]uint32)(unsafe.Pointer(&src[i])))
		v = v.And(maskVec)
		for lane := range lanes {
			val := hwy.GetLane_AVX512_Uint32x16(v, lane)
			{
				remaining_1 := bitWidth
				for remaining_1 > 0 {
					bitsAvailable_1 := 8 - *&bitPos
					bitsToWrite_1 := min(remaining_1, bitsAvailable_1)
					writeMask_1 := uint32((1 << bitsToWrite_1) - 1)
					bits_1 := val & writeMask_1
					val >>= bitsToWrite_1
					remaining_1 -= bitsToWrite_1
					dst[*&bytePos] |= byte(bits_1 << *&bitPos)
					*&bitPos += bitsToWrite_1
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
		v1 := archsimd.LoadUint32x16((*[16]uint32)(unsafe.Pointer(&src[i+16])))
		v1 = v1.And(maskVec)
		for lane := range lanes {
			val1 := hwy.GetLane_AVX512_Uint32x16(v1, lane)
			{
				remaining_11 := bitWidth
				for remaining_11 > 0 {
					bitsAvailable_11 := 8 - *&bitPos
					bitsToWrite_11 := min(remaining_11, bitsAvailable_11)
					writeMask_11 := uint32((1 << bitsToWrite_11) - 1)
					bits_11 := val1 & writeMask_11
					val1 >>= bitsToWrite_11
					remaining_11 -= bitsToWrite_11
					dst[*&bytePos] |= byte(bits_11 << *&bitPos)
					*&bitPos += bitsToWrite_11
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
		v2 := archsimd.LoadUint32x16((*[16]uint32)(unsafe.Pointer(&src[i+32])))
		v2 = v2.And(maskVec)
		for lane := range lanes {
			val2 := hwy.GetLane_AVX512_Uint32x16(v2, lane)
			{
				remaining_12 := bitWidth
				for remaining_12 > 0 {
					bitsAvailable_12 := 8 - *&bitPos
					bitsToWrite_12 := min(remaining_12, bitsAvailable_12)
					writeMask_12 := uint32((1 << bitsToWrite_12) - 1)
					bits_12 := val2 & writeMask_12
					val2 >>= bitsToWrite_12
					remaining_12 -= bitsToWrite_12
					dst[*&bytePos] |= byte(bits_12 << *&bitPos)
					*&bitPos += bitsToWrite_12
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
	}
	for ; i < len(src); i++ {
		val := src[i] & mask
		{
			remaining_2 := bitWidth
			for remaining_2 > 0 {
				bitsAvailable_2 := 8 - *&bitPos
				bitsToWrite_2 := min(remaining_2, bitsAvailable_2)
				writeMask_2 := uint32((1 << bitsToWrite_2) - 1)
				bits_2 := val & writeMask_2
				val >>= bitsToWrite_2
				remaining_2 -= bitsToWrite_2
				dst[*&bytePos] |= byte(bits_2 << *&bitPos)
				*&bitPos += bitsToWrite_2
				if *&bitPos >= 8 {
					*&bitPos = 0
					*&bytePos++
				}
			}
		}
	}
	if bitPos > 0 {
		return bytePos + 1
	}
	return bytePos
}

func BaseUnpack32_avx512(src []byte, bitWidth int, dst []uint32) int {
	if len(src) == 0 || bitWidth == 0 || len(dst) == 0 {
		return 0
	}
	if bitWidth > 32 {
		bitWidth = 32
	}
	lanes := 64
	mask := uint32((1 << bitWidth) - 1)
	bitPos := 0
	bytePos := 0
	totalBits := len(src) * 8
	var i int
	for i = 0; i < len(dst); i++ {
		if bytePos*8+bitPos+bitWidth > totalBits {
			break
		}
		dst[i] = unpackValue32(mask, bitWidth, &bitPos, &bytePos, src)
	}
	_ = lanes
	return i
}

func BasePack64_avx512(src []uint64, bitWidth int, dst []byte) int {
	if len(src) == 0 || bitWidth == 0 {
		return 0
	}
	if bitWidth > 64 {
		bitWidth = 64
	}
	lanes := 8
	var mask uint64
	if bitWidth == 64 {
		mask = ^uint64(0)
	} else {
		mask = (1 << bitWidth) - 1
	}
	maskVec := archsimd.BroadcastUint64x8(mask)
	bitPos := 0
	bytePos := 0
	var i int
	for i = 0; i+lanes*3 <= len(src); i += lanes * 3 {
		v := archsimd.LoadUint64x8((*[8]uint64)(unsafe.Pointer(&src[i])))
		v = v.And(maskVec)
		for lane := range lanes {
			val := hwy.GetLane_AVX512_Uint64x8(v, lane)
			{
				remaining_1 := bitWidth
				for remaining_1 > 0 {
					bitsAvailable_1 := 8 - *&bitPos
					bitsToWrite_1 := min(remaining_1, bitsAvailable_1)
					writeMask_1 := uint64((1 << bitsToWrite_1) - 1)
					bits_1 := val & writeMask_1
					val >>= bitsToWrite_1
					remaining_1 -= bitsToWrite_1
					dst[*&bytePos] |= byte(bits_1 << *&bitPos)
					*&bitPos += bitsToWrite_1
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
		v1 := archsimd.LoadUint64x8((*[8]uint64)(unsafe.Pointer(&src[i+8])))
		v1 = v1.And(maskVec)
		for lane := range lanes {
			val1 := hwy.GetLane_AVX512_Uint64x8(v1, lane)
			{
				remaining_11 := bitWidth
				for remaining_11 > 0 {
					bitsAvailable_11 := 8 - *&bitPos
					bitsToWrite_11 := min(remaining_11, bitsAvailable_11)
					writeMask_11 := uint64((1 << bitsToWrite_11) - 1)
					bits_11 := val1 & writeMask_11
					val1 >>= bitsToWrite_11
					remaining_11 -= bitsToWrite_11
					dst[*&bytePos] |= byte(bits_11 << *&bitPos)
					*&bitPos += bitsToWrite_11
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
		v2 := archsimd.LoadUint64x8((*[8]uint64)(unsafe.Pointer(&src[i+16])))
		v2 = v2.And(maskVec)
		for lane := range lanes {
			val2 := hwy.GetLane_AVX512_Uint64x8(v2, lane)
			{
				remaining_12 := bitWidth
				for remaining_12 > 0 {
					bitsAvailable_12 := 8 - *&bitPos
					bitsToWrite_12 := min(remaining_12, bitsAvailable_12)
					writeMask_12 := uint64((1 << bitsToWrite_12) - 1)
					bits_12 := val2 & writeMask_12
					val2 >>= bitsToWrite_12
					remaining_12 -= bitsToWrite_12
					dst[*&bytePos] |= byte(bits_12 << *&bitPos)
					*&bitPos += bitsToWrite_12
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
	}
	for ; i < len(src); i++ {
		val := src[i] & mask
		{
			remaining_2 := bitWidth
			for remaining_2 > 0 {
				bitsAvailable_2 := 8 - *&bitPos
				bitsToWrite_2 := min(remaining_2, bitsAvailable_2)
				writeMask_2 := uint64((1 << bitsToWrite_2) - 1)
				bits_2 := val & writeMask_2
				val >>= bitsToWrite_2
				remaining_2 -= bitsToWrite_2
				dst[*&bytePos] |= byte(bits_2 << *&bitPos)
				*&bitPos += bitsToWrite_2
				if *&bitPos >= 8 {
					*&bitPos = 0
					*&bytePos++
				}
			}
		}
	}
	if bitPos > 0 {
		return bytePos + 1
	}
	return bytePos
}

func BaseUnpack64_avx512(src []byte, bitWidth int, dst []uint64) int {
	if len(src) == 0 || bitWidth == 0 || len(dst) == 0 {
		return 0
	}
	if bitWidth > 64 {
		bitWidth = 64
	}
	lanes := 64
	var mask uint64
	if bitWidth == 64 {
		mask = ^uint64(0)
	} else {
		mask = (1 << bitWidth) - 1
	}
	bitPos := 0
	bytePos := 0
	totalBits := len(src) * 8
	var i int
	for i = 0; i < len(dst); i++ {
		if bytePos*8+bitPos+bitWidth > totalBits {
			break
		}
		dst[i] = unpackValue64(mask, bitWidth, &bitPos, &bytePos, src)
	}
	_ = lanes
	return i
}

func BaseDeltaEncode32_avx512(src []uint32, base uint32, dst []uint32) {
	if len(src) == 0 {
		return
	}
	if len(dst) < len(src) {
		return
	}
	lanes := 16
	dst[0] = src[0] - base
	prev := src[0]
	var i int
	i = 1
	for ; i+lanes*3 <= len(src); i += lanes * 3 {
		curr := archsimd.LoadUint32x16((*[16]uint32)(unsafe.Pointer(&src[i])))
		prevVec := archsimd.LoadUint32x16((*[16]uint32)(unsafe.Pointer(&src[i-1])))
		delta := curr.Sub(prevVec)
		delta.Store((*[16]uint32)(unsafe.Pointer(&dst[i])))
		prev = src[i+lanes-1]
		curr1 := archsimd.LoadUint32x16((*[16]uint32)(unsafe.Pointer(&src[i+16])))
		prevVec1 := archsimd.LoadUint32x16((*[16]uint32)(unsafe.Pointer(&src[i-1+16])))
		delta1 := curr1.Sub(prevVec1)
		delta1.Store((*[16]uint32)(unsafe.Pointer(&dst[i+16])))
		prev = src[i+lanes-1]
		curr2 := archsimd.LoadUint32x16((*[16]uint32)(unsafe.Pointer(&src[i+32])))
		prevVec2 := archsimd.LoadUint32x16((*[16]uint32)(unsafe.Pointer(&src[i-1+32])))
		delta2 := curr2.Sub(prevVec2)
		delta2.Store((*[16]uint32)(unsafe.Pointer(&dst[i+32])))
		prev = src[i+lanes-1]
	}
	for ; i < len(src); i++ {
		dst[i] = src[i] - prev
		prev = src[i]
	}
}

func BaseDeltaEncode64_avx512(src []uint64, base uint64, dst []uint64) {
	if len(src) == 0 {
		return
	}
	if len(dst) < len(src) {
		return
	}
	lanes := 8
	dst[0] = src[0] - base
	prev := src[0]
	var i int
	i = 1
	for ; i+lanes*3 <= len(src); i += lanes * 3 {
		curr := archsimd.LoadUint64x8((*[8]uint64)(unsafe.Pointer(&src[i])))
		prevVec := archsimd.LoadUint64x8((*[8]uint64)(unsafe.Pointer(&src[i-1])))
		delta := curr.Sub(prevVec)
		delta.Store((*[8]uint64)(unsafe.Pointer(&dst[i])))
		prev = src[i+lanes-1]
		curr1 := archsimd.LoadUint64x8((*[8]uint64)(unsafe.Pointer(&src[i+8])))
		prevVec1 := archsimd.LoadUint64x8((*[8]uint64)(unsafe.Pointer(&src[i-1+8])))
		delta1 := curr1.Sub(prevVec1)
		delta1.Store((*[8]uint64)(unsafe.Pointer(&dst[i+8])))
		prev = src[i+lanes-1]
		curr2 := archsimd.LoadUint64x8((*[8]uint64)(unsafe.Pointer(&src[i+16])))
		prevVec2 := archsimd.LoadUint64x8((*[8]uint64)(unsafe.Pointer(&src[i-1+16])))
		delta2 := curr2.Sub(prevVec2)
		delta2.Store((*[8]uint64)(unsafe.Pointer(&dst[i+16])))
		prev = src[i+lanes-1]
	}
	for ; i < len(src); i++ {
		dst[i] = src[i] - prev
		prev = src[i]
	}
}
