// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build arm64

package bitpack

import (
	"unsafe"

	"github.com/ajroetker/go-highway/hwy/asm"
)

func BasePack32_neon(src []uint32, bitWidth int, dst []byte) int {
	if len(src) == 0 || bitWidth == 0 {
		return 0
	}
	if bitWidth > 32 {
		bitWidth = 32
	}
	lanes := 4
	mask := uint32((1 << bitWidth) - 1)
	maskVec := asm.BroadcastUint32x4(mask)
	bitPos := 0
	bytePos := 0
	var i int
	for i = 0; i+lanes*2 <= len(src); i += lanes * 2 {
		v := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i])))
		v = v.And(maskVec)
		for lane := range lanes {
			val := v.Get(lane)
			{
				remaining_1 := bitWidth
				for remaining_1 > 0 {
					bitsAvailable_1 := 8 - *&bitPos
					bitsToWrite_1 := min(remaining_1, bitsAvailable_1)
					writeMask_1 := uint32((1 << bitsToWrite_1) - 1)
					bits_1 := val & writeMask_1
					val >>= bitsToWrite_1
					remaining_1 -= bitsToWrite_1
					dst[*&bytePos] |= byte(bits_1 << *&bitPos)
					*&bitPos += bitsToWrite_1
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
		v1 := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i+4])))
		v1 = v1.And(maskVec)
		for lane := range lanes {
			val1 := v1.Get(lane)
			{
				remaining_11 := bitWidth
				for remaining_11 > 0 {
					bitsAvailable_11 := 8 - *&bitPos
					bitsToWrite_11 := min(remaining_11, bitsAvailable_11)
					writeMask_11 := uint32((1 << bitsToWrite_11) - 1)
					bits_11 := val1 & writeMask_11
					val1 >>= bitsToWrite_11
					remaining_11 -= bitsToWrite_11
					dst[*&bytePos] |= byte(bits_11 << *&bitPos)
					*&bitPos += bitsToWrite_11
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
	}
	for ; i < len(src); i++ {
		val := src[i] & mask
		{
			remaining_2 := bitWidth
			for remaining_2 > 0 {
				bitsAvailable_2 := 8 - *&bitPos
				bitsToWrite_2 := min(remaining_2, bitsAvailable_2)
				writeMask_2 := uint32((1 << bitsToWrite_2) - 1)
				bits_2 := val & writeMask_2
				val >>= bitsToWrite_2
				remaining_2 -= bitsToWrite_2
				dst[*&bytePos] |= byte(bits_2 << *&bitPos)
				*&bitPos += bitsToWrite_2
				if *&bitPos >= 8 {
					*&bitPos = 0
					*&bytePos++
				}
			}
		}
	}
	if bitPos > 0 {
		return bytePos + 1
	}
	return bytePos
}

func BaseUnpack32_neon(src []byte, bitWidth int, dst []uint32) int {
	if len(src) == 0 || bitWidth == 0 || len(dst) == 0 {
		return 0
	}
	if bitWidth > 32 {
		bitWidth = 32
	}
	lanes := 16
	mask := uint32((1 << bitWidth) - 1)
	bitPos := 0
	bytePos := 0
	totalBits := len(src) * 8
	var i int
	for i = 0; i < len(dst); i++ {
		if bytePos*8+bitPos+bitWidth > totalBits {
			break
		}
		dst[i] = unpackValue32(mask, bitWidth, &bitPos, &bytePos, src)
	}
	_ = lanes
	return i
}

func BasePack64_neon(src []uint64, bitWidth int, dst []byte) int {
	if len(src) == 0 || bitWidth == 0 {
		return 0
	}
	if bitWidth > 64 {
		bitWidth = 64
	}
	lanes := 2
	var mask uint64
	if bitWidth == 64 {
		mask = ^uint64(0)
	} else {
		mask = (1 << bitWidth) - 1
	}
	maskVec := asm.BroadcastUint64x2(mask)
	bitPos := 0
	bytePos := 0
	var i int
	for i = 0; i+lanes*2 <= len(src); i += lanes * 2 {
		v := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i])))
		v = v.And(maskVec)
		for lane := range lanes {
			val := v.Get(lane)
			{
				remaining_1 := bitWidth
				for remaining_1 > 0 {
					bitsAvailable_1 := 8 - *&bitPos
					bitsToWrite_1 := min(remaining_1, bitsAvailable_1)
					writeMask_1 := uint64((1 << bitsToWrite_1) - 1)
					bits_1 := val & writeMask_1
					val >>= bitsToWrite_1
					remaining_1 -= bitsToWrite_1
					dst[*&bytePos] |= byte(bits_1 << *&bitPos)
					*&bitPos += bitsToWrite_1
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
		v1 := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i+2])))
		v1 = v1.And(maskVec)
		for lane := range lanes {
			val1 := v1.Get(lane)
			{
				remaining_11 := bitWidth
				for remaining_11 > 0 {
					bitsAvailable_11 := 8 - *&bitPos
					bitsToWrite_11 := min(remaining_11, bitsAvailable_11)
					writeMask_11 := uint64((1 << bitsToWrite_11) - 1)
					bits_11 := val1 & writeMask_11
					val1 >>= bitsToWrite_11
					remaining_11 -= bitsToWrite_11
					dst[*&bytePos] |= byte(bits_11 << *&bitPos)
					*&bitPos += bitsToWrite_11
					if *&bitPos >= 8 {
						*&bitPos = 0
						*&bytePos++
					}
				}
			}
		}
	}
	for ; i < len(src); i++ {
		val := src[i] & mask
		{
			remaining_2 := bitWidth
			for remaining_2 > 0 {
				bitsAvailable_2 := 8 - *&bitPos
				bitsToWrite_2 := min(remaining_2, bitsAvailable_2)
				writeMask_2 := uint64((1 << bitsToWrite_2) - 1)
				bits_2 := val & writeMask_2
				val >>= bitsToWrite_2
				remaining_2 -= bitsToWrite_2
				dst[*&bytePos] |= byte(bits_2 << *&bitPos)
				*&bitPos += bitsToWrite_2
				if *&bitPos >= 8 {
					*&bitPos = 0
					*&bytePos++
				}
			}
		}
	}
	if bitPos > 0 {
		return bytePos + 1
	}
	return bytePos
}

func BaseUnpack64_neon(src []byte, bitWidth int, dst []uint64) int {
	if len(src) == 0 || bitWidth == 0 || len(dst) == 0 {
		return 0
	}
	if bitWidth > 64 {
		bitWidth = 64
	}
	lanes := 16
	var mask uint64
	if bitWidth == 64 {
		mask = ^uint64(0)
	} else {
		mask = (1 << bitWidth) - 1
	}
	bitPos := 0
	bytePos := 0
	totalBits := len(src) * 8
	var i int
	for i = 0; i < len(dst); i++ {
		if bytePos*8+bitPos+bitWidth > totalBits {
			break
		}
		dst[i] = unpackValue64(mask, bitWidth, &bitPos, &bytePos, src)
	}
	_ = lanes
	return i
}

func BaseDeltaEncode32_neon(src []uint32, base uint32, dst []uint32) {
	if len(src) == 0 {
		return
	}
	if len(dst) < len(src) {
		return
	}
	lanes := 4
	dst[0] = src[0] - base
	prev := src[0]
	var i int
	i = 1
	for ; i+lanes*4 <= len(src); i += lanes * 4 {
		curr := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i])))
		prevVec := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i-1])))
		delta := curr.Sub(prevVec)
		delta.Store((*[4]uint32)(unsafe.Pointer(&dst[i])))
		prev = src[i+lanes-1]
		curr1 := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i+4])))
		prevVec1 := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i-1+4])))
		delta1 := curr1.Sub(prevVec1)
		delta1.Store((*[4]uint32)(unsafe.Pointer(&dst[i+4])))
		prev = src[i+lanes-1]
		curr2 := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i+8])))
		prevVec2 := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i-1+8])))
		delta2 := curr2.Sub(prevVec2)
		delta2.Store((*[4]uint32)(unsafe.Pointer(&dst[i+8])))
		prev = src[i+lanes-1]
		curr3 := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i+12])))
		prevVec3 := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&src[i-1+12])))
		delta3 := curr3.Sub(prevVec3)
		delta3.Store((*[4]uint32)(unsafe.Pointer(&dst[i+12])))
		prev = src[i+lanes-1]
	}
	for ; i < len(src); i++ {
		dst[i] = src[i] - prev
		prev = src[i]
	}
}

func BaseDeltaEncode64_neon(src []uint64, base uint64, dst []uint64) {
	if len(src) == 0 {
		return
	}
	if len(dst) < len(src) {
		return
	}
	lanes := 2
	dst[0] = src[0] - base
	prev := src[0]
	var i int
	i = 1
	for ; i+lanes*4 <= len(src); i += lanes * 4 {
		curr := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i])))
		prevVec := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i-1])))
		delta := curr.Sub(prevVec)
		delta.Store((*[2]uint64)(unsafe.Pointer(&dst[i])))
		prev = src[i+lanes-1]
		curr1 := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i+2])))
		prevVec1 := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i-1+2])))
		delta1 := curr1.Sub(prevVec1)
		delta1.Store((*[2]uint64)(unsafe.Pointer(&dst[i+2])))
		prev = src[i+lanes-1]
		curr2 := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i+4])))
		prevVec2 := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i-1+4])))
		delta2 := curr2.Sub(prevVec2)
		delta2.Store((*[2]uint64)(unsafe.Pointer(&dst[i+4])))
		prev = src[i+lanes-1]
		curr3 := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i+6])))
		prevVec3 := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&src[i-1+6])))
		delta3 := curr3.Sub(prevVec3)
		delta3.Store((*[2]uint64)(unsafe.Pointer(&dst[i+6])))
		prev = src[i+lanes-1]
	}
	for ; i < len(src); i++ {
		dst[i] = src[i] - prev
		prev = src[i]
	}
}
