// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package sort

import (
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
)

func BaseCompressPartition3Way_avx2(data []float32, pivot float32) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_avx2_Float64(data []float64, pivot float64) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_avx2_Int32(data []int32, pivot int32) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_avx2_Int64(data []int64, pivot int64) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_avx2_Uint32(data []uint32, pivot uint32) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_avx2_Uint64(data []uint64, pivot uint64) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition_avx2(data []float32, pivot float32) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 8
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastFloat32x8(pivot)
	preloadL := make([]float32, preloadSize)
	preloadR := make([]float32, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Float32x8
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&data[readR])))
		} else {
			v = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_F32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_F32x8(v, maskLess)
		remaining -= lanes
		compressed.Store((*[8]float32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[8]float32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+16 <= preloadSize; i += lanes * 2 {
		v := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_F32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_F32x8(v, maskLess)
		remaining -= lanes
		compressed.Store((*[8]float32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[8]float32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&preloadL[i+8])))
		maskLess1 := v1.Less(pivotVec)
		numLess1 := hwy.CountTrue_AVX2_F32x8(maskLess1)
		compressed1, _ := hwy.Compress_AVX2_F32x8(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[8]float32)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[8]float32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+8 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_F32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_F32x8(v, maskLess)
		remaining -= lanes
		compressed.Store((*[8]float32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[8]float32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]float32
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+8 <= preloadSize; i += lanes {
		v := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_F32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_F32x8(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_avx2_Float64(data []float64, pivot float64) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 4
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastFloat64x4(pivot)
	preloadL := make([]float64, preloadSize)
	preloadR := make([]float64, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Float64x4
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&data[readR])))
		} else {
			v = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_F64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_F64x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]float64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]float64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+8 <= preloadSize; i += lanes * 2 {
		v := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_F64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_F64x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]float64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]float64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&preloadL[i+4])))
		maskLess1 := v1.Less(pivotVec)
		numLess1 := hwy.CountTrue_AVX2_F64x4(maskLess1)
		compressed1, _ := hwy.Compress_AVX2_F64x4(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[4]float64)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[4]float64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+4 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_F64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_F64x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]float64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]float64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]float64
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+4 <= preloadSize; i += lanes {
		v := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_F64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_F64x4(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_avx2_Int32(data []int32, pivot int32) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 8
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastInt32x8(pivot)
	preloadL := make([]int32, preloadSize)
	preloadR := make([]int32, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Int32x8
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadInt32x8((*[8]int32)(unsafe.Pointer(&data[readR])))
		} else {
			v = archsimd.LoadInt32x8((*[8]int32)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_I32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_I32x8(v, maskLess)
		remaining -= lanes
		compressed.Store((*[8]int32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[8]int32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+16 <= preloadSize; i += lanes * 2 {
		v := archsimd.LoadInt32x8((*[8]int32)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_I32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_I32x8(v, maskLess)
		remaining -= lanes
		compressed.Store((*[8]int32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[8]int32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := archsimd.LoadInt32x8((*[8]int32)(unsafe.Pointer(&preloadL[i+8])))
		maskLess1 := v1.Less(pivotVec)
		numLess1 := hwy.CountTrue_AVX2_I32x8(maskLess1)
		compressed1, _ := hwy.Compress_AVX2_I32x8(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[8]int32)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[8]int32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+8 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadInt32x8((*[8]int32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_I32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_I32x8(v, maskLess)
		remaining -= lanes
		compressed.Store((*[8]int32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[8]int32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]int32
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+8 <= preloadSize; i += lanes {
		v := archsimd.LoadInt32x8((*[8]int32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_I32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_I32x8(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_avx2_Int64(data []int64, pivot int64) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 4
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastInt64x4(pivot)
	preloadL := make([]int64, preloadSize)
	preloadR := make([]int64, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Int64x4
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadInt64x4((*[4]int64)(unsafe.Pointer(&data[readR])))
		} else {
			v = archsimd.LoadInt64x4((*[4]int64)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_I64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_I64x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]int64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]int64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+8 <= preloadSize; i += lanes * 2 {
		v := archsimd.LoadInt64x4((*[4]int64)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_I64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_I64x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]int64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]int64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := archsimd.LoadInt64x4((*[4]int64)(unsafe.Pointer(&preloadL[i+4])))
		maskLess1 := v1.Less(pivotVec)
		numLess1 := hwy.CountTrue_AVX2_I64x4(maskLess1)
		compressed1, _ := hwy.Compress_AVX2_I64x4(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[4]int64)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[4]int64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+4 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadInt64x4((*[4]int64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_I64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_I64x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]int64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]int64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]int64
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+4 <= preloadSize; i += lanes {
		v := archsimd.LoadInt64x4((*[4]int64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_I64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_I64x4(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_avx2_Uint32(data []uint32, pivot uint32) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 8
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastUint32x8(pivot)
	preloadL := make([]uint32, preloadSize)
	preloadR := make([]uint32, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Uint32x8
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadUint32x8((*[8]uint32)(unsafe.Pointer(&data[readR])))
		} else {
			v = archsimd.LoadUint32x8((*[8]uint32)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_Uint32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_Uint32x8(v, maskLess)
		remaining -= lanes
		compressed.Store((*[8]uint32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[8]uint32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+16 <= preloadSize; i += lanes * 2 {
		v := archsimd.LoadUint32x8((*[8]uint32)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_Uint32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_Uint32x8(v, maskLess)
		remaining -= lanes
		compressed.Store((*[8]uint32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[8]uint32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := archsimd.LoadUint32x8((*[8]uint32)(unsafe.Pointer(&preloadL[i+8])))
		maskLess1 := v1.Less(pivotVec)
		numLess1 := hwy.CountTrue_AVX2_Uint32x8(maskLess1)
		compressed1, _ := hwy.Compress_AVX2_Uint32x8(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[8]uint32)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[8]uint32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+8 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadUint32x8((*[8]uint32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_Uint32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_Uint32x8(v, maskLess)
		remaining -= lanes
		compressed.Store((*[8]uint32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[8]uint32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]uint32
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+8 <= preloadSize; i += lanes {
		v := archsimd.LoadUint32x8((*[8]uint32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_Uint32x8(maskLess)
		compressed, _ := hwy.Compress_AVX2_Uint32x8(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_avx2_Uint64(data []uint64, pivot uint64) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 4
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastUint64x4(pivot)
	preloadL := make([]uint64, preloadSize)
	preloadR := make([]uint64, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Uint64x4
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadUint64x4((*[4]uint64)(unsafe.Pointer(&data[readR])))
		} else {
			v = archsimd.LoadUint64x4((*[4]uint64)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_Uint64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_Uint64x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]uint64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]uint64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+8 <= preloadSize; i += lanes * 2 {
		v := archsimd.LoadUint64x4((*[4]uint64)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_Uint64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_Uint64x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]uint64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]uint64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := archsimd.LoadUint64x4((*[4]uint64)(unsafe.Pointer(&preloadL[i+4])))
		maskLess1 := v1.Less(pivotVec)
		numLess1 := hwy.CountTrue_AVX2_Uint64x4(maskLess1)
		compressed1, _ := hwy.Compress_AVX2_Uint64x4(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[4]uint64)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[4]uint64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+4 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadUint64x4((*[4]uint64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_Uint64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_Uint64x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]uint64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]uint64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]uint64
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+4 <= preloadSize; i += lanes {
		v := archsimd.LoadUint64x4((*[4]uint64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.Less(pivotVec)
		numLess := hwy.CountTrue_AVX2_Uint64x4(maskLess)
		compressed, _ := hwy.Compress_AVX2_Uint64x4(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}
