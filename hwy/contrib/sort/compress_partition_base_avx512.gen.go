// Code generated by hwygen. DO NOT EDIT.
//go:build amd64 && goexperiment.simd

package sort

import (
	"simd/archsimd"
)

func BaseCompressPartition3Way_avx512(data []float32, pivot float32) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_avx512_Float64(data []float64, pivot float64) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_avx512_Int32(data []int32, pivot int32) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_avx512_Int64(data []int64, pivot int64) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition_avx512(data []float32, pivot float32) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 16
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastFloat32x16(pivot)
	preloadL := make([]float32, preloadSize)
	preloadR := make([]float32, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Float32x16
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadFloat32x16Slice(data[readR:])
		} else {
			v = archsimd.LoadFloat32x16Slice(data[readL:])
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysF32x4(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	for i := 0; i+16 <= preloadSize; i += lanes {
		v := archsimd.LoadFloat32x16Slice(preloadL[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysF32x4(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	for i := 0; i+16 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadFloat32x16Slice(preloadR[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysF32x4(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	var buf [32]float32
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+16 <= preloadSize; i += lanes {
		v := archsimd.LoadFloat32x16Slice(preloadR[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysF32x4(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_avx512_Float64(data []float64, pivot float64) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 8
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastFloat64x8(pivot)
	preloadL := make([]float64, preloadSize)
	preloadR := make([]float64, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Float64x8
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadFloat64x8Slice(data[readR:])
		} else {
			v = archsimd.LoadFloat64x8Slice(data[readL:])
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysF64x2(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	for i := 0; i+8 <= preloadSize; i += lanes {
		v := archsimd.LoadFloat64x8Slice(preloadL[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysF64x2(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	for i := 0; i+8 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadFloat64x8Slice(preloadR[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysF64x2(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	var buf [32]float64
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+8 <= preloadSize; i += lanes {
		v := archsimd.LoadFloat64x8Slice(preloadR[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysF64x2(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_avx512_Int32(data []int32, pivot int32) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 16
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastInt32x16(pivot)
	preloadL := make([]int32, preloadSize)
	preloadR := make([]int32, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Int32x16
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadInt32x16Slice(data[readR:])
		} else {
			v = archsimd.LoadInt32x16Slice(data[readL:])
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysI32x4(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	for i := 0; i+16 <= preloadSize; i += lanes {
		v := archsimd.LoadInt32x16Slice(preloadL[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysI32x4(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	for i := 0; i+16 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadInt32x16Slice(preloadR[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysI32x4(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	var buf [32]int32
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+16 <= preloadSize; i += lanes {
		v := archsimd.LoadInt32x16Slice(preloadR[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysI32x4(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_avx512_Int64(data []int64, pivot int64) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 8
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := archsimd.BroadcastInt64x8(pivot)
	preloadL := make([]int64, preloadSize)
	preloadR := make([]int64, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v archsimd.Int64x8
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = archsimd.LoadInt64x8Slice(data[readR:])
		} else {
			v = archsimd.LoadInt64x8Slice(data[readL:])
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysI64x2(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	for i := 0; i+8 <= preloadSize; i += lanes {
		v := archsimd.LoadInt64x8Slice(preloadL[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysI64x2(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	for i := 0; i+8 <= preloadSize-2*lanes; i += lanes {
		v := archsimd.LoadInt64x8Slice(preloadR[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysI64x2(v, maskLess)
		remaining -= lanes
		compressed.StoreSlice(data[writeL:])
		compressed.StoreSlice(data[remaining+writeL:])
		writeL += numLess
	}
	var buf [32]int64
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+8 <= preloadSize; i += lanes {
		v := archsimd.LoadInt64x8Slice(preloadR[i:])
		maskLess := v.LessThan(pivotVec)
		numLess := archsimd.CountTrue(maskLess)
		compressed, _ := archsimd.CompressKeysI64x2(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}
