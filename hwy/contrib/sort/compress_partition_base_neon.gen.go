// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build arm64

package sort

import (
	"unsafe"

	"github.com/ajroetker/go-highway/hwy/asm"
)

func BaseCompressPartition3Way_neon(data []float32, pivot float32) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_neon_Float64(data []float64, pivot float64) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_neon_Int32(data []int32, pivot int32) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_neon_Int64(data []int64, pivot int64) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_neon_Uint32(data []uint32, pivot uint32) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition3Way_neon_Uint64(data []uint64, pivot uint64) (int, int) {
	lt := 0
	gt := len(data)
	i := 0
	for i < gt {
		if data[i] < pivot {
			data[lt], data[i] = data[i], data[lt]
			lt++
			i++
		} else if data[i] > pivot {
			gt--
			data[i], data[gt] = data[gt], data[i]
		} else {
			i++
		}
	}
	return lt, gt
}

func BaseCompressPartition_neon(data []float32, pivot float32) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 4
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := asm.BroadcastFloat32x4(pivot)
	preloadL := make([]float32, preloadSize)
	preloadR := make([]float32, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v asm.Float32x4
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&data[readR])))
		} else {
			v = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysF32x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]float32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]float32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+8 <= preloadSize; i += lanes * 2 {
		v := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysF32x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]float32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]float32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&preloadL[i+4])))
		maskLess1 := v1.LessThan(pivotVec)
		numLess1 := asm.CountTrue(maskLess1)
		compressed1, _ := asm.CompressKeysF32x4(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[4]float32)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[4]float32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+4 <= preloadSize-2*lanes; i += lanes {
		v := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysF32x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]float32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]float32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]float32
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+4 <= preloadSize; i += lanes {
		v := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysF32x4(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_neon_Float64(data []float64, pivot float64) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 2
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := asm.BroadcastFloat64x2(pivot)
	preloadL := make([]float64, preloadSize)
	preloadR := make([]float64, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v asm.Float64x2
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&data[readR])))
		} else {
			v = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysF64x2(v, maskLess)
		remaining -= lanes
		compressed.Store((*[2]float64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[2]float64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+4 <= preloadSize; i += lanes * 2 {
		v := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysF64x2(v, maskLess)
		remaining -= lanes
		compressed.Store((*[2]float64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[2]float64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&preloadL[i+2])))
		maskLess1 := v1.LessThan(pivotVec)
		numLess1 := asm.CountTrue(maskLess1)
		compressed1, _ := asm.CompressKeysF64x2(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[2]float64)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[2]float64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+2 <= preloadSize-2*lanes; i += lanes {
		v := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysF64x2(v, maskLess)
		remaining -= lanes
		compressed.Store((*[2]float64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[2]float64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]float64
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+2 <= preloadSize; i += lanes {
		v := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysF64x2(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_neon_Int32(data []int32, pivot int32) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 4
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := asm.BroadcastInt32x4(pivot)
	preloadL := make([]int32, preloadSize)
	preloadR := make([]int32, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v asm.Int32x4
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = asm.LoadInt32x4((*[4]int32)(unsafe.Pointer(&data[readR])))
		} else {
			v = asm.LoadInt32x4((*[4]int32)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysI32x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]int32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]int32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+8 <= preloadSize; i += lanes * 2 {
		v := asm.LoadInt32x4((*[4]int32)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysI32x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]int32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]int32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := asm.LoadInt32x4((*[4]int32)(unsafe.Pointer(&preloadL[i+4])))
		maskLess1 := v1.LessThan(pivotVec)
		numLess1 := asm.CountTrue(maskLess1)
		compressed1, _ := asm.CompressKeysI32x4(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[4]int32)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[4]int32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+4 <= preloadSize-2*lanes; i += lanes {
		v := asm.LoadInt32x4((*[4]int32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysI32x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]int32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]int32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]int32
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+4 <= preloadSize; i += lanes {
		v := asm.LoadInt32x4((*[4]int32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysI32x4(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_neon_Int64(data []int64, pivot int64) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 2
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := asm.BroadcastInt64x2(pivot)
	preloadL := make([]int64, preloadSize)
	preloadR := make([]int64, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v asm.Int64x2
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = asm.LoadInt64x2((*[2]int64)(unsafe.Pointer(&data[readR])))
		} else {
			v = asm.LoadInt64x2((*[2]int64)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysI64x2(v, maskLess)
		remaining -= lanes
		compressed.Store((*[2]int64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[2]int64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+4 <= preloadSize; i += lanes * 2 {
		v := asm.LoadInt64x2((*[2]int64)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysI64x2(v, maskLess)
		remaining -= lanes
		compressed.Store((*[2]int64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[2]int64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := asm.LoadInt64x2((*[2]int64)(unsafe.Pointer(&preloadL[i+2])))
		maskLess1 := v1.LessThan(pivotVec)
		numLess1 := asm.CountTrue(maskLess1)
		compressed1, _ := asm.CompressKeysI64x2(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[2]int64)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[2]int64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+2 <= preloadSize-2*lanes; i += lanes {
		v := asm.LoadInt64x2((*[2]int64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysI64x2(v, maskLess)
		remaining -= lanes
		compressed.Store((*[2]int64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[2]int64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]int64
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+2 <= preloadSize; i += lanes {
		v := asm.LoadInt64x2((*[2]int64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysI64x2(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_neon_Uint32(data []uint32, pivot uint32) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 4
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := asm.BroadcastUint32x4(pivot)
	preloadL := make([]uint32, preloadSize)
	preloadR := make([]uint32, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v asm.Uint32x4
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&data[readR])))
		} else {
			v = asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysU32x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]uint32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]uint32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+8 <= preloadSize; i += lanes * 2 {
		v := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysU32x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]uint32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]uint32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&preloadL[i+4])))
		maskLess1 := v1.LessThan(pivotVec)
		numLess1 := asm.CountTrue(maskLess1)
		compressed1, _ := asm.CompressKeysU32x4(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[4]uint32)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[4]uint32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+4 <= preloadSize-2*lanes; i += lanes {
		v := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysU32x4(v, maskLess)
		remaining -= lanes
		compressed.Store((*[4]uint32)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[4]uint32)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]uint32
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+4 <= preloadSize; i += lanes {
		v := asm.LoadUint32x4((*[4]uint32)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysU32x4(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}

func BaseCompressPartition_neon_Uint64(data []uint64, pivot uint64) int {
	n := len(data)
	if n == 0 {
		return 0
	}
	lanes := 2
	kUnroll := 4
	preloadSize := kUnroll * lanes
	if n < 2*preloadSize {
		return scalarPartition2Way(data, pivot)
	}
	middleSize := n - 2*preloadSize
	if middleSize%lanes != 0 {
		return scalarPartition2Way(data, pivot)
	}
	pivotVec := asm.BroadcastUint64x2(pivot)
	preloadL := make([]uint64, preloadSize)
	preloadR := make([]uint64, preloadSize)
	copy(preloadL, data[:preloadSize])
	copy(preloadR, data[n-preloadSize:])
	readL := preloadSize
	readR := n - preloadSize
	writeL := 0
	remaining := n
	for readL < readR {
		var v asm.Uint64x2
		capacityL := readL - writeL
		if capacityL > preloadSize {
			readR -= lanes
			v = asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&data[readR])))
		} else {
			v = asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&data[readL])))
			readL += lanes
		}
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysU64x2(v, maskLess)
		remaining -= lanes
		compressed.Store((*[2]uint64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[2]uint64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	i := 0
	for ; i+4 <= preloadSize; i += lanes * 2 {
		v := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&preloadL[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysU64x2(v, maskLess)
		remaining -= lanes
		compressed.Store((*[2]uint64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[2]uint64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
		v1 := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&preloadL[i+2])))
		maskLess1 := v1.LessThan(pivotVec)
		numLess1 := asm.CountTrue(maskLess1)
		compressed1, _ := asm.CompressKeysU64x2(v1, maskLess1)
		remaining -= lanes
		compressed1.Store((*[2]uint64)(unsafe.Pointer(&data[writeL])))
		compressed1.Store((*[2]uint64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess1
	}
	for i := 0; i+2 <= preloadSize-2*lanes; i += lanes {
		v := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysU64x2(v, maskLess)
		remaining -= lanes
		compressed.Store((*[2]uint64)(unsafe.Pointer(&data[writeL])))
		compressed.Store((*[2]uint64)(unsafe.Pointer(&data[remaining+writeL])))
		writeL += numLess
	}
	var buf [32]uint64
	bufL := 0
	writeR := writeL + remaining
	for i := preloadSize - 2*lanes; i+2 <= preloadSize; i += lanes {
		v := asm.LoadUint64x2((*[2]uint64)(unsafe.Pointer(&preloadR[i])))
		maskLess := v.LessThan(pivotVec)
		numLess := asm.CountTrue(maskLess)
		compressed, _ := asm.CompressKeysU64x2(v, maskLess)
		compressed.StoreSlice(buf[bufL:])
		bufL += numLess
		numRight := lanes - numLess
		writeR -= numRight
		copy(data[writeR:], buf[bufL:bufL+numRight])
	}
	copy(data[writeL:], buf[:bufL])
	return writeL + bufL
}
