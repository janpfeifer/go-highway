// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build arm64

package activation

import (
	"github.com/ajroetker/go-highway/hwy"
)

var GELUFloat16 func(input []hwy.Float16, output []hwy.Float16)
var GELUBFloat16 func(input []hwy.BFloat16, output []hwy.BFloat16)
var GELUFloat32 func(input []float32, output []float32)
var GELUFloat64 func(input []float64, output []float64)
var GELUApproxFloat16 func(input []hwy.Float16, output []hwy.Float16)
var GELUApproxBFloat16 func(input []hwy.BFloat16, output []hwy.BFloat16)
var GELUApproxFloat32 func(input []float32, output []float32)
var GELUApproxFloat64 func(input []float64, output []float64)
var ReLUFloat16 func(input []hwy.Float16, output []hwy.Float16)
var ReLUBFloat16 func(input []hwy.BFloat16, output []hwy.BFloat16)
var ReLUFloat32 func(input []float32, output []float32)
var ReLUFloat64 func(input []float64, output []float64)
var SiLUFloat16 func(input []hwy.Float16, output []hwy.Float16)
var SiLUBFloat16 func(input []hwy.BFloat16, output []hwy.BFloat16)
var SiLUFloat32 func(input []float32, output []float32)
var SiLUFloat64 func(input []float64, output []float64)
var LeakyReLUFloat16 func(input []hwy.Float16, output []hwy.Float16, alpha hwy.Float16)
var LeakyReLUBFloat16 func(input []hwy.BFloat16, output []hwy.BFloat16, alpha hwy.BFloat16)
var LeakyReLUFloat32 func(input []float32, output []float32, alpha float32)
var LeakyReLUFloat64 func(input []float64, output []float64, alpha float64)
var ELUFloat16 func(input []hwy.Float16, output []hwy.Float16, alpha hwy.Float16)
var ELUBFloat16 func(input []hwy.BFloat16, output []hwy.BFloat16, alpha hwy.BFloat16)
var ELUFloat32 func(input []float32, output []float32, alpha float32)
var ELUFloat64 func(input []float64, output []float64, alpha float64)

// GELU computes the Gaussian Error Linear Unit activation function.
//
// GELU(x) = x * 0.5 * (1 + erf(x / sqrt(2)))
//
// This is the exact GELU formula used in BERT, GPT, and other transformer models.
// For a faster approximation, see BaseGELUApprox.
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func GELU[T hwy.Floats](input []T, output []T) {
	switch any(input).(type) {
	case []hwy.Float16:
		GELUFloat16(any(input).([]hwy.Float16), any(output).([]hwy.Float16))
	case []hwy.BFloat16:
		GELUBFloat16(any(input).([]hwy.BFloat16), any(output).([]hwy.BFloat16))
	case []float32:
		GELUFloat32(any(input).([]float32), any(output).([]float32))
	case []float64:
		GELUFloat64(any(input).([]float64), any(output).([]float64))
	}
}

// GELUApprox computes a fast approximation of GELU.
//
// Uses the sigmoid approximation: GELU(x) = x * sigmoid(1.702 * x)
//
// This is faster than the exact formula and commonly used in practice.
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func GELUApprox[T hwy.Floats](input []T, output []T) {
	switch any(input).(type) {
	case []hwy.Float16:
		GELUApproxFloat16(any(input).([]hwy.Float16), any(output).([]hwy.Float16))
	case []hwy.BFloat16:
		GELUApproxBFloat16(any(input).([]hwy.BFloat16), any(output).([]hwy.BFloat16))
	case []float32:
		GELUApproxFloat32(any(input).([]float32), any(output).([]float32))
	case []float64:
		GELUApproxFloat64(any(input).([]float64), any(output).([]float64))
	}
}

// ReLU computes the Rectified Linear Unit activation: max(0, x).
//
// ReLU is the most common activation function, providing fast computation
// and good gradient flow for positive values.
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func ReLU[T hwy.Floats](input []T, output []T) {
	switch any(input).(type) {
	case []hwy.Float16:
		ReLUFloat16(any(input).([]hwy.Float16), any(output).([]hwy.Float16))
	case []hwy.BFloat16:
		ReLUBFloat16(any(input).([]hwy.BFloat16), any(output).([]hwy.BFloat16))
	case []float32:
		ReLUFloat32(any(input).([]float32), any(output).([]float32))
	case []float64:
		ReLUFloat64(any(input).([]float64), any(output).([]float64))
	}
}

// SiLU computes the Sigmoid Linear Unit (also known as Swish) activation.
//
// SiLU(x) = x * sigmoid(x)
//
// SiLU is used in EfficientNet, GPT-J, and other modern architectures.
// It provides smooth gradients and better optimization than ReLU in some cases.
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func SiLU[T hwy.Floats](input []T, output []T) {
	switch any(input).(type) {
	case []hwy.Float16:
		SiLUFloat16(any(input).([]hwy.Float16), any(output).([]hwy.Float16))
	case []hwy.BFloat16:
		SiLUBFloat16(any(input).([]hwy.BFloat16), any(output).([]hwy.BFloat16))
	case []float32:
		SiLUFloat32(any(input).([]float32), any(output).([]float32))
	case []float64:
		SiLUFloat64(any(input).([]float64), any(output).([]float64))
	}
}

// LeakyReLU computes the Leaky ReLU activation with a configurable slope.
//
// LeakyReLU(x) = x if x > 0, else alpha * x
//
// This helps prevent "dying ReLU" by allowing small gradients for negative values.
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func LeakyReLU[T hwy.Floats](input []T, output []T, alpha T) {
	switch any(input).(type) {
	case []hwy.Float16:
		LeakyReLUFloat16(any(input).([]hwy.Float16), any(output).([]hwy.Float16), any(alpha).(hwy.Float16))
	case []hwy.BFloat16:
		LeakyReLUBFloat16(any(input).([]hwy.BFloat16), any(output).([]hwy.BFloat16), any(alpha).(hwy.BFloat16))
	case []float32:
		LeakyReLUFloat32(any(input).([]float32), any(output).([]float32), any(alpha).(float32))
	case []float64:
		LeakyReLUFloat64(any(input).([]float64), any(output).([]float64), any(alpha).(float64))
	}
}

// ELU computes the Exponential Linear Unit activation.
//
// ELU(x) = x if x > 0, else alpha * (exp(x) - 1)
//
// ELU has smooth gradients everywhere and can push mean activations toward zero.
//
// This function dispatches to the appropriate SIMD implementation at runtime.
func ELU[T hwy.Floats](input []T, output []T, alpha T) {
	switch any(input).(type) {
	case []hwy.Float16:
		ELUFloat16(any(input).([]hwy.Float16), any(output).([]hwy.Float16), any(alpha).(hwy.Float16))
	case []hwy.BFloat16:
		ELUBFloat16(any(input).([]hwy.BFloat16), any(output).([]hwy.BFloat16), any(alpha).(hwy.BFloat16))
	case []float32:
		ELUFloat32(any(input).([]float32), any(output).([]float32), any(alpha).(float32))
	case []float64:
		ELUFloat64(any(input).([]float64), any(output).([]float64), any(alpha).(float64))
	}
}

func init() {
	if hwy.NoSimdEnv() {
		initGeluFallback()
		return
	}
	initGeluNEON()
	return
}

func initGeluNEON() {
	GELUFloat16 = BaseGELU_neon_Float16
	GELUBFloat16 = BaseGELU_neon_BFloat16
	GELUFloat32 = BaseGELU_neon
	GELUFloat64 = BaseGELU_neon_Float64
	GELUApproxFloat16 = BaseGELUApprox_neon_Float16
	GELUApproxBFloat16 = BaseGELUApprox_neon_BFloat16
	GELUApproxFloat32 = BaseGELUApprox_neon
	GELUApproxFloat64 = BaseGELUApprox_neon_Float64
	ReLUFloat16 = BaseReLU_neon_Float16
	ReLUBFloat16 = BaseReLU_neon_BFloat16
	ReLUFloat32 = BaseReLU_neon
	ReLUFloat64 = BaseReLU_neon_Float64
	SiLUFloat16 = BaseSiLU_neon_Float16
	SiLUBFloat16 = BaseSiLU_neon_BFloat16
	SiLUFloat32 = BaseSiLU_neon
	SiLUFloat64 = BaseSiLU_neon_Float64
	LeakyReLUFloat16 = BaseLeakyReLU_neon_Float16
	LeakyReLUBFloat16 = BaseLeakyReLU_neon_BFloat16
	LeakyReLUFloat32 = BaseLeakyReLU_neon
	LeakyReLUFloat64 = BaseLeakyReLU_neon_Float64
	ELUFloat16 = BaseELU_neon_Float16
	ELUBFloat16 = BaseELU_neon_BFloat16
	ELUFloat32 = BaseELU_neon
	ELUFloat64 = BaseELU_neon_Float64
}

func initGeluFallback() {
	GELUFloat16 = BaseGELU_fallback_Float16
	GELUBFloat16 = BaseGELU_fallback_BFloat16
	GELUFloat32 = BaseGELU_fallback
	GELUFloat64 = BaseGELU_fallback_Float64
	GELUApproxFloat16 = BaseGELUApprox_fallback_Float16
	GELUApproxBFloat16 = BaseGELUApprox_fallback_BFloat16
	GELUApproxFloat32 = BaseGELUApprox_fallback
	GELUApproxFloat64 = BaseGELUApprox_fallback_Float64
	ReLUFloat16 = BaseReLU_fallback_Float16
	ReLUBFloat16 = BaseReLU_fallback_BFloat16
	ReLUFloat32 = BaseReLU_fallback
	ReLUFloat64 = BaseReLU_fallback_Float64
	SiLUFloat16 = BaseSiLU_fallback_Float16
	SiLUBFloat16 = BaseSiLU_fallback_BFloat16
	SiLUFloat32 = BaseSiLU_fallback
	SiLUFloat64 = BaseSiLU_fallback_Float64
	LeakyReLUFloat16 = BaseLeakyReLU_fallback_Float16
	LeakyReLUBFloat16 = BaseLeakyReLU_fallback_BFloat16
	LeakyReLUFloat32 = BaseLeakyReLU_fallback
	LeakyReLUFloat64 = BaseLeakyReLU_fallback_Float64
	ELUFloat16 = BaseELU_fallback_Float16
	ELUBFloat16 = BaseELU_fallback_BFloat16
	ELUFloat32 = BaseELU_fallback
	ELUFloat64 = BaseELU_fallback_Float64
}
