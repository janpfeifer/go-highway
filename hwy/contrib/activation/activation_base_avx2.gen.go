// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package activation

import (
	stdmath "math"
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
	"github.com/ajroetker/go-highway/hwy/contrib/math"
)

// Hoisted constants - pre-broadcasted at package init time
var (
	BaseELU_AVX2_vOne_f32          = archsimd.BroadcastFloat32x8(1.0)
	BaseELU_AVX2_vOne_f64          = archsimd.BroadcastFloat64x4(1.0)
	BaseELU_AVX2_vZero_f32         = archsimd.BroadcastFloat32x8(0.0)
	BaseELU_AVX2_vZero_f64         = archsimd.BroadcastFloat64x4(0.0)
	BaseGELUApprox_AVX2_vCoeff_f32 = archsimd.BroadcastFloat32x8(1.702)
	BaseGELUApprox_AVX2_vCoeff_f64 = archsimd.BroadcastFloat64x4(1.702)
	BaseGELU_AVX2_vHalf_f32        = archsimd.BroadcastFloat32x8(0.5)
	BaseGELU_AVX2_vHalf_f64        = archsimd.BroadcastFloat64x4(0.5)
	BaseGELU_AVX2_vInvSqrt2_f32    = archsimd.BroadcastFloat32x8(0.7071067811865476)
	BaseGELU_AVX2_vInvSqrt2_f64    = archsimd.BroadcastFloat64x4(0.7071067811865476)
	BaseGELU_AVX2_vOne_f32         = archsimd.BroadcastFloat32x8(1.0)
	BaseGELU_AVX2_vOne_f64         = archsimd.BroadcastFloat64x4(1.0)
	BaseReLU_AVX2_vZero_f32        = archsimd.BroadcastFloat32x8(0.0)
	BaseReLU_AVX2_vZero_f64        = archsimd.BroadcastFloat64x4(0.0)
)

func BaseGELU_avx2_Float16(input []hwy.Float16, output []hwy.Float16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vHalf := asm.BroadcastFloat16x8AVX2(uint16(hwy.Float32ToFloat16(float32(0.5))))
	vOne := asm.BroadcastFloat16x8AVX2(uint16(hwy.Float32ToFloat16(float32(1.0))))
	vInvSqrt2 := asm.BroadcastFloat16x8AVX2(uint16(hwy.Float32ToFloat16(float32(0.7071067811865476))))
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		xScaled := x.Mul(vInvSqrt2)
		erfX := math.BaseErfVec_avx2_Float16(xScaled)
		onePlusErf := vOne.Add(erfX)
		halfOnePlusErf := vHalf.Mul(onePlusErf)
		result := x.Mul(halfOnePlusErf)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		xScaled1 := x1.Mul(vInvSqrt2)
		erfX1 := math.BaseErfVec_avx2_Float16(xScaled1)
		onePlusErf1 := vOne.Add(erfX1)
		halfOnePlusErf1 := vHalf.Mul(onePlusErf1)
		result1 := x1.Mul(halfOnePlusErf1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		xScaled := x.Mul(vInvSqrt2)
		erfX := math.BaseErfVec_avx2_Float16(xScaled)
		onePlusErf := vOne.Add(erfX)
		halfOnePlusErf := vHalf.Mul(onePlusErf)
		result := x.Mul(halfOnePlusErf)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i].Float32())
		output[i] = hwy.Float32ToFloat16(float32(x * 0.5 * (1.0 + stdmath.Erf(x*0.7071067811865476))))
	}
}

func BaseGELU_avx2_BFloat16(input []hwy.BFloat16, output []hwy.BFloat16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vHalf := asm.BroadcastBFloat16x8AVX2(uint16(hwy.Float32ToBFloat16(float32(0.5))))
	vOne := asm.BroadcastBFloat16x8AVX2(uint16(hwy.Float32ToBFloat16(float32(1.0))))
	vInvSqrt2 := asm.BroadcastBFloat16x8AVX2(uint16(hwy.Float32ToBFloat16(float32(0.7071067811865476))))
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		xScaled := x.Mul(vInvSqrt2)
		erfX := math.BaseErfVec_avx2_BFloat16(xScaled)
		onePlusErf := vOne.Add(erfX)
		halfOnePlusErf := vHalf.Mul(onePlusErf)
		result := x.Mul(halfOnePlusErf)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		xScaled1 := x1.Mul(vInvSqrt2)
		erfX1 := math.BaseErfVec_avx2_BFloat16(xScaled1)
		onePlusErf1 := vOne.Add(erfX1)
		halfOnePlusErf1 := vHalf.Mul(onePlusErf1)
		result1 := x1.Mul(halfOnePlusErf1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		xScaled := x.Mul(vInvSqrt2)
		erfX := math.BaseErfVec_avx2_BFloat16(xScaled)
		onePlusErf := vOne.Add(erfX)
		halfOnePlusErf := vHalf.Mul(onePlusErf)
		result := x.Mul(halfOnePlusErf)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i].Float32())
		output[i] = hwy.Float32ToBFloat16(float32(x * 0.5 * (1.0 + stdmath.Erf(x*0.7071067811865476))))
	}
}

func BaseGELU_avx2(input []float32, output []float32) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vHalf := BaseGELU_AVX2_vHalf_f32
	vOne := BaseGELU_AVX2_vOne_f32
	vInvSqrt2 := BaseGELU_AVX2_vInvSqrt2_f32
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		xScaled := x.Mul(vInvSqrt2)
		erfX := math.BaseErfVec_avx2(xScaled)
		onePlusErf := vOne.Add(erfX)
		halfOnePlusErf := vHalf.Mul(onePlusErf)
		result := x.Mul(halfOnePlusErf)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii+8])))
		xScaled1 := x1.Mul(vInvSqrt2)
		erfX1 := math.BaseErfVec_avx2(xScaled1)
		onePlusErf1 := vOne.Add(erfX1)
		halfOnePlusErf1 := vHalf.Mul(onePlusErf1)
		result1 := x1.Mul(halfOnePlusErf1)
		result1.Store((*[8]float32)(unsafe.Pointer(&output[ii+8])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		xScaled := x.Mul(vInvSqrt2)
		erfX := math.BaseErfVec_avx2(xScaled)
		onePlusErf := vOne.Add(erfX)
		halfOnePlusErf := vHalf.Mul(onePlusErf)
		result := x.Mul(halfOnePlusErf)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i])
		output[i] = float32(x * 0.5 * (1.0 + stdmath.Erf(x*0.7071067811865476)))
	}
}

func BaseGELU_avx2_Float64(input []float64, output []float64) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vHalf := BaseGELU_AVX2_vHalf_f64
	vOne := BaseGELU_AVX2_vOne_f64
	vInvSqrt2 := BaseGELU_AVX2_vInvSqrt2_f64
	lanes := 4
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		xScaled := x.Mul(vInvSqrt2)
		erfX := math.BaseErfVec_avx2_Float64(xScaled)
		onePlusErf := vOne.Add(erfX)
		halfOnePlusErf := vHalf.Mul(onePlusErf)
		result := x.Mul(halfOnePlusErf)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii+4])))
		xScaled1 := x1.Mul(vInvSqrt2)
		erfX1 := math.BaseErfVec_avx2_Float64(xScaled1)
		onePlusErf1 := vOne.Add(erfX1)
		halfOnePlusErf1 := vHalf.Mul(onePlusErf1)
		result1 := x1.Mul(halfOnePlusErf1)
		result1.Store((*[4]float64)(unsafe.Pointer(&output[ii+4])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		xScaled := x.Mul(vInvSqrt2)
		erfX := math.BaseErfVec_avx2_Float64(xScaled)
		onePlusErf := vOne.Add(erfX)
		halfOnePlusErf := vHalf.Mul(onePlusErf)
		result := x.Mul(halfOnePlusErf)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i])
		output[i] = float64(x * 0.5 * (1.0 + stdmath.Erf(x*0.7071067811865476)))
	}
}

func BaseGELUApprox_avx2_Float16(input []hwy.Float16, output []hwy.Float16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vCoeff := asm.BroadcastFloat16x8AVX2(uint16(hwy.Float32ToFloat16(float32(1.702))))
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		xScaled := x.Mul(vCoeff)
		sigmoidX := math.BaseSigmoidVec_avx2_Float16(xScaled)
		result := x.Mul(sigmoidX)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		xScaled1 := x1.Mul(vCoeff)
		sigmoidX1 := math.BaseSigmoidVec_avx2_Float16(xScaled1)
		result1 := x1.Mul(sigmoidX1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		xScaled := x.Mul(vCoeff)
		sigmoidX := math.BaseSigmoidVec_avx2_Float16(xScaled)
		result := x.Mul(sigmoidX)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i].Float32())
		sigmoid := 1.0 / (1.0 + stdmath.Exp(-1.702*x))
		output[i] = hwy.Float32ToFloat16(float32(x * sigmoid))
	}
}

func BaseGELUApprox_avx2_BFloat16(input []hwy.BFloat16, output []hwy.BFloat16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vCoeff := asm.BroadcastBFloat16x8AVX2(uint16(hwy.Float32ToBFloat16(float32(1.702))))
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		xScaled := x.Mul(vCoeff)
		sigmoidX := math.BaseSigmoidVec_avx2_BFloat16(xScaled)
		result := x.Mul(sigmoidX)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		xScaled1 := x1.Mul(vCoeff)
		sigmoidX1 := math.BaseSigmoidVec_avx2_BFloat16(xScaled1)
		result1 := x1.Mul(sigmoidX1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		xScaled := x.Mul(vCoeff)
		sigmoidX := math.BaseSigmoidVec_avx2_BFloat16(xScaled)
		result := x.Mul(sigmoidX)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i].Float32())
		sigmoid := 1.0 / (1.0 + stdmath.Exp(-1.702*x))
		output[i] = hwy.Float32ToBFloat16(float32(x * sigmoid))
	}
}

func BaseGELUApprox_avx2(input []float32, output []float32) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vCoeff := BaseGELUApprox_AVX2_vCoeff_f32
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		xScaled := x.Mul(vCoeff)
		sigmoidX := math.BaseSigmoidVec_avx2(xScaled)
		result := x.Mul(sigmoidX)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii+8])))
		xScaled1 := x1.Mul(vCoeff)
		sigmoidX1 := math.BaseSigmoidVec_avx2(xScaled1)
		result1 := x1.Mul(sigmoidX1)
		result1.Store((*[8]float32)(unsafe.Pointer(&output[ii+8])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		xScaled := x.Mul(vCoeff)
		sigmoidX := math.BaseSigmoidVec_avx2(xScaled)
		result := x.Mul(sigmoidX)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i])
		sigmoid := 1.0 / (1.0 + stdmath.Exp(-1.702*x))
		output[i] = float32(x * sigmoid)
	}
}

func BaseGELUApprox_avx2_Float64(input []float64, output []float64) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vCoeff := BaseGELUApprox_AVX2_vCoeff_f64
	lanes := 4
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		xScaled := x.Mul(vCoeff)
		sigmoidX := math.BaseSigmoidVec_avx2_Float64(xScaled)
		result := x.Mul(sigmoidX)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii+4])))
		xScaled1 := x1.Mul(vCoeff)
		sigmoidX1 := math.BaseSigmoidVec_avx2_Float64(xScaled1)
		result1 := x1.Mul(sigmoidX1)
		result1.Store((*[4]float64)(unsafe.Pointer(&output[ii+4])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		xScaled := x.Mul(vCoeff)
		sigmoidX := math.BaseSigmoidVec_avx2_Float64(xScaled)
		result := x.Mul(sigmoidX)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i])
		sigmoid := 1.0 / (1.0 + stdmath.Exp(-1.702*x))
		output[i] = float64(x * sigmoid)
	}
}

func BaseReLU_avx2_Float16(input []hwy.Float16, output []hwy.Float16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vZero := asm.BroadcastFloat16x8AVX2(uint16(hwy.Float32ToFloat16(float32(0.0))))
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		result := x.Max(vZero)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		result1 := x1.Max(vZero)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		result := x.Max(vZero)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		if input[i].Float32() > 0 {
			output[i] = hwy.Float32ToFloat16(input[i].Float32())
		} else {
			output[i] = hwy.Float32ToFloat16(0)
		}
	}
}

func BaseReLU_avx2_BFloat16(input []hwy.BFloat16, output []hwy.BFloat16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vZero := asm.BroadcastBFloat16x8AVX2(uint16(hwy.Float32ToBFloat16(float32(0.0))))
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		result := x.Max(vZero)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		result1 := x1.Max(vZero)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		result := x.Max(vZero)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		if input[i].Float32() > 0 {
			output[i] = hwy.Float32ToBFloat16(input[i].Float32())
		} else {
			output[i] = hwy.Float32ToBFloat16(0)
		}
	}
}

func BaseReLU_avx2(input []float32, output []float32) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vZero := BaseReLU_AVX2_vZero_f32
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		result := x.Max(vZero)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii+8])))
		result1 := x1.Max(vZero)
		result1.Store((*[8]float32)(unsafe.Pointer(&output[ii+8])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		result := x.Max(vZero)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		if input[i] > 0 {
			output[i] = input[i]
		} else {
			output[i] = 0
		}
	}
}

func BaseReLU_avx2_Float64(input []float64, output []float64) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vZero := BaseReLU_AVX2_vZero_f64
	lanes := 4
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		result := x.Max(vZero)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii+4])))
		result1 := x1.Max(vZero)
		result1.Store((*[4]float64)(unsafe.Pointer(&output[ii+4])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		result := x.Max(vZero)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		if input[i] > 0 {
			output[i] = input[i]
		} else {
			output[i] = 0
		}
	}
}

func BaseSiLU_avx2_Float16(input []hwy.Float16, output []hwy.Float16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		sigmoidX := math.BaseSigmoidVec_avx2_Float16(x)
		result := x.Mul(sigmoidX)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		sigmoidX1 := math.BaseSigmoidVec_avx2_Float16(x1)
		result1 := x1.Mul(sigmoidX1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		sigmoidX := math.BaseSigmoidVec_avx2_Float16(x)
		result := x.Mul(sigmoidX)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i].Float32())
		sigmoid := 1.0 / (1.0 + stdmath.Exp(-x))
		output[i] = hwy.Float32ToFloat16(float32(x * sigmoid))
	}
}

func BaseSiLU_avx2_BFloat16(input []hwy.BFloat16, output []hwy.BFloat16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		sigmoidX := math.BaseSigmoidVec_avx2_BFloat16(x)
		result := x.Mul(sigmoidX)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		sigmoidX1 := math.BaseSigmoidVec_avx2_BFloat16(x1)
		result1 := x1.Mul(sigmoidX1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		sigmoidX := math.BaseSigmoidVec_avx2_BFloat16(x)
		result := x.Mul(sigmoidX)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i].Float32())
		sigmoid := 1.0 / (1.0 + stdmath.Exp(-x))
		output[i] = hwy.Float32ToBFloat16(float32(x * sigmoid))
	}
}

func BaseSiLU_avx2(input []float32, output []float32) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		sigmoidX := math.BaseSigmoidVec_avx2(x)
		result := x.Mul(sigmoidX)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii+8])))
		sigmoidX1 := math.BaseSigmoidVec_avx2(x1)
		result1 := x1.Mul(sigmoidX1)
		result1.Store((*[8]float32)(unsafe.Pointer(&output[ii+8])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		sigmoidX := math.BaseSigmoidVec_avx2(x)
		result := x.Mul(sigmoidX)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i])
		sigmoid := 1.0 / (1.0 + stdmath.Exp(-x))
		output[i] = float32(x * sigmoid)
	}
}

func BaseSiLU_avx2_Float64(input []float64, output []float64) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	lanes := 4
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		sigmoidX := math.BaseSigmoidVec_avx2_Float64(x)
		result := x.Mul(sigmoidX)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii+4])))
		sigmoidX1 := math.BaseSigmoidVec_avx2_Float64(x1)
		result1 := x1.Mul(sigmoidX1)
		result1.Store((*[4]float64)(unsafe.Pointer(&output[ii+4])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		sigmoidX := math.BaseSigmoidVec_avx2_Float64(x)
		result := x.Mul(sigmoidX)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i])
		sigmoid := 1.0 / (1.0 + stdmath.Exp(-x))
		output[i] = float64(x * sigmoid)
	}
}

func BaseLeakyReLU_avx2_Float16(input []hwy.Float16, output []hwy.Float16, alpha hwy.Float16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vAlpha := asm.BroadcastFloat16x8AVX2(uint16(alpha))
	lanes := 8
	ii := 0
	for ; ii+lanes*4 <= size; ii += lanes * 4 {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		negPart := x.Mul(vAlpha)
		result := x.Max(negPart)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		negPart1 := x1.Mul(vAlpha)
		result1 := x1.Max(negPart1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
		x2 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+16:][0]))
		negPart2 := x2.Mul(vAlpha)
		result2 := x2.Max(negPart2)
		result2.StorePtr(unsafe.Pointer(&output[ii+16:][0]))
		x3 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+24:][0]))
		negPart3 := x3.Mul(vAlpha)
		result3 := x3.Max(negPart3)
		result3.StorePtr(unsafe.Pointer(&output[ii+24:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		negPart := x.Mul(vAlpha)
		result := x.Max(negPart)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		if input[i].Float32() > 0 {
			output[i] = hwy.Float32ToFloat16(input[i].Float32())
		} else {
			output[i] = hwy.Float32ToFloat16(alpha.Float32() * input[i].Float32())
		}
	}
}

func BaseLeakyReLU_avx2_BFloat16(input []hwy.BFloat16, output []hwy.BFloat16, alpha hwy.BFloat16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vAlpha := asm.BroadcastBFloat16x8AVX2(uint16(alpha))
	lanes := 8
	ii := 0
	for ; ii+lanes*4 <= size; ii += lanes * 4 {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		negPart := x.Mul(vAlpha)
		result := x.Max(negPart)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		negPart1 := x1.Mul(vAlpha)
		result1 := x1.Max(negPart1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
		x2 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+16:][0]))
		negPart2 := x2.Mul(vAlpha)
		result2 := x2.Max(negPart2)
		result2.StorePtr(unsafe.Pointer(&output[ii+16:][0]))
		x3 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+24:][0]))
		negPart3 := x3.Mul(vAlpha)
		result3 := x3.Max(negPart3)
		result3.StorePtr(unsafe.Pointer(&output[ii+24:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		negPart := x.Mul(vAlpha)
		result := x.Max(negPart)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		if input[i].Float32() > 0 {
			output[i] = hwy.Float32ToBFloat16(input[i].Float32())
		} else {
			output[i] = hwy.Float32ToBFloat16(alpha.Float32() * input[i].Float32())
		}
	}
}

func BaseLeakyReLU_avx2(input []float32, output []float32, alpha float32) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vAlpha := archsimd.BroadcastFloat32x8(alpha)
	lanes := 8
	ii := 0
	for ; ii+lanes*4 <= size; ii += lanes * 4 {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		negPart := x.Mul(vAlpha)
		result := x.Max(negPart)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii+8])))
		negPart1 := x1.Mul(vAlpha)
		result1 := x1.Max(negPart1)
		result1.Store((*[8]float32)(unsafe.Pointer(&output[ii+8])))
		x2 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii+16])))
		negPart2 := x2.Mul(vAlpha)
		result2 := x2.Max(negPart2)
		result2.Store((*[8]float32)(unsafe.Pointer(&output[ii+16])))
		x3 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii+24])))
		negPart3 := x3.Mul(vAlpha)
		result3 := x3.Max(negPart3)
		result3.Store((*[8]float32)(unsafe.Pointer(&output[ii+24])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		negPart := x.Mul(vAlpha)
		result := x.Max(negPart)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		if input[i] > 0 {
			output[i] = input[i]
		} else {
			output[i] = alpha * input[i]
		}
	}
}

func BaseLeakyReLU_avx2_Float64(input []float64, output []float64, alpha float64) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vAlpha := archsimd.BroadcastFloat64x4(alpha)
	lanes := 4
	ii := 0
	for ; ii+lanes*4 <= size; ii += lanes * 4 {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		negPart := x.Mul(vAlpha)
		result := x.Max(negPart)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii+4])))
		negPart1 := x1.Mul(vAlpha)
		result1 := x1.Max(negPart1)
		result1.Store((*[4]float64)(unsafe.Pointer(&output[ii+4])))
		x2 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii+8])))
		negPart2 := x2.Mul(vAlpha)
		result2 := x2.Max(negPart2)
		result2.Store((*[4]float64)(unsafe.Pointer(&output[ii+8])))
		x3 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii+12])))
		negPart3 := x3.Mul(vAlpha)
		result3 := x3.Max(negPart3)
		result3.Store((*[4]float64)(unsafe.Pointer(&output[ii+12])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		negPart := x.Mul(vAlpha)
		result := x.Max(negPart)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		if input[i] > 0 {
			output[i] = input[i]
		} else {
			output[i] = alpha * input[i]
		}
	}
}

func BaseTanh_avx2_Float16(input []hwy.Float16, output []hwy.Float16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		result := math.BaseTanhVec_avx2_Float16(x)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		result1 := math.BaseTanhVec_avx2_Float16(x1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		result := math.BaseTanhVec_avx2_Float16(x)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i].Float32())
		output[i] = hwy.Float32ToFloat16(float32(stdmath.Tanh(x)))
	}
}

func BaseTanh_avx2_BFloat16(input []hwy.BFloat16, output []hwy.BFloat16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		result := math.BaseTanhVec_avx2_BFloat16(x)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		result1 := math.BaseTanhVec_avx2_BFloat16(x1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		result := math.BaseTanhVec_avx2_BFloat16(x)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i].Float32())
		output[i] = hwy.Float32ToBFloat16(float32(stdmath.Tanh(x)))
	}
}

func BaseTanh_avx2(input []float32, output []float32) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		result := math.BaseTanhVec_avx2(x)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii+8])))
		result1 := math.BaseTanhVec_avx2(x1)
		result1.Store((*[8]float32)(unsafe.Pointer(&output[ii+8])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		result := math.BaseTanhVec_avx2(x)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i])
		output[i] = float32(stdmath.Tanh(x))
	}
}

func BaseTanh_avx2_Float64(input []float64, output []float64) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	lanes := 4
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		result := math.BaseTanhVec_avx2_Float64(x)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii+4])))
		result1 := math.BaseTanhVec_avx2_Float64(x1)
		result1.Store((*[4]float64)(unsafe.Pointer(&output[ii+4])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		result := math.BaseTanhVec_avx2_Float64(x)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		x := float64(input[i])
		output[i] = float64(stdmath.Tanh(x))
	}
}

func BaseELU_avx2_Float16(input []hwy.Float16, output []hwy.Float16, alpha hwy.Float16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vZero := asm.BroadcastFloat16x8AVX2(uint16(hwy.Float32ToFloat16(float32(0.0))))
	vOne := asm.BroadcastFloat16x8AVX2(uint16(hwy.Float32ToFloat16(float32(1.0))))
	vAlpha := asm.BroadcastFloat16x8AVX2(uint16(alpha))
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		expX := math.BaseExpVec_avx2_Float16(x)
		expM1 := expX.Sub(vOne)
		negPart := vAlpha.Mul(expM1)
		isPositive := x.Greater(vZero)
		result := x.Merge(negPart, isPositive)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		expX1 := math.BaseExpVec_avx2_Float16(x1)
		expM11 := expX1.Sub(vOne)
		negPart1 := vAlpha.Mul(expM11)
		isPositive1 := x1.Greater(vZero)
		result1 := x1.Merge(negPart1, isPositive1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		expX := math.BaseExpVec_avx2_Float16(x)
		expM1 := expX.Sub(vOne)
		negPart := vAlpha.Mul(expM1)
		isPositive := x.Greater(vZero)
		result := x.Merge(negPart, isPositive)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		if input[i].Float32() > 0 {
			output[i] = hwy.Float32ToFloat16(input[i].Float32())
		} else {
			x := float64(input[i].Float32())
			output[i] = hwy.Float32ToFloat16(float32(float64(alpha.Float32()) * (stdmath.Exp(x) - 1.0)))
		}
	}
}

func BaseELU_avx2_BFloat16(input []hwy.BFloat16, output []hwy.BFloat16, alpha hwy.BFloat16) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vZero := asm.BroadcastBFloat16x8AVX2(uint16(hwy.Float32ToBFloat16(float32(0.0))))
	vOne := asm.BroadcastBFloat16x8AVX2(uint16(hwy.Float32ToBFloat16(float32(1.0))))
	vAlpha := asm.BroadcastBFloat16x8AVX2(uint16(alpha))
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		expX := math.BaseExpVec_avx2_BFloat16(x)
		expM1 := expX.Sub(vOne)
		negPart := vAlpha.Mul(expM1)
		isPositive := x.Greater(vZero)
		result := x.Merge(negPart, isPositive)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
		x1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii+8:][0]))
		expX1 := math.BaseExpVec_avx2_BFloat16(x1)
		expM11 := expX1.Sub(vOne)
		negPart1 := vAlpha.Mul(expM11)
		isPositive1 := x1.Greater(vZero)
		result1 := x1.Merge(negPart1, isPositive1)
		result1.StorePtr(unsafe.Pointer(&output[ii+8:][0]))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&input[ii:][0]))
		expX := math.BaseExpVec_avx2_BFloat16(x)
		expM1 := expX.Sub(vOne)
		negPart := vAlpha.Mul(expM1)
		isPositive := x.Greater(vZero)
		result := x.Merge(negPart, isPositive)
		result.StorePtr(unsafe.Pointer(&output[ii:][0]))
	}
	for i := ii; i < size; i++ {
		if input[i].Float32() > 0 {
			output[i] = hwy.Float32ToBFloat16(input[i].Float32())
		} else {
			x := float64(input[i].Float32())
			output[i] = hwy.Float32ToBFloat16(float32(float64(alpha.Float32()) * (stdmath.Exp(x) - 1.0)))
		}
	}
}

func BaseELU_avx2(input []float32, output []float32, alpha float32) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vZero := BaseELU_AVX2_vZero_f32
	vOne := BaseELU_AVX2_vOne_f32
	vAlpha := archsimd.BroadcastFloat32x8(alpha)
	lanes := 8
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		expX := math.BaseExpVec_avx2(x)
		expM1 := expX.Sub(vOne)
		negPart := vAlpha.Mul(expM1)
		isPositive := x.Greater(vZero)
		result := x.Merge(negPart, isPositive)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii+8])))
		expX1 := math.BaseExpVec_avx2(x1)
		expM11 := expX1.Sub(vOne)
		negPart1 := vAlpha.Mul(expM11)
		isPositive1 := x1.Greater(vZero)
		result1 := x1.Merge(negPart1, isPositive1)
		result1.Store((*[8]float32)(unsafe.Pointer(&output[ii+8])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&input[ii])))
		expX := math.BaseExpVec_avx2(x)
		expM1 := expX.Sub(vOne)
		negPart := vAlpha.Mul(expM1)
		isPositive := x.Greater(vZero)
		result := x.Merge(negPart, isPositive)
		result.Store((*[8]float32)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		if input[i] > 0 {
			output[i] = input[i]
		} else {
			x := float64(input[i])
			output[i] = float32(float64(alpha) * (stdmath.Exp(x) - 1.0))
		}
	}
}

func BaseELU_avx2_Float64(input []float64, output []float64, alpha float64) {
	size := min(len(input), len(output))
	if size == 0 {
		return
	}
	vZero := BaseELU_AVX2_vZero_f64
	vOne := BaseELU_AVX2_vOne_f64
	vAlpha := archsimd.BroadcastFloat64x4(alpha)
	lanes := 4
	ii := 0
	for ; ii+lanes*2 <= size; ii += lanes * 2 {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		expX := math.BaseExpVec_avx2_Float64(x)
		expM1 := expX.Sub(vOne)
		negPart := vAlpha.Mul(expM1)
		isPositive := x.Greater(vZero)
		result := x.Merge(negPart, isPositive)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
		x1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii+4])))
		expX1 := math.BaseExpVec_avx2_Float64(x1)
		expM11 := expX1.Sub(vOne)
		negPart1 := vAlpha.Mul(expM11)
		isPositive1 := x1.Greater(vZero)
		result1 := x1.Merge(negPart1, isPositive1)
		result1.Store((*[4]float64)(unsafe.Pointer(&output[ii+4])))
	}
	for ; ii+lanes <= size; ii += lanes {
		x := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&input[ii])))
		expX := math.BaseExpVec_avx2_Float64(x)
		expM1 := expX.Sub(vOne)
		negPart := vAlpha.Mul(expM1)
		isPositive := x.Greater(vZero)
		result := x.Merge(negPart, isPositive)
		result.Store((*[4]float64)(unsafe.Pointer(&output[ii])))
	}
	for i := ii; i < size; i++ {
		if input[i] > 0 {
			output[i] = input[i]
		} else {
			x := float64(input[i])
			output[i] = float64(float64(alpha) * (stdmath.Exp(x) - 1.0))
		}
	}
}
