// Copyright 2025 go-highway Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package main

import (
	"fmt"
	"go/ast"
	"go/token"
	"maps"
	"sort"
	"strconv"
	"strings"
)

// isUnsignedIntType returns true if the element type is an unsigned integer type.
func isUnsignedIntType(elemType string) bool {
	return elemType == "uint8" || elemType == "uint16" || elemType == "uint32" || elemType == "uint64"
}

// is64BitIntType returns true if the element type is a 64-bit integer (signed or unsigned).
func is64BitIntType(elemType string) bool {
	return elemType == "int64" || elemType == "uint64"
}

// returnsVecType checks if any return type contains "Vec" or "Mask".
// Vec-returning functions use hwy.Vec operations which already work for Float16/BFloat16.
func returnsVecType(returns []Param) bool {
	for _, ret := range returns {
		if strings.Contains(ret.Type, "Vec") || strings.Contains(ret.Type, "Mask") {
			return true
		}
	}
	return false
}

// optimizeSliceToPointer converts a slice expression to an optimized address expression.
// For slice expressions like src[i:], it generates &src[i] instead of &src[i:][0].
// This avoids the performance overhead where Go doesn't optimize &slice[i:][0] to &slice[i].
//
// Examples:
//   - src[i:]    -> &src[i]      (optimized)
//   - src[i+n:]  -> &src[i+n]    (optimized)
//   - src        -> &src[0]      (no slice, use index 0)
//   - src[i:j]   -> &src[i:j][0] (has high bound, can't optimize)
func optimizeSliceToPointer(expr ast.Expr) *ast.UnaryExpr {
	// Check if expr is a slice expression like src[i:]
	if sliceExpr, ok := expr.(*ast.SliceExpr); ok {
		// Only optimize src[low:] patterns (no high bound, no max)
		// src[low:high] needs to keep bounds for safety
		if sliceExpr.Low != nil && sliceExpr.High == nil && !sliceExpr.Slice3 {
			// Transform src[low:] to &src[low]
			return &ast.UnaryExpr{
				Op: token.AND,
				X: &ast.IndexExpr{
					X:     sliceExpr.X,
					Index: sliceExpr.Low,
				},
			}
		}
	}

	// Default: use &expr[0]
	return &ast.UnaryExpr{
		Op: token.AND,
		X: &ast.IndexExpr{
			X:     expr,
			Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
		},
	}
}

// TransformResult contains the transformed function and any hoisted constants.
type TransformResult struct {
	FuncDecl      *ast.FuncDecl
	HoistedConsts []HoistedConst
}

// HoistedConst represents a constant that was hoisted from a function to package level.
type HoistedConst struct {
	VarName   string // Package-level var name (e.g., "BaseSigmoid_one_f32")
	Value     string // The constant value (e.g., "1.0")
	VecType   string // Vector type (e.g., "Float32x8")
	Broadcast string // Broadcast function (e.g., "archsimd.BroadcastFloat32x8")
}

// TransformOptions contains additional context for transformation.
type TransformOptions struct {
	TypeSpecificConsts map[string]*TypeSpecificConst
	ConditionalBlocks  []ConditionalBlock
	FileSet            *token.FileSet         // For resolving line numbers in conditional blocks
	Imports            map[string]string      // map[local_name]import_path for resolving package references
	AllFuncs           map[string]*ParsedFunc // All functions in file for inlining helpers
	SkipHalfPrecNEON   bool                   // Skip NEON asm specialization for this half-precision function
}

// Transform transforms a parsed function for a specific target and element type.
// It clones the AST, specializes generics, and transforms hwy operations.
func Transform(pf *ParsedFunc, target Target, elemType string) *TransformResult {
	return TransformWithOptions(pf, target, elemType, nil)
}

// TransformWithOptions transforms a parsed function with additional options.
func TransformWithOptions(pf *ParsedFunc, target Target, elemType string, opts *TransformOptions) *TransformResult {
	if opts == nil {
		opts = &TransformOptions{}
	}

	// First, filter the original body based on conditional blocks.
	// We need to do this BEFORE cloning because the original AST has valid positions.
	var filteredBody *ast.BlockStmt
	if len(opts.ConditionalBlocks) > 0 && opts.FileSet != nil {
		filteredBody = filterConditionalBlocks(pf.Body, opts.ConditionalBlocks, opts.FileSet, target.Name, elemType)
	} else {
		filteredBody = pf.Body
	}

	// Create new function declaration (don't copy Doc - emitter handles comments)
	funcDecl := &ast.FuncDecl{
		Name: ast.NewIdent(pf.Name + target.Suffix()),
		Type: &ast.FuncType{
			Params:  &ast.FieldList{},
			Results: pf.buildResultsWithTarget(elemType, target, opts.SkipHalfPrecNEON),
		},
		Body: cloneBlockStmt(filteredBody),
	}

	// Build parameter list with specialized types
	for _, param := range pf.Params {
		paramType := specializeType(param.Type, pf.TypeParams, elemType)
		// Also transform hwy.Vec[T] to concrete vector types for SIMD targets
		paramType = specializeVecType(paramType, elemType, target, opts.SkipHalfPrecNEON)
		field := &ast.Field{
			Names: []*ast.Ident{ast.NewIdent(param.Name)},
			Type:  parseTypeExpr(paramType),
		}
		funcDecl.Type.Params.List = append(funcDecl.Type.Params.List, field)
	}

	// For fallback target with predicate functions, generate scalar loop body
	// to avoid allocations from hwy.Load/pred.Apply
	if target.Name == "Fallback" && hasPredicateParam(pf) {
		if scalarBody := generateScalarPredicateBody(pf, elemType); scalarBody != nil {
			funcDecl.Body = scalarBody
			return &TransformResult{
				FuncDecl:      funcDecl,
				HoistedConsts: nil,
			}
		}
	}

	// Transform the function body
	ctx := &transformContext{
		target:                  target,
		elemType:                elemType,
		typeParams:              pf.TypeParams,
		loopInfo:                pf.LoopInfo,
		lanesVars:               make(map[string]bool),
		localVars:               make(map[string]bool),
		stackArrayVars:          make(map[string]bool),
		hoistedConsts:           make(map[string]HoistedConst),
		funcName:                pf.Name,
		typeSpecificConsts:      opts.TypeSpecificConsts,
		conditionalBlocks:       opts.ConditionalBlocks,
		fset:                    opts.FileSet,
		imports:                 opts.Imports,
		varTypes:                make(map[string]string),
		halfPrecisionSlices:     make(map[string]bool),
		halfPrecisionScalarVars: make(map[string]bool),
		varVecLanes:             make(map[string]int),
		varVecElemType:          make(map[string]string),
		allFuncs:                opts.AllFuncs,
		skipHalfPrecNEON:        opts.SkipHalfPrecNEON,
	}

	// Add function parameters to localVars to prevent them from being hoisted
	// Also track half-precision slice and scalar parameters, and parameter types
	for _, param := range pf.Params {
		ctx.localVars[param.Name] = true
		// Track parameter types for type inference (needed for inferring slice element types)
		ctx.varTypes[param.Name] = param.Type
		// Check if parameter is a slice of half-precision type
		if isHalfPrecisionSliceType(param.Type, elemType) {
			ctx.halfPrecisionSlices[param.Name] = true
		}
		// Check if parameter is a scalar half-precision type
		if isHalfPrecisionScalarType(param.Type, elemType) {
			ctx.halfPrecisionScalarVars[param.Name] = true
		}
	}

	// Also track named return values as half-precision scalars
	// For functions like BaseMinMax[T hwy.Floats](v []T) (min, max T),
	// the named return values min and max should be tracked as half-precision scalars
	for _, ret := range pf.Returns {
		if ret.Name != "" && isHalfPrecisionScalarType(ret.Type, elemType) {
			ctx.halfPrecisionScalarVars[ret.Name] = true
		}
	}

	// Collect all locally-defined variable names to avoid hoisting them as constants
	collectLocalVariables(funcDecl.Body, ctx)

	// Pre-scan for Load sizes to determine inferredFuncLanes before processing Set calls.
	// This ensures hoisted constants match the actual vector width used by Load operations.
	if loadSize := findMaxLoadSizeForElemType(funcDecl.Body, elemType); loadSize > 0 {
		ctx.inferredFuncLanes = loadSize
	}

	// Inline local helper function calls before main transformation.
	// This ensures helper bodies get specialized for the target/elemType.
	inlineHelperCalls(funcDecl.Body, ctx)

	// Resolve type-specific constant references
	// Pattern 1: expC0 -> expC0_f32 (base name lookup)
	// Pattern 2: expC0_f32 -> expC0_f64 (suffix swapping for compilable base files)
	transformIdentifiers(funcDecl.Body, ctx)

	transformNode(funcDecl.Body, ctx)

	// Post-process: scalarize fallback functions that only use simple ops.
	// This converts hwy.Vec operations to pure scalar Go code for better performance
	// by eliminating the allocation overhead of 1-element Vec wrappers.
	// NOTE: Don't scalarize Float16/BFloat16 functions - their arithmetic operators
	// do integer math (since they're uint16 under the hood), which produces wrong results.
	// The non-scalarized path uses transformHalfPrecisionFallback to fix this.
	wasScalarized := false
	if target.Name == "Fallback" && !isHalfPrecisionType(elemType) {
		if canScalarizeFallback(funcDecl) {
			scalarizeFallback(funcDecl, elemType)
			wasScalarized = true
		}
	}

	// Post-process: convert "_ = expr" assignments to expression statements.
	// This is needed because tryTransformToInPlace marks in-place ops with _ = voidFunc()
	// which is invalid Go when the function returns nothing (e.g., MulAddAcc).
	if target.Name == "NEON" {
		convertBlankAssignToExprStmt(funcDecl.Body)
	}

	// Post-process to replace NumLanes() calls and ReduceSum() calls
	if target.Name != "Fallback" {
		postProcessSIMD(funcDecl.Body, ctx)
	}

	// Post-process to convert stack array usages to slice expressions
	if target.Name != "Fallback" && len(ctx.stackArrayVars) > 0 {
		convertStackArrayUsages(funcDecl.Body, ctx)
	}

	// Post-process to transform scalar operations for Float16/BFloat16.
	// Scalar Go operations (+, -, *, /, >, <, etc.) don't work on Float16/BFloat16
	// (they're uint16 under the hood), so we convert to float32 for computation.
	// This applies to all targets (Fallback, NEON, AVX2, AVX512) since scalar tail
	// loops exist in all targets.
	// Skip Vec-returning functions - they use hwy.Vec operations which already work.
	// Skip scalarized fallback functions - they just copy values, no arithmetic needed.
	if isHalfPrecisionType(elemType) && !returnsVecType(pf.Returns) && !wasScalarized {
		transformHalfPrecisionFallback(funcDecl.Body, ctx)
	}

	// Apply loop unrolling if there's a SIMD loop (not for fallback)
	if pf.LoopInfo != nil && target.Name != "Fallback" {
		lanes := target.LanesFor(elemType)
		unrollFactor := computeUnrollFactor(pf.LoopInfo, pf.HwyCalls, target)
		if unrollFactor > 1 {
			// Find the main SIMD loop and unroll it
			if mainLoop := findMainSimdLoop(funcDecl.Body, pf.LoopInfo); mainLoop != nil {
				unrollLoopWithCleanup(funcDecl.Body, mainLoop, pf.LoopInfo, unrollFactor, lanes)
			}
		}
	}

	// Insert tail handling if there's a loop and function doesn't return a value
	// (functions that return values have their own tail handling in the template)
	if pf.LoopInfo != nil && len(pf.Returns) == 0 {
		insertTailHandling(funcDecl.Body, pf.LoopInfo, elemType, target, pf.Name, pf.Params, pf.TypeParams)
	}

	// Collect hoisted constants in deterministic order
	var hoisted []HoistedConst
	keys := make([]string, 0, len(ctx.hoistedConsts))
	for k := range ctx.hoistedConsts {
		keys = append(keys, k)
	}
	sort.Strings(keys)
	for _, k := range keys {
		hoisted = append(hoisted, ctx.hoistedConsts[k])
	}

	return &TransformResult{
		FuncDecl:      funcDecl,
		HoistedConsts: hoisted,
	}
}

// OperationComplexity categorizes operations by their register pressure and latency.
type OperationComplexity int

const (
	// ComplexitySimple: basic arithmetic (Add, Sub, Mul, FMA) - low register pressure
	ComplexitySimple OperationComplexity = iota
	// ComplexityMedium: comparisons, blends, shuffles - moderate register pressure
	ComplexityMedium
	// ComplexityComplex: transcendentals (Exp, Log, Sin, etc.) - high register pressure
	ComplexityComplex
	// ComplexityReduction: reductions (Sum, Min, Max) - data dependencies limit ILP
	ComplexityReduction
)

// simpleOps are operations with low register pressure that can be heavily unrolled.
var simpleOps = map[string]bool{
	"Add": true, "Sub": true, "Mul": true, "Div": true,
	"FMA": true, "MulAdd": true, "MulSub": true,
	"Neg": true, "Abs": true, "Min": true, "Max": true,
	"And": true, "Or": true, "Xor": true, "AndNot": true, "Not": true,
	"Load": true, "LoadSlice": true, "Store": true, "StoreSlice": true, "Set": true, "Zero": true,
	// Meta-operations that don't affect complexity
	"MaxLanes": true, "NumLanes": true, "Lanes": true,
	"Vec": true, "Mask": true, // Type references
}

// complexOps are operations that use many registers (polynomial coefficients, etc.).
var complexOps = map[string]bool{
	"Exp": true, "Exp2": true, "Exp10": true,
	"Log": true, "Log2": true, "Log10": true,
	"Sin": true, "Cos": true, "SinCos": true,
	"Tanh": true, "Sinh": true, "Cosh": true,
	"Asinh": true, "Acosh": true, "Atanh": true,
	"Sigmoid": true, "Erf": true, "Pow": true,
	"Sqrt": true, "RSqrt": true,
}

// reductionOps have data dependencies that limit instruction-level parallelism.
var reductionOps = map[string]bool{
	"ReduceSum": true, "ReduceMin": true, "ReduceMax": true,
}

// analyzeLoopComplexity determines the complexity of operations in a loop body.
func analyzeLoopComplexity(hwyCalls []HwyCall) OperationComplexity {
	hasComplex := false
	hasReduction := false
	hasMedium := false

	for _, call := range hwyCalls {
		if complexOps[call.FuncName] {
			hasComplex = true
		}
		if reductionOps[call.FuncName] {
			hasReduction = true
		}
		if !simpleOps[call.FuncName] && !complexOps[call.FuncName] && !reductionOps[call.FuncName] {
			hasMedium = true
		}
	}

	// Return the highest complexity found
	if hasComplex {
		return ComplexityComplex
	}
	if hasReduction {
		return ComplexityReduction
	}
	if hasMedium {
		return ComplexityMedium
	}
	return ComplexitySimple
}

// computeUnrollFactor determines the automatic unroll factor based on operation complexity
// and target architecture. Returns 1 if unrolling should be disabled.
func computeUnrollFactor(loopInfo *LoopInfo, hwyCalls []HwyCall, target Target) int {
	if loopInfo == nil {
		return 1
	}

	// Honor explicit //hwy:unroll directive
	if loopInfo.UnrollHint > 0 {
		return loopInfo.UnrollHint
	}
	// //hwy:unroll 0 or //hwy:unroll 1 disables unrolling
	if loopInfo.UnrollHint == 0 {
		// No directive - use automatic heuristics
	} else {
		return 1 // Explicit disable
	}

	// Analyze operation complexity
	complexity := analyzeLoopComplexity(hwyCalls)

	// Base unroll factors by complexity
	var baseFactor int
	switch complexity {
	case ComplexitySimple:
		baseFactor = 4 // Simple ops can be heavily unrolled
	case ComplexityMedium:
		baseFactor = 2 // Moderate unrolling
	case ComplexityComplex:
		baseFactor = 2 // Limited by register pressure from polynomial coefficients
	case ComplexityReduction:
		baseFactor = 2 // Data dependencies limit ILP anyway
	default:
		baseFactor = 2
	}

	// Adjust for target architecture
	// AVX-512 has 32 registers vs AVX2's 16, so can be more aggressive
	switch target.Name {
	case "AVX512":
		if baseFactor < 4 && complexity != ComplexityComplex {
			baseFactor = min(baseFactor+1, 4)
		}
	case "NEON":
		// NEON has 32 V registers but narrower, keep moderate
		baseFactor = min(baseFactor, 4)
	case "Fallback":
		// No unrolling for fallback - it's scalar anyway
		return 1
	}

	return baseFactor
}

// unrollLoopWithCleanup applies loop unrolling and inserts a cleanup loop for remaining elements.
// After unrolling with factor N, the main loop processes N*lanes elements per iteration.
// A cleanup loop is inserted to process any remaining full vector chunks (< N*lanes but >= lanes),
// UNLESS the function already has an explicit tail loop after the main loop.
func unrollLoopWithCleanup(body *ast.BlockStmt, forStmt *ast.ForStmt, loopInfo *LoopInfo, unrollFactor int, lanes int) {
	if body == nil || forStmt == nil || loopInfo == nil || unrollFactor <= 1 {
		return
	}

	// Check if there's already a tail loop after the main loop (explicit tail handling).
	// If so, the cleanup loop is unnecessary since the existing tail loop handles all remaining elements.
	needsCleanupLoop := !hasExplicitTailLoop(body, forStmt, loopInfo.Iterator)

	// Clone the original loop body before unrolling (for the cleanup loop)
	var origBodyClone []ast.Stmt
	var origCond ast.Expr
	var origPost ast.Stmt

	if needsCleanupLoop {
		origBodyClone = make([]ast.Stmt, len(forStmt.Body.List))
		for i, stmt := range forStmt.Body.List {
			origBodyClone[i] = cloneStmt(stmt)
		}
		if forStmt.Cond != nil {
			origCond = cloneExpr(forStmt.Cond)
		}
		if forStmt.Post != nil {
			origPost = cloneStmt(forStmt.Post)
		}
	}

	// Check if iterator is declared in the loop's Init (e.g., "for ii := 0; ...")
	// If so, we need to hoist it to allow cleanup loop (or existing tail loop) access
	var hoistedDecl ast.Stmt
	if forStmt.Init != nil {
		if assign, ok := forStmt.Init.(*ast.AssignStmt); ok && assign.Tok == token.DEFINE {
			// Check if this declares the iterator we're tracking
			for _, lhs := range assign.Lhs {
				if ident, ok := lhs.(*ast.Ident); ok && ident.Name == loopInfo.Iterator {
					// Hoist the declaration: create "ii := 0" before the loop
					hoistedDecl = cloneStmt(forStmt.Init)
					// Remove Init from the main loop (it becomes "for ; cond; post")
					forStmt.Init = nil
					break
				}
			}
		}
	}

	// Apply unrolling to the main loop (this modifies forStmt in place)
	unrollLoop(forStmt, loopInfo, unrollFactor, lanes)

	// Find the position of the unrolled loop and insert cleanup loop (if needed) after it
	for i, stmt := range body.List {
		if stmt == forStmt {
			// Build new statement list
			newList := make([]ast.Stmt, 0, len(body.List)+2)
			newList = append(newList, body.List[:i]...)

			// Insert hoisted declaration if needed
			if hoistedDecl != nil {
				newList = append(newList, hoistedDecl)
			}

			// Insert main (unrolled) loop
			newList = append(newList, forStmt)

			// Insert cleanup loop only if function doesn't have its own tail handling
			if needsCleanupLoop {
				cleanupLoop := &ast.ForStmt{
					Cond: origCond,
					Post: origPost,
					Body: &ast.BlockStmt{
						List: origBodyClone,
					},
				}
				newList = append(newList, cleanupLoop)
			}

			// Insert remaining statements
			newList = append(newList, body.List[i+1:]...)
			body.List = newList
			return
		}
	}
}

// hasExplicitTailLoop checks if there's another for loop after the given loop
// that uses the same iterator, indicating explicit tail handling.
func hasExplicitTailLoop(body *ast.BlockStmt, mainLoop *ast.ForStmt, iterator string) bool {
	foundMain := false
	for _, stmt := range body.List {
		if stmt == mainLoop {
			foundMain = true
			continue
		}
		if foundMain {
			if fl, ok := stmt.(*ast.ForStmt); ok {
				if matchesLoopIterator(fl, iterator) {
					return true
				}
			}
		}
	}
	return false
}

// unrollLoop applies loop unrolling to a for loop, creating N copies of the body.
// It modifies the loop in place:
// - Multiplies the stride by unrollFactor
// - Replicates the body with adjusted indices (i, i+lanes, i+2*lanes, ...)
// - Renames variables to avoid redeclaration (x -> x0, x1, x2, ...)
func unrollLoop(forStmt *ast.ForStmt, loopInfo *LoopInfo, unrollFactor int, lanes int) {
	if forStmt == nil || loopInfo == nil || unrollFactor <= 1 {
		return
	}

	// Clone the original body statements
	origBody := forStmt.Body.List

	// Collect variable names declared in the loop body (need renaming for unrolled copies)
	declaredVars := collectDeclaredVars(origBody)

	// Build the unrolled body
	var unrolledBody []ast.Stmt

	for u := range unrollFactor {
		for _, stmt := range origBody {
			// Clone the statement
			cloned := cloneStmt(stmt)

			// Rename variables for unrolled iterations (x -> x0, x1, x2, ...)
			if u > 0 {
				renameVarsInStmt(cloned, declaredVars, u)
			}

			// Adjust indices for all iterations except the first
			if u > 0 {
				adjustLoopIndices(cloned, loopInfo.Iterator, u, lanes)
			}

			unrolledBody = append(unrolledBody, cloned)
		}
	}

	// Update the loop body
	forStmt.Body.List = unrolledBody

	// Update the stride: i += lanes -> i += lanes * unrollFactor
	if assignStmt, ok := forStmt.Post.(*ast.AssignStmt); ok {
		if len(assignStmt.Rhs) == 1 {
			// Check if it's already a constant (transformed by transformForStmt)
			if lit, ok := assignStmt.Rhs[0].(*ast.BasicLit); ok && lit.Kind == token.INT {
				// Multiply the stride
				oldStride, _ := strconv.Atoi(lit.Value)
				lit.Value = strconv.Itoa(oldStride * unrollFactor)
			} else {
				// Wrap in multiplication: stride * unrollFactor
				assignStmt.Rhs[0] = &ast.BinaryExpr{
					X:  assignStmt.Rhs[0],
					Op: token.MUL,
					Y:  &ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(unrollFactor)},
				}
			}
		}
	}

	// Update the condition to account for unrolled stride
	// Change: i+lanes <= n -> i+lanes*unrollFactor <= n
	if binExpr, ok := forStmt.Cond.(*ast.BinaryExpr); ok {
		if innerBin, ok := binExpr.X.(*ast.BinaryExpr); ok {
			if innerBin.Op == token.ADD {
				// Handle both literal and variable lanes
				switch y := innerBin.Y.(type) {
				case *ast.BasicLit:
					if y.Kind == token.INT {
						oldLanes, _ := strconv.Atoi(y.Value)
						y.Value = strconv.Itoa(oldLanes * unrollFactor)
					}
				case *ast.Ident:
					// lanes variable - wrap in multiplication: lanes * unrollFactor
					innerBin.Y = &ast.BinaryExpr{
						X:  y,
						Op: token.MUL,
						Y:  &ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(unrollFactor)},
					}
				}
			}
		}
	}
}

// collectDeclaredVars finds all variable names declared with := or var in the statements.
// It excludes the blank identifier "_" which should never be renamed.
func collectDeclaredVars(stmts []ast.Stmt) map[string]bool {
	vars := make(map[string]bool)
	for _, stmt := range stmts {
		ast.Inspect(stmt, func(n ast.Node) bool {
			if assign, ok := n.(*ast.AssignStmt); ok && assign.Tok == token.DEFINE {
				for _, lhs := range assign.Lhs {
					if ident, ok := lhs.(*ast.Ident); ok && ident.Name != "_" {
						vars[ident.Name] = true
					}
				}
			}
			// Also collect names from "var x, y Type" declarations
			if decl, ok := n.(*ast.GenDecl); ok && decl.Tok == token.VAR {
				for _, spec := range decl.Specs {
					if vs, ok := spec.(*ast.ValueSpec); ok {
						for _, name := range vs.Names {
							if name.Name != "_" {
								vars[name.Name] = true
							}
						}
					}
				}
			}
			return true
		})
	}
	return vars
}

// renameVarsInStmt renames declared variables and their uses by appending the iteration number.
// E.g., for iteration 2: x -> x2, result -> result2
func renameVarsInStmt(stmt ast.Stmt, declaredVars map[string]bool, iteration int) {
	suffix := strconv.Itoa(iteration)

	ast.Inspect(stmt, func(n ast.Node) bool {
		if ident, ok := n.(*ast.Ident); ok {
			if declaredVars[ident.Name] {
				ident.Name = ident.Name + suffix
			}
		}
		return true
	})
}

// adjustLoopIndices adjusts array/slice indices in a statement by adding offset*lanes.
// For iteration u (0-indexed), transforms:
//   - data[i:] -> data[i+u*lanes:]
//   - Load(data[i:]) -> Load(data[i+u*lanes:])
func adjustLoopIndices(stmt ast.Stmt, iterator string, iteration int, lanes int) {
	offset := iteration * lanes

	ast.Inspect(stmt, func(n ast.Node) bool {
		switch node := n.(type) {
		case *ast.SliceExpr:
			// Transform src[i:] or src[i:n] where low uses the iterator
			if node.Low != nil {
				node.Low = addOffsetToExpr(node.Low, iterator, offset)
			}
		case *ast.IndexExpr:
			// Transform src[i] where index uses the iterator
			if node.Index != nil {
				node.Index = addOffsetToExpr(node.Index, iterator, offset)
			}
		}
		return true
	})
}

// addOffsetToExpr adds an offset to an expression if it references the iterator.
// E.g., if iterator="i" and offset=8: i -> i+8, i+lanes -> i+lanes+8, i-1 -> i-1+8
func addOffsetToExpr(expr ast.Expr, iterator string, offset int) ast.Expr {
	// Check if expr directly references the iterator
	if ident, ok := expr.(*ast.Ident); ok && ident.Name == iterator {
		return &ast.BinaryExpr{
			X:  expr,
			Op: token.ADD,
			Y:  &ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(offset)},
		}
	}

	// Check if expr is i+something or i-something
	if binExpr, ok := expr.(*ast.BinaryExpr); ok && (binExpr.Op == token.ADD || binExpr.Op == token.SUB) {
		if ident, ok := binExpr.X.(*ast.Ident); ok && ident.Name == iterator {
			// Transform i+N to i+N+offset, i-N to i-N+offset
			return &ast.BinaryExpr{
				X:  binExpr,
				Op: token.ADD,
				Y:  &ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(offset)},
			}
		}
	}

	return expr
}

// findMainSimdLoop finds the main SIMD loop in a function body that matches the given LoopInfo.
func findMainSimdLoop(body *ast.BlockStmt, loopInfo *LoopInfo) *ast.ForStmt {
	if body == nil || loopInfo == nil {
		return nil
	}

	for _, stmt := range body.List {
		forStmt, ok := stmt.(*ast.ForStmt)
		if !ok {
			continue
		}

		// Check if this loop's iterator matches loopInfo.Iterator
		if matchesLoopIterator(forStmt, loopInfo.Iterator) {
			return forStmt
		}
	}

	return nil
}

// transformContext stores context information for a function transformation process.
type transformContext struct {
	target                  Target
	elemType                string
	typeParams              []TypeParam
	lanesVars               map[string]bool // Variables assigned from NumLanes()
	localVars               map[string]bool // Variables defined locally in the function
	stackArrayVars          map[string]bool // Variables that are stack arrays (need [:] when used as slice)
	loopInfo                *LoopInfo
	hoistedConsts           map[string]HoistedConst       // Hoisted constants (key is local var name)
	funcName                string                        // Current function name for generating unique hoisted names
	typeSpecificConsts      map[string]*TypeSpecificConst // Type-specific constant registry
	conditionalBlocks       []ConditionalBlock            // Conditional blocks to process
	fset                    *token.FileSet                // For resolving line numbers
	imports                 map[string]string             // map[local_name]import_path for resolving package references
	varTypes                map[string]string             // map[var_name]type for type inference (e.g., "int32", "hwy.Float16")
	halfPrecisionScalarVars map[string]bool               // Variables assigned from half-precision slice reads
	halfPrecisionSlices     map[string]bool               // Slice variables that hold half-precision elements
	varVecLanes             map[string]int                // map[var_name]lanes for detected vector sizes from Load
	varVecElemType          map[string]string             // map[var_name]elemType for detected element types from Load
	inferredFuncLanes       int                           // Inferred lane count for function (from first detected Load size)
	allFuncs                map[string]*ParsedFunc        // All functions in file for inlining helpers
	inlineCounter           int                           // Counter for unique variable naming during inlining
	skipHalfPrecNEON        bool                          // Skip NEON asm specialization for half-precision (use generic hwy.Vec[T] path)
}

// vecLoadInfo contains inferred information from an hwy.Load call.
type vecLoadInfo struct {
	lanes    int    // Number of vector lanes (0 if not detected)
	elemType string // Element type (empty if not detected or same as function's elemType)
}

// inferVecLanesFromLoad checks if an expression is an hwy.Load call with a detectable slice size.
// Returns the number of lanes and element type if detected.
func inferVecLanesFromLoad(expr ast.Expr, ctx *transformContext) vecLoadInfo {
	call, ok := expr.(*ast.CallExpr)
	if !ok {
		return vecLoadInfo{}
	}

	// Check for hwy.LoadSlice(...) or hwy.Load[T](...) call
	var funcName string
	var explicitElemType string // Explicit type param from hwy.Load[uint8] style calls
	switch fun := call.Fun.(type) {
	case *ast.SelectorExpr:
		// hwy.LoadSlice(...)
		pkgIdent, ok := fun.X.(*ast.Ident)
		if !ok || pkgIdent.Name != "hwy" {
			return vecLoadInfo{}
		}
		funcName = fun.Sel.Name
	case *ast.IndexExpr:
		// hwy.Load[T](...) - generic call with explicit type param
		sel, ok := fun.X.(*ast.SelectorExpr)
		if !ok {
			return vecLoadInfo{}
		}
		pkgIdent, ok := sel.X.(*ast.Ident)
		if !ok || pkgIdent.Name != "hwy" {
			return vecLoadInfo{}
		}
		funcName = sel.Sel.Name
		// Extract explicit type parameter
		if typeIdent, ok := fun.Index.(*ast.Ident); ok {
			explicitElemType = typeIdent.Name
		}
	default:
		return vecLoadInfo{}
	}

	if funcName != "LoadSlice" {
		return vecLoadInfo{}
	}

	// Check if we have an argument with a detectable slice size
	if len(call.Args) == 0 {
		return vecLoadInfo{}
	}

	sliceBytes := getSliceSize(call.Args[0])
	if sliceBytes <= 0 {
		return vecLoadInfo{}
	}

	// Determine element type: explicit type param > function's default
	effectiveElemType := ctx.elemType
	if explicitElemType != "" {
		effectiveElemType = explicitElemType
	}

	elemSize := elemTypeSize(effectiveElemType)
	if elemSize <= 0 {
		return vecLoadInfo{}
	}

	// Return explicit element type if provided
	returnElemType := ""
	if explicitElemType != "" {
		returnElemType = explicitElemType
	}

	return vecLoadInfo{
		lanes:    sliceBytes / elemSize,
		elemType: returnElemType,
	}
}

// inferTypeFromExpr analyzes an expression and returns its inferred type.
// Returns "int32" for expressions like hwy.ConvertToInt32(...), hwy.Set[int32](...), etc.
// Returns empty string if type cannot be inferred.
func inferTypeFromExpr(expr ast.Expr, ctx *transformContext) string {
	call, ok := expr.(*ast.CallExpr)
	if !ok {
		return ""
	}

	// Check for hwy.Set[int32](...) or similar indexed expressions
	if indexExpr, ok := call.Fun.(*ast.IndexExpr); ok {
		if sel, ok := indexExpr.X.(*ast.SelectorExpr); ok {
			if pkgIdent, ok := sel.X.(*ast.Ident); ok && pkgIdent.Name == "hwy" {
				// Check the type parameter
				if ident, ok := indexExpr.Index.(*ast.Ident); ok {
					if ident.Name == "int32" {
						return "int32"
					}
				}
			}
		}
	}

	// Check for hwy.ConvertToInt32(...) or method call .ConvertToInt32()
	if sel, ok := call.Fun.(*ast.SelectorExpr); ok {
		funcName := sel.Sel.Name
		switch funcName {
		case "ConvertToInt32":
			return "int32"
		case "And", "Or", "Xor", "AndNot", "ShiftLeft", "ShiftRight", "Add", "Sub", "Mul":
			// For bitwise and arithmetic operations, check if BOTH arguments are int32
			// This handles expressions like hwy.And(hwy.Add(kInt, intOne), intThree)
			if len(call.Args) >= 2 {
				arg0IsInt32 := isInt32ExprHelper(call.Args[0], ctx)
				arg1IsInt32 := isInt32ExprHelper(call.Args[1], ctx)
				if arg0IsInt32 && arg1IsInt32 {
					return "int32"
				}
			}
		}
	}

	return ""
}

// isInt32ExprHelper is a helper that checks if an expression is int32 without causing recursion.
func isInt32ExprHelper(expr ast.Expr, ctx *transformContext) bool {
	switch e := expr.(type) {
	case *ast.Ident:
		return ctx.varTypes[e.Name] == "int32"
	case *ast.CallExpr:
		// Recursively check for int32-returning calls
		return inferTypeFromExpr(e, ctx) == "int32"
	}
	return false
}

// isInt32Expr checks if an expression is of int32 type based on tracked variable types.
func isInt32Expr(expr ast.Expr, ctx *transformContext) bool {
	switch e := expr.(type) {
	case *ast.Ident:
		return ctx.varTypes[e.Name] == "int32"
	case *ast.CallExpr:
		// Check if this is a function call that returns int32
		return inferTypeFromExpr(e, ctx) == "int32"
	}
	return false
}

// isComparisonOp returns true if the operation is a comparison operation.
func isComparisonOp(opName string) bool {
	switch opName {
	case "Equal", "Greater", "GreaterThan", "Less", "LessThan",
		"GreaterEqual", "GreaterThanOrEqual", "LessEqual", "LessThanOrEqual":
		return true
	}
	return false
}

// collectLocalVariables walks the AST and collects all locally-defined variable names.
// This is used to exclude local variables from constant hoisting.
func collectLocalVariables(node ast.Node, ctx *transformContext) {
	if node == nil {
		return
	}

	ast.Inspect(node, func(n ast.Node) bool {
		switch stmt := n.(type) {
		case *ast.AssignStmt:
			// Collect all LHS identifiers from := and = assignments
			// Only := definitely defines new variables, but we track both to be safe
			if stmt.Tok == token.DEFINE {
				for i, lhs := range stmt.Lhs {
					if ident, ok := lhs.(*ast.Ident); ok {
						ctx.localVars[ident.Name] = true
						// Track variable types for type inference
						if i < len(stmt.Rhs) {
							if inferredType := inferTypeFromExpr(stmt.Rhs[i], ctx); inferredType != "" {
								ctx.varTypes[ident.Name] = inferredType
							}
							// Track vector lanes and element type for variables assigned from Load
							if loadInfo := inferVecLanesFromLoad(stmt.Rhs[i], ctx); loadInfo.lanes > 0 {
								ctx.varVecLanes[ident.Name] = loadInfo.lanes
								if loadInfo.elemType != "" {
									ctx.varVecElemType[ident.Name] = loadInfo.elemType
								}
								// Set function-wide inferred lanes on first detection
								if ctx.inferredFuncLanes == 0 {
									ctx.inferredFuncLanes = loadInfo.lanes
								}
							}
						}
					}
				}
			}
		case *ast.DeclStmt:
			// var declarations
			if genDecl, ok := stmt.Decl.(*ast.GenDecl); ok {
				if genDecl.Tok == token.VAR {
					for _, spec := range genDecl.Specs {
						if valueSpec, ok := spec.(*ast.ValueSpec); ok {
							for _, name := range valueSpec.Names {
								ctx.localVars[name.Name] = true
							}
						}
					}
				}
			}
		case *ast.RangeStmt:
			// for k, v := range ...
			if stmt.Tok == token.DEFINE {
				if ident, ok := stmt.Key.(*ast.Ident); ok && ident.Name != "_" {
					ctx.localVars[ident.Name] = true
				}
				if ident, ok := stmt.Value.(*ast.Ident); ok && ident.Name != "_" {
					ctx.localVars[ident.Name] = true
				}
			}
		case *ast.ForStmt:
			// for i := 0; ... - the init statement
			if stmt.Init != nil {
				if assign, ok := stmt.Init.(*ast.AssignStmt); ok && assign.Tok == token.DEFINE {
					for _, lhs := range assign.Lhs {
						if ident, ok := lhs.(*ast.Ident); ok {
							ctx.localVars[ident.Name] = true
						}
					}
				}
			}
		}
		return true
	})
}

// transformNode recursively transforms AST nodes.
func transformNode(node ast.Node, ctx *transformContext) {
	if node == nil {
		return
	}

	ast.Inspect(node, func(n ast.Node) bool {
		switch node := n.(type) {
		case *ast.CallExpr:
			transformCallExpr(node, ctx)
			// Also check for type conversions like T(1)
			transformTypeConversion(node, ctx)
			// Transform function references passed as arguments
			transformFuncRefArgs(node, ctx)
		case *ast.DeclStmt:
			// Transform variable declarations
			if genDecl, ok := node.Decl.(*ast.GenDecl); ok {
				transformGenDecl(genDecl, ctx)
			}
		case *ast.AssignStmt:
			// Transform assignments
			transformAssignStmt(node, ctx)
		case *ast.ForStmt:
			// Transform for loop for SIMD (stride, condition)
			transformForStmt(node, ctx)
		case *ast.CompositeLit:
			// Transform composite literal types (e.g., [4]hwy.Vec[float32]{} -> [4]asm.Float32x4{})
			transformCompositeLit(node, ctx)
		}
		return true
	})
}

// transformForStmt transforms for loops for SIMD targets.
// Changes: for ii := 0; ii < size; ii += v.NumLanes()
// To:      for ii := 0; ii+8 <= size; ii += 8
// Also handles: for ii := 0; ii < size; ii += lanes (where lanes was from NumLanes())
// Only transforms loops that use NumLanes() stride (not scalar tail loops).
func transformForStmt(stmt *ast.ForStmt, ctx *transformContext) {
	if ctx.target.Name == "Fallback" || ctx.loopInfo == nil {
		return
	}

	lanes := ctx.target.LanesFor(ctx.elemType)

	// Check if this loop uses NumLanes() stride - only transform those loops
	isSimdLoop := false
	if assignStmt, ok := stmt.Post.(*ast.AssignStmt); ok {
		if len(assignStmt.Rhs) == 1 {
			// Case 1: ii += v.NumLanes() - direct call
			if call, ok := assignStmt.Rhs[0].(*ast.CallExpr); ok {
				if sel, ok := call.Fun.(*ast.SelectorExpr); ok {
					if sel.Sel.Name == "NumElements" || sel.Sel.Name == "NumLanes" {
						isSimdLoop = true
						// Transform post: ii += v.NumLanes() -> ii += lanes
						assignStmt.Rhs[0] = &ast.BasicLit{
							Kind:  token.INT,
							Value: strconv.Itoa(lanes),
						}
					}
				}
			}
			// Case 2: ii += lanes - variable assigned from NumLanes()
			if ident, ok := assignStmt.Rhs[0].(*ast.Ident); ok {
				if ctx.lanesVars[ident.Name] {
					isSimdLoop = true
					// The variable was already replaced with a constant in transformAssignStmt,
					// but we still need to transform the loop condition
				}
			}
			// Case 3: ii += 8 (or other constant) - already transformed
			if lit, ok := assignStmt.Rhs[0].(*ast.BasicLit); ok {
				if lit.Kind == token.INT {
					// Check if the value matches our lanes - this means it was already transformed
					if lit.Value == strconv.Itoa(lanes) {
						isSimdLoop = true
					}
				}
			}
		}
	}

	// Only transform condition for SIMD loops (not scalar tail loops)
	if isSimdLoop {
		// Transform condition: ii < size -> ii+lanes <= size
		if binExpr, ok := stmt.Cond.(*ast.BinaryExpr); ok {
			if binExpr.Op == token.LSS {
				// Change ii < size to ii+lanes <= size
				binExpr.Op = token.LEQ
				binExpr.X = &ast.BinaryExpr{
					X:  binExpr.X,
					Op: token.ADD,
					Y: &ast.BasicLit{
						Kind:  token.INT,
						Value: strconv.Itoa(lanes),
					},
				}
			}
		}
	}
}

// transformCompositeLit transforms composite literal types for SIMD targets.
// Converts types like [4]hwy.Vec[float32]{} to [4]asm.Float32x4{} for NEON
// or [8]archsimd.Float32x8{} for AVX2.
func transformCompositeLit(lit *ast.CompositeLit, ctx *transformContext) {
	if lit.Type == nil {
		return
	}

	// For fallback target, don't transform hwy.Vec types
	if ctx.target.Name == "Fallback" {
		return
	}

	// Check if the type is an array with hwy.Vec element type
	arrayType, ok := lit.Type.(*ast.ArrayType)
	if !ok {
		return
	}

	// Transform the element type if it's hwy.Vec[T] or similar
	typeStr := exprToString(arrayType.Elt)

	// First specialize generic type parameters (T -> float32)
	specialized := specializeType(typeStr, ctx.typeParams, ctx.elemType)

	// Then transform hwy.Vec[float32] -> asm.Float32x4 for SIMD targets
	specialized = specializeVecType(specialized, ctx.elemType, ctx.target, ctx.skipHalfPrecNEON)

	if specialized != typeStr {
		arrayType.Elt = parseTypeExpr(specialized)
	}
}

// transformTypeConversion converts T(1) to float32(1) for generic type parameters.
func transformTypeConversion(call *ast.CallExpr, ctx *transformContext) {
	// Check if this is a type conversion T(value) where T is a type parameter
	ident, ok := call.Fun.(*ast.Ident)
	if !ok {
		return
	}

	// Check if the identifier is a type parameter
	for _, tp := range ctx.typeParams {
		if ident.Name == tp.Name {
			// Replace T with the concrete element type
			ident.Name = ctx.elemType
			return
		}
	}
}

// transformCallExpr transforms hwy.* and contrib.* function calls.
func transformCallExpr(call *ast.CallExpr, ctx *transformContext) {
	// First, check for calls to other Base* functions and add target suffix
	// This applies to ALL targets including fallback, since generated functions
	// are always concrete (BaseApply_fallback, not generic BaseApply)
	if ident, ok := call.Fun.(*ast.Ident); ok {
		if strings.HasPrefix(ident.Name, "Base") {
			// Transform BaseFoo to BaseFoo_avx2 (or BaseFoo_fallback, etc.)
			suffix := ctx.target.Suffix()
			// Add type suffix for non-float32 types, but ONLY if the current function
			// has type parameters (indicating it's a generic function with type variants).
			// Concrete functions like BaseEncodeStreamVByte32GroupSIMD([]uint32) don't
			// have type variants, so their internal Base* calls shouldn't add type suffix.
			if len(ctx.typeParams) > 0 {
				switch ctx.elemType {
				case "float64":
					suffix = suffix + "_Float64"
				case "hwy.Float16":
					suffix = suffix + "_Float16"
				case "hwy.BFloat16":
					suffix = suffix + "_BFloat16"
				case "int32":
					suffix = suffix + "_Int32"
				case "int64":
					suffix = suffix + "_Int64"
				case "uint32":
					suffix = suffix + "_Uint32"
				case "uint64":
					suffix = suffix + "_Uint64"
				}
			}
			ident.Name = ident.Name + suffix
		}
	}

	// Transform Vec method calls like .Store() -> .StoreSlice() for SIMD targets
	// This handles cases like fn(x).Store(dst) where fn returns a Vec
	// Skip this for package-level function calls like hwy.StoreSlice() which are handled later
	if ctx.target.Name != "Fallback" {
		if sel, ok := call.Fun.(*ast.SelectorExpr); ok {
			// Don't transform package-level function calls
			if ident, ok := sel.X.(*ast.Ident); ok && ident.Name == "hwy" {
				// Skip - this is a package-level function, handled later
			} else {
				switch sel.Sel.Name {
				case "Store":
					// Transform .Store(dst) -> .StoreSlice(dst)
					// But skip if the argument is a pointer type (from Store transformation)
					// Store produces v.Store((*[N]T)(unsafe.Pointer(...))) which should stay as Store
					isPointerArg := false
					if len(call.Args) == 1 {
						if _, isCall := call.Args[0].(*ast.CallExpr); isCall {
							// Argument is a call expression like (*[N]T)(ptr) - likely from Store
							isPointerArg = true
						}
					}
					if !isPointerArg {
						sel.Sel.Name = "StoreSlice"
						// Cast []hwy.Float16/[]hwy.BFloat16 -> []uint16 for half-precision
						if isHalfPrecisionType(ctx.elemType) && len(call.Args) == 1 {
							call.Args[0] = halfPrecSliceToUint16(call.Args[0])
						}
					}
				case "Data":
					transformDataMethod(call, ctx)
					return
				case "GetBit":
					transformGetBitMethod(call, ctx)
					return
				}
			}
		}
	}

	// Also handle generic Base* calls like BaseFoo[T](x)
	// All targets get the suffix since generated functions are concrete
	if indexExpr, ok := call.Fun.(*ast.IndexExpr); ok {
		if ident, ok := indexExpr.X.(*ast.Ident); ok {
			if strings.HasPrefix(ident.Name, "Base") {
				suffix := ctx.target.Suffix()
				// Add type suffix for non-float32 types
				switch ctx.elemType {
				case "float64":
					suffix = suffix + "_Float64"
				case "hwy.Float16":
					suffix = suffix + "_Float16"
				case "hwy.BFloat16":
					suffix = suffix + "_BFloat16"
				case "int32":
					suffix = suffix + "_Int32"
				case "int64":
					suffix = suffix + "_Int64"
				case "uint32":
					suffix = suffix + "_Uint32"
				case "uint64":
					suffix = suffix + "_Uint64"
				}
				// Strip the type param and add suffix
				call.Fun = ast.NewIdent(ident.Name + suffix)
			}
		}
	}

	var selExpr *ast.SelectorExpr
	var ok bool
	var hasExplicitTypeParam bool // Track if we have an explicit type param to preserve

	// Handle both regular calls (hwy.Load) and generic calls (hwy.Zero[T])
	switch fun := call.Fun.(type) {
	case *ast.SelectorExpr:
		selExpr = fun
	case *ast.IndexExpr:
		// Generic function call like hwy.Zero[T]() or hwy.Load[uint8]()
		// The IndexExpr wraps the SelectorExpr
		selExpr, ok = fun.X.(*ast.SelectorExpr)
		if !ok {
			return
		}
		// Check if the type parameter is a concrete type (not a generic type param like T)
		if typeIdent, ok := fun.Index.(*ast.Ident); ok {
			typeName := typeIdent.Name
			isTypeParam := false
			for _, tp := range ctx.typeParams {
				if typeName == tp.Name {
					isTypeParam = true
					break
				}
			}
			if !isTypeParam {
				// This is an explicit concrete type like uint8, float32, etc.
				// Keep the IndexExpr so transformToFunction can use it
				hasExplicitTypeParam = true
			}
		}
		// Transform hwy.Const[T](val) to hwy.Set(val) for non-float32 types.
		// hwy.Const takes float32, which loses precision for float64 targets.
		// Named constants are suffix-transformed to the correct type by the generator.
		// Literals (like 0.5) are untyped in Go, so Set[float64](0.5) preserves
		// full precision. For half-precision types, literals must stay with Const
		// since Set[Float16](0.5) won't compile (no implicit conversion).
		// Note: We don't return here - let the transformation continue so Set gets
		// transformed to asm.Broadcast* for SIMD targets.
		if selExpr.Sel.Name == "Const" {
			if ident, ok := selExpr.X.(*ast.Ident); ok && ident.Name == "hwy" {
				if ctx.elemType != "float32" && len(call.Args) > 0 {
					// Named constants and binary expressions: always convert to Set
					if _, isIdent := call.Args[0].(*ast.Ident); isIdent {
						selExpr.Sel.Name = "Set"
					} else if _, isBinary := call.Args[0].(*ast.BinaryExpr); isBinary {
						selExpr.Sel.Name = "Set"
					}
					// Literals: convert to Set for native types (float64) to avoid
					// float32 precision truncation. Half-precision types must keep
					// Const because Go can't convert untyped floats to Float16/BFloat16.
					if !isHalfPrecisionType(ctx.elemType) {
						if _, isLit := call.Args[0].(*ast.BasicLit); isLit {
							selExpr.Sel.Name = "Set"
						}
					}
				}
			}
		}
		if ctx.target.Name == "Fallback" {
			// For fallback, replace type param with concrete type
			// hwy.Zero[T]() -> hwy.Zero[float32]()
			for _, tp := range ctx.typeParams {
				if ident, ok := fun.Index.(*ast.Ident); ok && ident.Name == tp.Name {
					ident.Name = ctx.elemType
				}
			}
			// Keep the IndexExpr (with type param), just update the type
		} else if isHalfPrecisionType(ctx.elemType) {
			// For Float16/BFloat16 on SIMD targets, keep the type param for functions
			// like Const, Set, Zero that need it for type inference
			funcName := selExpr.Sel.Name
			switch funcName {
			case "Const":
				// For NEON target with asm types, convert to asm.BroadcastFloat16x8/BFloat16x8
				if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
					broadcastFuncName := "BroadcastFloat16x8"
					if isBFloat16Type(ctx.elemType) {
						broadcastFuncName = "BroadcastBFloat16x8"
					}
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("asm"),
						Sel: ast.NewIdent(broadcastFuncName),
					}
					if len(call.Args) > 0 {
						convFunc := "Float32ToFloat16"
						if isBFloat16Type(ctx.elemType) {
							convFunc = "Float32ToBFloat16"
						}
						call.Args[0] = &ast.CallExpr{
							Fun: ast.NewIdent("uint16"),
							Args: []ast.Expr{
								&ast.CallExpr{
									Fun: &ast.SelectorExpr{
										X:   ast.NewIdent("hwy"),
										Sel: ast.NewIdent(convFunc),
									},
									Args: []ast.Expr{
										&ast.CallExpr{
											Fun:  ast.NewIdent("float32"),
											Args: []ast.Expr{call.Args[0]},
										},
									},
								},
							},
						}
					}
					return
				}
				// For AVX promoted types, convert to asm.BroadcastFloat16x8AVX2(...) etc.
				if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
					typeName := ctx.target.TypeMap[ctx.elemType]
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("asm"),
						Sel: ast.NewIdent("Broadcast" + typeName),
					}
					if len(call.Args) > 0 {
						convFunc := "Float32ToFloat16"
						if isBFloat16Type(ctx.elemType) {
							convFunc = "Float32ToBFloat16"
						}
						call.Args[0] = &ast.CallExpr{
							Fun: ast.NewIdent("uint16"),
							Args: []ast.Expr{
								&ast.CallExpr{
									Fun: &ast.SelectorExpr{
										X:   ast.NewIdent("hwy"),
										Sel: ast.NewIdent(convFunc),
									},
									Args: []ast.Expr{
										&ast.CallExpr{
											Fun:  ast.NewIdent("float32"),
											Args: []ast.Expr{call.Args[0]},
										},
									},
								},
							},
						}
					}
					return
				}
				// For Fallback or skip: replace type param with concrete type
				for _, tp := range ctx.typeParams {
					if ident, ok := fun.Index.(*ast.Ident); ok && ident.Name == tp.Name {
						ident.Name = ctx.elemType
					}
				}
			case "Set", "Zero":
				// For NEON without skip, these are handled later in the SelectorExpr path
				// Replace type param with concrete type (e.g., hwy.Set[T] -> hwy.Set[hwy.Float16])
				for _, tp := range ctx.typeParams {
					if ident, ok := fun.Index.(*ast.Ident); ok && ident.Name == tp.Name {
						ident.Name = ctx.elemType
					}
				}
				// Keep the IndexExpr with the concrete type
			case "ConvertExponentToFloat":
				if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
					// For AVX promoted: asm.Float16x8AVX2FromFloat32x8(e.ConvertToFloat32())
					wrapFunc := fmt.Sprintf("%sFromFloat32x%d", ctx.target.TypeMap[ctx.elemType], ctx.target.LanesFor("float32"))
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("asm"),
						Sel: ast.NewIdent(wrapFunc),
					}
					call.Args = []ast.Expr{
						&ast.CallExpr{
							Fun: &ast.SelectorExpr{
								X:   call.Args[0],
								Sel: ast.NewIdent("ConvertToFloat32"),
							},
						},
					}
					return
				}
				// Transform to non-generic ConvertToF16/ConvertToBF16
				if ctx.elemType == "hwy.Float16" {
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("hwy"),
						Sel: ast.NewIdent("ConvertToF16"),
					}
				} else {
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("hwy"),
						Sel: ast.NewIdent("ConvertToBF16"),
					}
				}
				return // Already handled, don't continue transformation
			default:
				// For other functions, strip the type param (will be transformed later)
				call.Fun = selExpr
			}
		} else {
			// For SIMD targets with native types, handle special cases first
			funcName := selExpr.Sel.Name
			switch funcName {
			case "ConvertExponentToFloat":
				// Transform to method call: e.ConvertToFloat32() or e.ConvertToFloat64()
				if len(call.Args) >= 1 {
					var methodName string
					if ctx.elemType == "float64" {
						methodName = "ConvertToFloat64"
					} else {
						methodName = "ConvertToFloat32"
					}
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent(methodName),
					}
					call.Args = nil
				}
				return
			default:
				// Strip the type param (will be transformed later)
				// BUT preserve IndexExpr if we have an explicit concrete type param
				// (e.g., hwy.Load[uint8]) so transformToFunction can use it
				if !hasExplicitTypeParam {
					call.Fun = selExpr
				}
			}
		}
	case *ast.IndexListExpr:
		// Generic function call with multiple type params like hwy.Func[T, U]()
		selExpr, ok = fun.X.(*ast.SelectorExpr)
		if !ok {
			return
		}
		if ctx.target.Name == "Fallback" {
			// For fallback, replace type params with concrete types
			for i, idx := range fun.Indices {
				if ident, ok := idx.(*ast.Ident); ok {
					for _, tp := range ctx.typeParams {
						if ident.Name == tp.Name {
							fun.Indices[i] = ast.NewIdent(ctx.elemType)
						}
					}
				}
			}
		} else {
			call.Fun = selExpr
		}
	default:
		return
	}

	ident, ok := selExpr.X.(*ast.Ident)
	if !ok {
		return
	}

	// Handle hwy.* and contrib subpackage calls
	switch ident.Name {
	case "hwy", "contrib", "math", "vec", "matvec", "matmul", "algo", "image", "bitpack", "sort":
		// Continue processing
	default:
		return
	}

	funcName := selExpr.Sel.Name

	// Handle cross-package Base* function calls (e.g., algo.BaseApply, math.BaseExpVec)
	// These need target suffix added, similar to same-package Base* calls
	if strings.HasPrefix(funcName, "Base") {
		suffix := ctx.target.Suffix()
		if ctx.elemType == "float64" {
			suffix = suffix + "_Float64"
		} else if isFloat16Type(ctx.elemType) {
			suffix = suffix + "_Float16"
		} else if isBFloat16Type(ctx.elemType) {
			suffix = suffix + "_BFloat16"
		}
		selExpr.Sel.Name = funcName + suffix
		// Strip the IndexExpr (type parameter) if present, since the
		// target-specific variant is a concrete function, not generic.
		// e.g., math.BaseSigmoidVec[float32](x) -> math.BaseSigmoidVec_neon(x)
		call.Fun = selExpr
		return
	}

	opInfo, ok := ctx.target.OpMap[funcName]
	if !ok {
		// Unknown operation, leave as-is
		return
	}

	// Transform based on operation type
	if opInfo.IsMethod {
		transformToMethod(call, funcName, opInfo, ctx)
	} else {
		transformToFunction(call, funcName, opInfo, ctx)
	}
}

// transformDataMethod transforms v.Data() to a temporary slice.
func transformDataMethod(call *ast.CallExpr, ctx *transformContext) {
	sel, ok := call.Fun.(*ast.SelectorExpr)
	if !ok {
		return
	}
	vecExpr := sel.X
	lanes := ctx.target.LanesFor(ctx.elemType)

	// func() []T { var tmp [lanes]T; v.StoreSlice(tmp[:]); return tmp[:] }()

	// var tmp [lanes]T
	decl := &ast.DeclStmt{
		Decl: &ast.GenDecl{
			Tok: token.VAR,
			Specs: []ast.Spec{
				&ast.ValueSpec{
					Names: []*ast.Ident{ast.NewIdent("_simd_tmp")},
					Type: &ast.ArrayType{
						Len: &ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(lanes)},
						Elt: ast.NewIdent(ctx.elemType),
					},
				},
			},
		},
	}

	// v.StoreSlice(tmp[:]) or hwy.StoreSlice(v, tmp[:]) for half-precision
	var storeCall *ast.CallExpr
	if isHalfPrecisionType(ctx.elemType) && !isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
		// hwy.StoreSlice(v, tmp[:]) for half-precision types on Fallback
		storeFun := &ast.SelectorExpr{
			X:   ast.NewIdent("hwy"),
			Sel: ast.NewIdent("Store"),
		}
		storeCall = &ast.CallExpr{
			Fun: storeFun,
			Args: []ast.Expr{
				cloneExpr(vecExpr),
				&ast.SliceExpr{
					X: ast.NewIdent("_simd_tmp"),
				},
			},
		}
	} else if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
		// v.StoreSlice(cast(tmp[:])) for AVX promoted half-precision
		storeCall = &ast.CallExpr{
			Fun: &ast.SelectorExpr{
				X:   cloneExpr(vecExpr),
				Sel: ast.NewIdent("StoreSlice"),
			},
			Args: []ast.Expr{
				halfPrecSliceToUint16(&ast.SliceExpr{
					X: ast.NewIdent("_simd_tmp"),
				}),
			},
		}
	} else {
		// v.StoreSlice(tmp[:]) for native SIMD types
		storeCall = &ast.CallExpr{
			Fun: &ast.SelectorExpr{
				X:   cloneExpr(vecExpr),
				Sel: ast.NewIdent("StoreSlice"),
			},
			Args: []ast.Expr{
				&ast.SliceExpr{
					X: ast.NewIdent("_simd_tmp"),
				},
			},
		}
	}

	// return tmp[:]
	retStmt := &ast.ReturnStmt{
		Results: []ast.Expr{
			&ast.SliceExpr{
				X: ast.NewIdent("_simd_tmp"),
			},
		},
	}

	// Function literal
	funcLit := &ast.FuncLit{
		Type: &ast.FuncType{
			Results: &ast.FieldList{
				List: []*ast.Field{
					{Type: &ast.ArrayType{Elt: ast.NewIdent(ctx.elemType)}},
				},
			},
		},
		Body: &ast.BlockStmt{
			List: []ast.Stmt{
				decl,
				&ast.ExprStmt{X: storeCall},
				retStmt,
			},
		},
	}

	// Replace call with invocation - modify fields directly instead of replacing entire struct
	call.Fun = funcLit
	call.Args = nil
	call.Ellipsis = 0
}

// transformGetBitMethod transforms mask.GetBit(i) to check the i-th element.
func transformGetBitMethod(call *ast.CallExpr, ctx *transformContext) {
	if len(call.Args) != 1 {
		return
	}
	indexExpr := call.Args[0]
	sel, ok := call.Fun.(*ast.SelectorExpr)
	if !ok {
		return
	}
	maskExpr := sel.X
	lanes := ctx.target.LanesFor(ctx.elemType)

	// For half-precision types on Fallback/NEON (non-AVX-promoted), keep as hwy.Mask.GetBit
	if isHalfPrecisionType(ctx.elemType) && !isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
		transformGetBitMethodHalfPrecision(call, maskExpr, indexExpr, lanes, ctx)
		return
	}
	// For AVX promoted half-precision, fall through to use SIMD extraction (same as float32)

	// Use Int32 vector for extraction to match most masks used with GetBit
	intVecTypeName := getVectorTypeNameForInt("int32", ctx.elemType, ctx.target)
	pkgName := getVecPackageName(ctx.target)

	// func() bool {
	//   vOne := pkg.BroadcastInt32x4(1)
	//   vZero := pkg.BroadcastInt32x4(0)
	//   vMasked := vOne.Merge(vZero, mask)
	//   var tmp [lanes]int32
	//   vMasked.StoreSlice(tmp[:])
	//   return tmp[i] != 0
	// }()

	// 1. vOne := pkg.BroadcastInt32x*(1)
	vOneDecl := &ast.AssignStmt{
		Lhs: []ast.Expr{ast.NewIdent("_vOne")},
		Tok: token.DEFINE,
		Rhs: []ast.Expr{
			&ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + intVecTypeName),
				},
				Args: []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: "1"}},
			},
		},
	}

	// 2. vZero := pkg.BroadcastInt32x*(0)
	vZeroDecl := &ast.AssignStmt{
		Lhs: []ast.Expr{ast.NewIdent("_vZero")},
		Tok: token.DEFINE,
		Rhs: []ast.Expr{
			&ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + intVecTypeName),
				},
				Args: []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: "0"}},
			},
		},
	}

	// 3. vMasked := vOne.Merge(vZero, mask)
	vMaskedDecl := &ast.AssignStmt{
		Lhs: []ast.Expr{ast.NewIdent("_vMasked")},
		Tok: token.DEFINE,
		Rhs: []ast.Expr{
			&ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent("_vOne"),
					Sel: ast.NewIdent("Merge"),
				},
				Args: []ast.Expr{
					ast.NewIdent("_vZero"),
					cloneExpr(maskExpr),
				},
			},
		},
	}

	// 4. var tmp [lanes]int32
	tmpDecl := &ast.DeclStmt{
		Decl: &ast.GenDecl{
			Tok: token.VAR,
			Specs: []ast.Spec{
				&ast.ValueSpec{
					Names: []*ast.Ident{ast.NewIdent("_simd_mask_tmp")},
					Type: &ast.ArrayType{
						Len: &ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(lanes)},
						Elt: ast.NewIdent("int32"),
					},
				},
			},
		},
	}

	// 5. vMasked.StoreSlice(tmp[:])
	storeCall := &ast.CallExpr{
		Fun: &ast.SelectorExpr{
			X:   ast.NewIdent("_vMasked"),
			Sel: ast.NewIdent("StoreSlice"),
		},
		Args: []ast.Expr{
			&ast.SliceExpr{
				X: ast.NewIdent("_simd_mask_tmp"),
			},
		},
	}

	// 6. return tmp[i] != 0
	checkExpr := &ast.BinaryExpr{
		X: &ast.IndexExpr{
			X:     ast.NewIdent("_simd_mask_tmp"),
			Index: cloneExpr(indexExpr),
		},
		Op: token.NEQ,
		Y:  &ast.BasicLit{Kind: token.INT, Value: "0"},
	}

	retStmt := &ast.ReturnStmt{
		Results: []ast.Expr{checkExpr},
	}

	funcLit := &ast.FuncLit{
		Type: &ast.FuncType{
			Results: &ast.FieldList{
				List: []*ast.Field{
					{Type: ast.NewIdent("bool")},
				},
			},
		},
		Body: &ast.BlockStmt{
			List: []ast.Stmt{
				vOneDecl,
				vZeroDecl,
				vMaskedDecl,
				tmpDecl,
				&ast.ExprStmt{X: storeCall},
				retStmt,
			},
		},
	}

	*call = ast.CallExpr{
		Fun: funcLit,
	}
}

// transformToMethod converts hwy.Add(a, b) to a.Add(b) for SIMD targets.
// For Fallback, keeps hwy.Add(a, b) as-is.
func transformToMethod(call *ast.CallExpr, funcName string, opInfo OpInfo, ctx *transformContext) {
	if len(call.Args) < 1 {
		return
	}

	// For fallback, keep hwy calls as-is (don't convert to method calls)
	if ctx.target.Name == "Fallback" {
		// Just update the package name if needed
		switch fun := call.Fun.(type) {
		case *ast.SelectorExpr:
			fun.X = ast.NewIdent("hwy")
		case *ast.IndexExpr:
			if sel, ok := fun.X.(*ast.SelectorExpr); ok {
				sel.X = ast.NewIdent("hwy")
			}
		}
		return
	}

	// For Float16/BFloat16 on SIMD targets, use hwy package functions instead of methods.
	// archsimd doesn't have native support for half-precision types.
	if isHalfPrecisionType(ctx.elemType) {
		// For NEON target, convert Merge/IfThenElse to asm.IfThenElseFloat16/BFloat16
		if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
			if funcName == "Merge" && len(call.Args) >= 3 {
				asmFunc := "IfThenElseFloat16"
				if isBFloat16Type(ctx.elemType) {
					asmFunc = "IfThenElseBFloat16"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(asmFunc),
				}
				// Reorder: (yes, no, mask) -> (mask, yes, no)
				call.Args = []ast.Expr{call.Args[2], call.Args[0], call.Args[1]}
				return
			}
			if funcName == "IfThenElse" && len(call.Args) >= 3 {
				asmFunc := "IfThenElseFloat16"
				if isBFloat16Type(ctx.elemType) {
					asmFunc = "IfThenElseBFloat16"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(asmFunc),
				}
				return
			}
		}
		// For AVX promoted types, convert Merge/IfThenElse to method calls
		if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
			if funcName == "Merge" && len(call.Args) >= 3 {
				// hwy.Merge(yes, no, mask) -> yes.Merge(no, mask)
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent("Merge"),
				}
				call.Args = []ast.Expr{call.Args[1], call.Args[2]}
				return
			}
			if funcName == "IfThenElse" && len(call.Args) >= 3 {
				// hwy.IfThenElse(mask, yes, no) -> yes.Merge(no, mask)
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[1],
					Sel: ast.NewIdent("Merge"),
				}
				call.Args = []ast.Expr{call.Args[2], call.Args[0]}
				return
			}
		}
		// Handle Merge specially - needs argument reordering
		// hwy.Merge(yes, no, mask) -> hwy.IfThenElseF16(mask, yes, no)
		if funcName == "Merge" && len(call.Args) >= 3 {
			suffix := "F16"
			if isBFloat16Type(ctx.elemType) {
				suffix = "BF16"
			}
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent("IfThenElse" + suffix),
			}
			// Reorder: (yes, no, mask) -> (mask, yes, no)
			call.Args = []ast.Expr{call.Args[2], call.Args[0], call.Args[1]}
			return
		}
		// Handle IfThenElse - same signature as IfThenElseF16, no reordering needed
		// hwy.IfThenElse(mask, yes, no) -> hwy.IfThenElseF16(mask, yes, no)
		if funcName == "IfThenElse" && len(call.Args) >= 3 {
			suffix := "F16"
			if isBFloat16Type(ctx.elemType) {
				suffix = "BF16"
			}
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent("IfThenElse" + suffix),
			}
			// Args stay in same order
			return
		}

		if f16FuncName := getHalfPrecisionFuncName(funcName, ctx.elemType); f16FuncName != "" {
			// For operations with 2 operands, check if both are int32 - if so, keep generic hwy function
			// This handles comparisons (Equal, Greater), arithmetic (Add, Sub, Mul), and bitwise (And, Or)
			// operations that may operate on int32 intermediate values (like octant calculations in trig functions)
			if len(call.Args) >= 2 {
				if isInt32Expr(call.Args[0], ctx) && isInt32Expr(call.Args[1], ctx) {
					if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
						// For AVX promoted, int32 variables are archsimd.Int32x8 - use method calls
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(funcName),
						}
						call.Args = call.Args[1:]
						return
					}
					// Keep as generic hwy.Add, hwy.Equal, hwy.And, etc. for int32 operands
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("hwy"),
						Sel: ast.NewIdent(funcName),
					}
					return
				}
			}
			// For NEON and AVX promoted targets, use method calls on asm types for arithmetic and comparison operations
			if (ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				switch funcName {
				case "Add", "Sub", "Mul", "Div", "Min", "Max":
					// hwy.AddF16(a, b) -> a.Add(b)
					if len(call.Args) >= 2 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(funcName),
						}
						call.Args = call.Args[1:]
						return
					}
				case "FMA", "MulAdd":
					// hwy.FMAF16(a, b, c) -> a.MulAdd(b, c)
					if len(call.Args) >= 3 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent("MulAdd"),
						}
						call.Args = call.Args[1:]
						return
					}
				case "Neg", "Abs", "Sqrt":
					// hwy.NegF16(a) -> a.Neg()
					if len(call.Args) >= 1 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(funcName),
						}
						call.Args = nil
						return
					}
				case "ReduceSum":
					// hwy.ReduceSum(v) -> v.ReduceSum()
					if len(call.Args) >= 1 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent("ReduceSum"),
						}
						call.Args = nil
						return
					}
				case "ReduceMax", "ReduceMin":
					// hwy.ReduceMaxF16(v) -> v.ReduceMax()
					if len(call.Args) >= 1 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(funcName),
						}
						call.Args = nil
						return
					}
				case "GreaterThan", "Greater", "LessThan", "Less",
					"GreaterEqual", "GreaterThanOrEqual", "LessEqual", "LessThanOrEqual",
					"Equal", "NotEqual":
					if len(call.Args) >= 2 {
						methodName := funcName
						if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
							// AVX promoted types use archsimd-style short names
							switch funcName {
							case "GreaterThan":
								methodName = "Greater"
							case "LessThan":
								methodName = "Less"
							case "GreaterThanOrEqual":
								methodName = "GreaterEqual"
							case "LessThanOrEqual":
								methodName = "LessEqual"
							}
						} else {
							// NEON uses long names
							switch funcName {
							case "Greater":
								methodName = "GreaterThan"
							case "Less":
								methodName = "LessThan"
							case "GreaterEqual":
								methodName = "GreaterThanOrEqual"
							case "LessEqual":
								methodName = "LessThanOrEqual"
							}
						}
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(methodName),
						}
						call.Args = call.Args[1:]
						return
					}
				}
			}
			// For Fallback, transform to hwy.AddF16(a, b), hwy.MulF16(a, b), etc.
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent(f16FuncName),
			}
			// Args stay as-is (already in the correct order for function calls)
			return
		}

		// For operations that don't have F16/BF16 variants, keep as hwy function calls
		// instead of converting to method calls (which don't exist on hwy.Vec[Float16])
		switch funcName {
		case "RoundToEven", "ConvertToInt32", "ConvertToFloat32":
			// For AVX promoted types, use method calls
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 1 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent(funcName),
					}
					call.Args = nil
					return
				}
			}
			// Keep as hwy function call for Fallback - do NOT convert to method
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent(funcName),
			}
			return
		case "Not":
			// For NEON/AVX promoted targets with half-precision types, convert to method call: hwy.Not(v) -> v.Not()
			if (ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && isHalfPrecisionType(ctx.elemType)) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 1 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("Not"),
					}
					call.Args = nil
					return
				}
			}
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent(funcName),
			}
			return
		case "Xor":
			// For NEON/AVX promoted targets with half-precision types, convert to method call: hwy.Xor(a, b) -> a.Xor(b)
			if (ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && isHalfPrecisionType(ctx.elemType)) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 2 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("Xor"),
					}
					call.Args = call.Args[1:]
					return
				}
			}
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent(funcName),
			}
			return
		case "And":
			// For NEON/AVX promoted targets with half-precision types, convert to method call: hwy.And(a, b) -> a.And(b)
			if (ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && isHalfPrecisionType(ctx.elemType)) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 2 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("And"),
					}
					call.Args = call.Args[1:]
					return
				}
			}
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent(funcName),
			}
			return
		case "Or", "AndNot":
			// For AVX promoted types, use method calls
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 2 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent(funcName),
					}
					call.Args = call.Args[1:]
					return
				}
			}
			// These don't have method forms on NEON asm types yet
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent(funcName),
			}
			return
		case "NotEqual":
			// For AVX promoted types, use method call
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 2 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("NotEqual"),
					}
					call.Args = call.Args[1:]
					return
				}
			}
			// For Fallback, keep as hwy.NotEqual(a, b)
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent("NotEqual"),
			}
			return
		case "Pow":
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 2 {
				// For AVX promoted: inline scalar Pow via float32 buffers
				// Use AsFloat32xN() instead of Data() to avoid transformer interception
				lanes := ctx.target.LanesFor("float32")
				asmType := ctx.target.TypeMap[ctx.elemType]
				wrapFunc := fmt.Sprintf("%sFromFloat32x%d", asmType, lanes)
				loadFunc := fmt.Sprintf("LoadFloat32x%dSlice", lanes)
				asF32Method := fmt.Sprintf("AsFloat32x%d", lanes)
				vecPkg := getVecPackageName(ctx.target)
				lanesStr := strconv.Itoa(lanes)
				baseArg := call.Args[0]
				expArg := call.Args[1]

				call.Fun = genPowIIFE(asmType, wrapFunc, loadFunc, asF32Method, vecPkg, lanesStr, baseArg, expArg)
				call.Args = nil
				return
			}
			// hwy.Pow on hwy.Vec[Float16/BFloat16] doesn't have a method form,
			// keep as hwy.Pow(base, exp)
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent("Pow"),
			}
			return
		case "MaskAnd", "MaskOr", "MaskXor", "MaskAndNot":
			// For AVX promoted types, masks are archsimd.Mask32x8 - use method calls
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 2 {
				methodName := strings.TrimPrefix(funcName, "Mask") // MaskAnd -> And
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent(methodName),
				}
				call.Args = call.Args[1:]
				return
			}
			// Mask operations on hwy.Mask[Float16/BFloat16] don't have method forms,
			// so keep them as hwy.MaskAnd, hwy.MaskOr, etc.
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent(funcName),
			}
			return
		case "Store":
			// For NEON target, convert to method call with unsafe.Pointer
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && len(call.Args) >= 2 {
				// hwy.StoreSlice(v, dst) -> v.StorePtr(unsafe.Pointer(&dst[0]))
				vecArg := call.Args[0]
				sliceArg := call.Args[1]
				call.Fun = &ast.SelectorExpr{
					X:   vecArg,
					Sel: ast.NewIdent("StorePtr"),
				}
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{
							&ast.UnaryExpr{
								Op: token.AND,
								X: &ast.IndexExpr{
									X:     sliceArg,
									Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
								},
							},
						},
					},
				}
				return
			}
			// For AVX promoted types, convert to pointer-based method call: v.StorePtr(ptr)
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 2 {
				vecArg := call.Args[0]
				sliceArg := call.Args[1]
				call.Fun = &ast.SelectorExpr{
					X:   vecArg,
					Sel: ast.NewIdent("StorePtr"),
				}
				// unsafe.Pointer(&dst[0])
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{
							&ast.UnaryExpr{
								Op: token.AND,
								X: &ast.IndexExpr{
									X:     sliceArg,
									Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
								},
							},
						},
					},
				}
				return
			}
			// For Fallback, keep hwy.StoreSlice(v, dst) as-is
			return
		case "Pow2":
			// For AVX promoted types: Pow2 operates on float32 internally
			// asm.Float16x8AVX2FromFloat32x8(hwy.Pow2_AVX2_F32x8(kInt))
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 1 {
				pow2Func := fmt.Sprintf("Pow2_%s_F32x%d", ctx.target.Name, ctx.target.LanesFor("float32"))
				wrapFunc := ctx.target.TypeMap[ctx.elemType] + "FromFloat32x" + fmt.Sprintf("%d", ctx.target.LanesFor("float32"))
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(wrapFunc),
				}
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("hwy"),
							Sel: ast.NewIdent(pow2Func),
						},
						Args: call.Args,
					},
				}
				return
			}
			// Fallback/NEON: Pow2 needs a type parameter: hwy.Pow2[hwy.Float16](kInt)
			call.Fun = &ast.IndexExpr{
				X: &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent("Pow2"),
				},
				Index: ast.NewIdent(ctx.elemType),
			}
			return
		case "SignBit":
			// For NEON target with asm types, use asm.SignBitFloat16x8()/asm.SignBitBFloat16x8()
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
				signBitFuncName := "SignBitFloat16x8"
				if isBFloat16Type(ctx.elemType) {
					signBitFuncName = "SignBitBFloat16x8"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(signBitFuncName),
				}
				call.Args = nil
				return
			}
			// For AVX promoted types, use asm.SignBitFloat16x8AVX2() etc.
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				signBitFuncName := "SignBit" + ctx.target.TypeMap[ctx.elemType]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(signBitFuncName),
				}
				call.Args = nil
				return
			}
			// For Fallback, SignBit needs a type parameter: hwy.SignBit[hwy.Float16]()
			call.Fun = &ast.IndexExpr{
				X: &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent("SignBit"),
				},
				Index: ast.NewIdent(ctx.elemType),
			}
			return
		case "Set", "Zero", "Const":
			// For NEON target, use concrete asm types which have in-place methods
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
				if funcName == "Zero" {
					// hwy.Zero[Float16]() -> asm.ZeroFloat16x8()
					zeroFuncName := "ZeroFloat16x8"
					if isBFloat16Type(ctx.elemType) {
						zeroFuncName = "ZeroBFloat16x8"
					}
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("asm"),
						Sel: ast.NewIdent(zeroFuncName),
					}
					return
				}
				if funcName == "Set" || funcName == "Const" {
					// hwy.Set[Float16](val) -> asm.BroadcastFloat16x8(uint16(val))
					// Note: val is already hwy.Float16 which is uint16 underneath
					broadcastFuncName := "BroadcastFloat16x8"
					if isBFloat16Type(ctx.elemType) {
						broadcastFuncName = "BroadcastBFloat16x8"
					}
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("asm"),
						Sel: ast.NewIdent(broadcastFuncName),
					}
					// Convert arg to uint16 - hwy.Float16/BFloat16 are uint16 aliases
					if len(call.Args) > 0 {
						call.Args[0] = &ast.CallExpr{
							Fun:  ast.NewIdent("uint16"),
							Args: []ast.Expr{call.Args[0]},
						}
					}
					return
				}
			}
			// For AVX promoted types, use concrete asm types
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				typeName := ctx.target.TypeMap[ctx.elemType]
				if funcName == "Zero" {
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("asm"),
						Sel: ast.NewIdent("Zero" + typeName),
					}
					return
				}
				if funcName == "Set" || funcName == "Const" {
					call.Fun = &ast.SelectorExpr{
						X:   ast.NewIdent("asm"),
						Sel: ast.NewIdent("Broadcast" + typeName),
					}
					if len(call.Args) > 0 {
						call.Args[0] = &ast.CallExpr{
							Fun:  ast.NewIdent("uint16"),
							Args: []ast.Expr{call.Args[0]},
						}
					}
					return
				}
			}
			// For Fallback targets, keep hwy.Set[T](val), hwy.Zero[T](), and hwy.Const[T](val) as-is
			return
		case "Load":
			// For NEON target, use concrete asm load functions
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && len(call.Args) >= 1 {
				loadFuncName := "LoadFloat16x8Slice"
				if isBFloat16Type(ctx.elemType) {
					loadFuncName = "LoadBFloat16x8Slice"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(loadFuncName),
				}
				// Cast []hwy.Float16/[]hwy.BFloat16 -> []uint16
				call.Args[0] = halfPrecSliceToUint16(call.Args[0])
				return
			}
			// For AVX promoted types, use concrete asm load functions
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 1 {
				loadFuncName := "Load" + ctx.target.TypeMap[ctx.elemType] + "Slice"
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(loadFuncName),
				}
				// Cast []hwy.Float16/[]hwy.BFloat16 -> []uint16
				call.Args[0] = halfPrecSliceToUint16(call.Args[0])
				return
			}
			// For Fallback, keep hwy.Load as-is
			return
		}

		// For operations without F16/BF16 variants, fall through to regular handling
		// but this may cause issues if they try to use method calls
	}

	// For AVX2/AVX512, use wrapper functions for ReduceMax (unsigned only) and GetLane (all types).
	// archsimd doesn't have ReduceMax for unsigned types or a direct GetLane method.
	if ctx.target.Name == "AVX2" || ctx.target.Name == "AVX512" {
		switch funcName {
		case "ReduceMax":
			// hwy.ReduceMax(v) -> hwy.ReduceMax_AVX2_Uint32x8(v) (unsigned only)
			if isUnsignedIntType(ctx.elemType) && len(call.Args) >= 1 {
				vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
				wrapperName := fmt.Sprintf("ReduceMax_%s_%s", ctx.target.Name, vecTypeName)
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent(wrapperName),
				}
				// Args stay as-is
				return
			}
		case "GetLane":
			// hwy.GetLane(v, i) -> hwy.GetLane_AVX2_Float32x8(v, i) etc.
			if len(call.Args) >= 2 {
				vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
				wrapperName := fmt.Sprintf("GetLane_%s_%s", ctx.target.Name, vecTypeName)
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent(wrapperName),
				}
				// Args stay as-is
				return
			}
		}
	}

	// For 64-bit integer types on AVX2, use wrapper functions for Max and Min.
	// AVX2 doesn't have VPMAXSQ/VPMINUQ/VPMAXUQ/VPMINSQ instructions (only AVX-512 has them).
	if is64BitIntType(ctx.elemType) && ctx.target.Name == "AVX2" {
		switch funcName {
		case "Max":
			// hwy.Max(a, b) -> hwy.Max_AVX2_Uint64x4(a, b) or hwy.Max_AVX2_Int64x4(a, b)
			if len(call.Args) >= 2 {
				vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
				wrapperName := fmt.Sprintf("Max_%s_%s", ctx.target.Name, vecTypeName)
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent(wrapperName),
				}
				// Args stay as-is
				return
			}
		case "Min":
			// hwy.Min(a, b) -> hwy.Min_AVX2_Uint64x4(a, b) or hwy.Min_AVX2_Int64x4(a, b)
			if len(call.Args) >= 2 {
				vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
				wrapperName := fmt.Sprintf("Min_%s_%s", ctx.target.Name, vecTypeName)
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent(wrapperName),
				}
				// Args stay as-is
				return
			}
		}
	}

	// For SIMD targets, convert to method calls on archsimd types
	switch funcName {
	case "StoreSlice":
		// For half-precision types with skipHalfPrecNEON (generic hwy.Vec path),
		// keep as hwy.StoreSlice(v, dst) - don't convert to method call
		if isHalfPrecisionType(ctx.elemType) && ctx.skipHalfPrecNEON {
			return
		}
		// hwy.StoreSlice(v, dst) -> v.StoreSlice(dst)
		if len(call.Args) >= 2 {
			call.Fun = &ast.SelectorExpr{
				X:   call.Args[0],
				Sel: ast.NewIdent("StoreSlice"),
			}
			sliceArg := call.Args[1]
			// For NEON and AVX promoted half-precision: cast []hwy.Float16/[]hwy.BFloat16 -> []uint16
			if (isHalfPrecisionType(ctx.elemType) && ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				sliceArg = halfPrecSliceToUint16(sliceArg)
			}
			call.Args = []ast.Expr{sliceArg}
		}

	case "Store":
		// For NEON half-precision: hwy.StoreSlice(v, dst) -> v.StorePtr(unsafe.Pointer(&dst[0]))
		if isHalfPrecisionType(ctx.elemType) && ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
			if len(call.Args) >= 2 {
				vecArg := call.Args[0]
				sliceArg := call.Args[1]
				call.Fun = &ast.SelectorExpr{
					X:   vecArg,
					Sel: ast.NewIdent("StorePtr"),
				}
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{
							&ast.UnaryExpr{
								Op: token.AND,
								X: &ast.IndexExpr{
									X:     sliceArg,
									Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
								},
							},
						},
					},
				}
			}
			return
		}
		// For AVX promoted half-precision: hwy.StoreSlice(v, dst) -> v.StorePtr(unsafe.Pointer(&dst[0]))
		if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
			if len(call.Args) >= 2 {
				vecArg := call.Args[0]
				sliceArg := call.Args[1]
				call.Fun = &ast.SelectorExpr{
					X:   vecArg,
					Sel: ast.NewIdent("StorePtr"),
				}
				// unsafe.Pointer(&dst[0])
				addrExpr := optimizeSliceToPointer(sliceArg)
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{addrExpr},
					},
				}
			}
			return
		}
		// Keep hwy.StoreSlice(v, dst) as-is for Fallback half-precision types
		if isHalfPrecisionType(ctx.elemType) {
			return
		}

		// hwy.StoreSlice(v, dst) -> v.Store((*[8]float32)(unsafe.Pointer(&dst[0])))
		if len(call.Args) >= 2 {
			methodName := "Store"
			lanes := ctx.target.LanesFor(ctx.elemType)

			// unsafe.Pointer(&dst[idx]) - optimized to avoid &dst[i:][0]
			dst := call.Args[1]
			addrExpr := optimizeSliceToPointer(dst)
			ptr := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent("unsafe"),
					Sel: ast.NewIdent("Pointer"),
				},
				Args: []ast.Expr{addrExpr},
			}

			// (*[lanes]T)(ptr)
			cast := &ast.CallExpr{
				Fun: &ast.ParenExpr{
					X: &ast.StarExpr{
						X: &ast.ArrayType{
							Len: &ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(lanes)},
							Elt: ast.NewIdent(ctx.elemType),
						},
					},
				},
				Args: []ast.Expr{ptr},
			}

			call.Fun = &ast.SelectorExpr{
				X:   call.Args[0],
				Sel: ast.NewIdent(methodName),
			}
			call.Args = []ast.Expr{cast}
		}

	case "MaskStore":
		// hwy.MaskStore(mask, v, dst) -> v.MaskStoreSlice(mask, dst)
		if len(call.Args) >= 3 {
			call.Fun = &ast.SelectorExpr{
				X:   call.Args[1],
				Sel: ast.NewIdent("MaskStoreSlice"),
			}
			call.Args = []ast.Expr{call.Args[0], call.Args[2]}
		}

	case "Neg":
		// hwy.Neg(x) -> pkg.BroadcastFloat32x8(0).Sub(x) for SIMD
		// (archsimd/asm types don't have a Neg method, so we use 0 - x)
		if len(call.Args) >= 1 {
			vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
			pkgName := getVecPackageName(ctx.target)
			// Create pkg.BroadcastFloat32x8(0)
			zeroLit := &ast.BasicLit{Kind: token.INT, Value: "0"}
			zeroCall := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + vecTypeName),
				},
				Args: []ast.Expr{zeroLit},
			}
			call.Fun = &ast.SelectorExpr{
				X:   zeroCall,
				Sel: ast.NewIdent("Sub"),
			}
			// Args stays as [x]
		}

	case "Pow2":
		// hwy.Pow2[T](kInt) -> kInt.Pow2Float32() or kInt.Pow2Float64()
		// based on context element type
		if len(call.Args) >= 1 {
			var methodName string
			switch ctx.elemType {
			case "float32":
				methodName = "Pow2Float32"
			case "float64":
				methodName = "Pow2Float64"
			default:
				methodName = "Pow2Float32" // fallback
			}
			call.Fun = &ast.SelectorExpr{
				X:   call.Args[0],
				Sel: ast.NewIdent(methodName),
			}
			call.Args = nil
		}

	case "GetExponent":
		// For Float16/BFloat16 on Fallback, use hwy.GetExponent which has proper handling
		if isHalfPrecisionType(ctx.elemType) && !isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent("GetExponent"),
			}
			return
		}
		if len(call.Args) >= 1 {
			x := call.Args[0]
			// For AVX promoted half-precision, the underlying data is float32
			effectiveElem := ctx.elemType
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				effectiveElem = "float32"
			}
			intVecTypeName := getVectorTypeNameForInt("int32", effectiveElem, ctx.target)
			if effectiveElem == "float64" {
				intVecTypeName = getVectorTypeNameForInt("int64", effectiveElem, ctx.target)
			}
			pkgName := getVecPackageName(ctx.target)

			// 1. x.AsInt32() / x.AsInt64()
			var asIntMethod string
			var shift int
			var mask string
			var bias string

			if effectiveElem == "float32" {
				asIntMethod = "AsInt32x8"
				// Check targets.go OpMap["AsInt32"].Name
				if op, ok := ctx.target.OpMap["AsInt32"]; ok {
					asIntMethod = op.Name
				}
				shift = 23
				mask = "255" // 0xFF
				bias = "127"
			} else {
				asIntMethod = "AsInt64x4"
				if op, ok := ctx.target.OpMap["AsInt64"]; ok {
					asIntMethod = op.Name
				}
				shift = 52
				mask = "2047" // 0x7FF
				bias = "1023"
			}

			// x.AsInt32()
			expr := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   cloneExpr(x),
					Sel: ast.NewIdent(asIntMethod),
				},
			}

			// .ShiftAllRight(shift)
			expr = &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   expr,
					Sel: ast.NewIdent("ShiftAllRight"),
				},
				Args: []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(shift)}},
			}

			// .And(Broadcast(mask))
			broadcastMask := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + intVecTypeName),
				},
				Args: []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: mask}},
			}
			expr = &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   expr,
					Sel: ast.NewIdent("And"),
				},
				Args: []ast.Expr{broadcastMask},
			}

			// .Sub(Broadcast(bias))
			broadcastBias := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + intVecTypeName),
				},
				Args: []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: bias}},
			}
			expr = &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   expr,
					Sel: ast.NewIdent("Sub"),
				},
				Args: []ast.Expr{broadcastBias},
			}

			// NOTE: Don't add .ConvertToFloat() here - let ConvertExponentToFloat handle the
			// int-to-float conversion. This keeps GetExponent returning integers as expected.

			*call = *expr
		}

	case "GetMantissa":
		// For non-AVX-promoted half-precision, use hwy.GetMantissa which has proper handling
		if isHalfPrecisionType(ctx.elemType) && !isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent("GetMantissa"),
			}
			return
		}
		if len(call.Args) >= 1 {
			x := call.Args[0]
			// For AVX promoted half-precision, the underlying data is float32
			effectiveElem := ctx.elemType
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				effectiveElem = "float32"
			}
			intVecTypeName := getVectorTypeNameForInt("int32", effectiveElem, ctx.target)
			if effectiveElem == "float64" {
				intVecTypeName = getVectorTypeNameForInt("int64", effectiveElem, ctx.target)
			}
			pkgName := getVecPackageName(ctx.target)

			var asIntMethod string
			var mask string
			var one string
			var asFloatMethod string

			if effectiveElem == "float32" {
				asIntMethod = "AsInt32x8"
				if op, ok := ctx.target.OpMap["AsInt32"]; ok {
					asIntMethod = op.Name
				}
				mask = "8388607"   // 0x7FFFFF
				one = "1065353216" // 0x3F800000
				asFloatMethod = "AsFloat32x8"
				if op, ok := ctx.target.OpMap["AsFloat32"]; ok {
					asFloatMethod = op.Name
				}
			} else {
				asIntMethod = "AsInt64x4"
				if op, ok := ctx.target.OpMap["AsInt64"]; ok {
					asIntMethod = op.Name
				}
				mask = "4503599627370495"   // 0x000FFFFFFFFFFFFF
				one = "4607182418800017408" // 0x3FF0000000000000
				asFloatMethod = "AsFloat64x4"
				if op, ok := ctx.target.OpMap["AsFloat64"]; ok {
					asFloatMethod = op.Name
				}
			}

			// x.AsInt32()
			expr := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   cloneExpr(x),
					Sel: ast.NewIdent(asIntMethod),
				},
			}

			// .And(Broadcast(mask))
			broadcastMask := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + intVecTypeName),
				},
				Args: []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: mask}},
			}
			expr = &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   expr,
					Sel: ast.NewIdent("And"),
				},
				Args: []ast.Expr{broadcastMask},
			}

			// .Or(Broadcast(one))
			broadcastOne := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + intVecTypeName),
				},
				Args: []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: one}},
			}
			expr = &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   expr,
					Sel: ast.NewIdent("Or"),
				},
				Args: []ast.Expr{broadcastOne},
			}

			// .AsFloat32()
			expr = &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   expr,
					Sel: ast.NewIdent(asFloatMethod),
				},
			}

			// For AVX promoted half-precision, wrap the Float32x8 result back in the asm type
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				wrapFunc := fmt.Sprintf("%sFromFloat32x%d", ctx.target.TypeMap[ctx.elemType], ctx.target.LanesFor("float32"))
				expr = &ast.CallExpr{
					Fun: &ast.SelectorExpr{
						X:   ast.NewIdent("asm"),
						Sel: ast.NewIdent(wrapFunc),
					},
					Args: []ast.Expr{expr},
				}
			}

			*call = *expr
		}

	case "Abs":
		// hwy.Abs(x) -> x.Max(negX) where negX = pkg.Broadcast*(0).Sub(x)
		// archsimd doesn't have Abs method, so we implement |x| = max(x, -x)
		if opInfo.Package == "special" && len(call.Args) >= 1 {
			vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
			pkgName := getVecPackageName(ctx.target)
			x := call.Args[0]
			// Create pkg.Broadcast*(0)
			zeroLit := &ast.BasicLit{Kind: token.INT, Value: "0"}
			zeroCall := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + vecTypeName),
				},
				Args: []ast.Expr{zeroLit},
			}
			// Create pkg.Broadcast*(0).Sub(x) = -x
			negX := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   zeroCall,
					Sel: ast.NewIdent("Sub"),
				},
				Args: []ast.Expr{cloneExpr(x)},
			}
			// Create x.Max(-x)
			call.Fun = &ast.SelectorExpr{
				X:   x,
				Sel: ast.NewIdent("Max"),
			}
			call.Args = []ast.Expr{negX}
		} else {
			// Normal Abs method call
			if len(call.Args) >= 1 {
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent(opInfo.Name),
				}
				call.Args = nil
			}
		}

	case "IsNaN":
		// hwy.IsNaN(x) -> x.Equal(x).Xor(one.Equal(one))
		// NaN != NaN, so x.Equal(x) is false (all 0s) for NaN elements
		// We XOR with all-true mask to invert, giving true for NaN
		if opInfo.Package == "special" && len(call.Args) >= 1 {
			vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
			pkgName := getVecPackageName(ctx.target)
			x := call.Args[0]
			// Create pkg.Broadcast*(1.0)
			oneLit := &ast.BasicLit{Kind: token.FLOAT, Value: "1.0"}
			oneCall := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + vecTypeName),
				},
				Args: []ast.Expr{oneLit},
			}
			// Create one.Equal(one) to get all-true mask
			allTrue := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   oneCall,
					Sel: ast.NewIdent("Equal"),
				},
				Args: []ast.Expr{cloneExpr(oneCall)},
			}
			// Create x.Equal(x)
			xEqX := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   x,
					Sel: ast.NewIdent("Equal"),
				},
				Args: []ast.Expr{cloneExpr(x)},
			}
			// Create x.Equal(x).Xor(allTrue) to invert
			call.Fun = &ast.SelectorExpr{
				X:   xEqX,
				Sel: ast.NewIdent("Xor"),
			}
			call.Args = []ast.Expr{allTrue}
		}

	case "MaskNot":
		// hwy.MaskNot(mask) -> mask.Xor(allTrue)
		// where allTrue = one.Equal(one) (comparing 1.0 == 1.0 gives all-true mask)
		if opInfo.Package == "special" && len(call.Args) >= 1 {
			vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
			pkgName := getVecPackageName(ctx.target)
			mask := call.Args[0]

			// Create pkg.Broadcast*(1.0) for float types or 1 for int types
			var oneLit ast.Expr
			if ctx.elemType == "float32" || ctx.elemType == "float64" {
				oneLit = &ast.BasicLit{Kind: token.FLOAT, Value: "1.0"}
			} else {
				oneLit = &ast.BasicLit{Kind: token.INT, Value: "1"}
			}
			oneCall := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + vecTypeName),
				},
				Args: []ast.Expr{oneLit},
			}
			// Create one.Equal(one) to get all-true mask
			allTrue := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   oneCall,
					Sel: ast.NewIdent("Equal"),
				},
				Args: []ast.Expr{cloneExpr(oneCall)},
			}
			// Create mask.Xor(allTrue) to invert
			call.Fun = &ast.SelectorExpr{
				X:   mask,
				Sel: ast.NewIdent("Xor"),
			}
			call.Args = []ast.Expr{allTrue}
		}

	case "IsInf":
		// hwy.IsInf(x, sign) -> compare with +Inf and/or -Inf
		// sign=0: either +Inf or -Inf, sign=1: +Inf only, sign=-1: -Inf only
		if opInfo.Package == "special" && len(call.Args) >= 2 {
			vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
			pkgName := getVecPackageName(ctx.target)
			x := call.Args[0]
			signArg := call.Args[1]

			// Determine sign value (0, 1, or -1)
			signVal := 0
			if lit, ok := signArg.(*ast.BasicLit); ok && lit.Kind == token.INT {
				if lit.Value == "1" {
					signVal = 1
				} else if lit.Value == "-1" {
					signVal = -1
				}
			} else if unary, ok := signArg.(*ast.UnaryExpr); ok && unary.Op == token.SUB {
				if lit, ok := unary.X.(*ast.BasicLit); ok && lit.Kind == token.INT && lit.Value == "1" {
					signVal = -1
				}
			}

			// Create math.Inf(1) with type conversion for float32
			posInfExpr := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent("stdmath"),
					Sel: ast.NewIdent("Inf"),
				},
				Args: []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: "1"}},
			}
			// For float32, wrap in type conversion
			var posInfArg ast.Expr = posInfExpr
			if ctx.elemType == "float32" {
				posInfArg = &ast.CallExpr{
					Fun:  ast.NewIdent("float32"),
					Args: []ast.Expr{posInfExpr},
				}
			}

			// Create pkg.Broadcast*(posInf) for +Inf
			posInfCall := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + vecTypeName),
				},
				Args: []ast.Expr{posInfArg},
			}

			// Create math.Inf(-1) with type conversion for float32
			negInfExpr := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent("stdmath"),
					Sel: ast.NewIdent("Inf"),
				},
				Args: []ast.Expr{
					&ast.UnaryExpr{
						Op: token.SUB,
						X:  &ast.BasicLit{Kind: token.INT, Value: "1"},
					},
				},
			}
			// For float32, wrap in type conversion
			var negInfArg ast.Expr = negInfExpr
			if ctx.elemType == "float32" {
				negInfArg = &ast.CallExpr{
					Fun:  ast.NewIdent("float32"),
					Args: []ast.Expr{negInfExpr},
				}
			}

			// Create pkg.Broadcast*(negInf) for -Inf
			negInfCall := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + vecTypeName),
				},
				Args: []ast.Expr{negInfArg},
			}

			switch signVal {
			case 1:
				// Check +Inf only: x.Equal(posInf)
				call.Fun = &ast.SelectorExpr{
					X:   x,
					Sel: ast.NewIdent("Equal"),
				}
				call.Args = []ast.Expr{posInfCall}
			case -1:
				// Check -Inf only: x.Equal(negInf)
				call.Fun = &ast.SelectorExpr{
					X:   x,
					Sel: ast.NewIdent("Equal"),
				}
				call.Args = []ast.Expr{negInfCall}
			default:
				// Check either: x.Equal(posInf).Or(x.Equal(negInf))
				posInfMask := &ast.CallExpr{
					Fun: &ast.SelectorExpr{
						X:   cloneExpr(x),
						Sel: ast.NewIdent("Equal"),
					},
					Args: []ast.Expr{posInfCall},
				}
				negInfMask := &ast.CallExpr{
					Fun: &ast.SelectorExpr{
						X:   x,
						Sel: ast.NewIdent("Equal"),
					},
					Args: []ast.Expr{negInfCall},
				}
				call.Fun = &ast.SelectorExpr{
					X:   posInfMask,
					Sel: ast.NewIdent("Or"),
				}
				call.Args = []ast.Expr{negInfMask}
			}
		}

	case "ShiftRight", "ShiftLeft", "ShiftAllRight", "ShiftAllLeft":
		// hwy.ShiftRight(v, shift) -> v.ShiftAllRight(uint64(shift))
		// archsimd's ShiftAllRight/ShiftAllLeft expect uint64, but hwy uses int
		if len(call.Args) >= 2 {
			call.Fun = &ast.SelectorExpr{
				X:   call.Args[0],
				Sel: ast.NewIdent(opInfo.Name),
			}
			shiftArg := call.Args[1]
			// Wrap shift in uint64() cast for archsimd targets
			if ctx.target.VecPackage == "archsimd" {
				shiftArg = &ast.CallExpr{
					Fun:  ast.NewIdent("uint64"),
					Args: []ast.Expr{shiftArg},
				}
			}
			call.Args = []ast.Expr{shiftArg}
		}

	case "And", "Xor":
		// archsimd float types don't have And/Xor methods, only int types do.
		// For float types on archsimd, use hwy wrappers.
		// BUT: if both operands are int32 vectors, use method call since Int32x8 has And/Xor.
		if ctx.target.VecPackage == "archsimd" && (ctx.elemType == "float32" || ctx.elemType == "float64") {
			// Check if both operands are int32 - if so, use method call
			if len(call.Args) >= 2 && isInt32Expr(call.Args[0], ctx) && isInt32Expr(call.Args[1], ctx) {
				// Int32x8 has .And() method - use method call a.And(b)
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent(opInfo.Name),
				}
				call.Args = call.Args[1:]
			} else {
				// hwy.And(a, b) -> hwy.And_AVX2_F32x8(a, b)
				fullName := fmt.Sprintf("%s_%s_%s", opInfo.Name, ctx.target.Name, getShortTypeName(ctx.elemType, ctx.target))
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent(fullName),
				}
				// Keep args as [a, b]
			}
		} else if isHalfPrecisionType(ctx.elemType) {
			// For half-precision contexts, integer operations (like octant masking in sin/cos)
			// may use hwy.Vec[int32] (Fallback/NEON) or archsimd.Int32x8 (AVX promoted).
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 2 &&
				isInt32Expr(call.Args[0], ctx) && isInt32Expr(call.Args[1], ctx) {
				// AVX promoted: int32 variables are archsimd.Int32x8 which has And/Xor methods
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent(opInfo.Name),
				}
				call.Args = call.Args[1:]
			} else {
				// Non-AVX promoted: keep as generic hwy function
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent(opInfo.Name),
				}
			}
			// Keep args as [a, b] (for non-AVX case)
		} else {
			// Integer types or non-archsimd: use method call a.And(b)
			if len(call.Args) >= 2 {
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent(opInfo.Name),
				}
				call.Args = call.Args[1:]
			}
		}

	case "Not":
		// archsimd float types don't have Not method, only int types do.
		// For float types on archsimd, use hwy wrappers.
		if ctx.target.VecPackage == "archsimd" && (ctx.elemType == "float32" || ctx.elemType == "float64") {
			// hwy.Not(a) -> hwy.Not_AVX2_F32x8(a)
			fullName := fmt.Sprintf("%s_%s_%s", opInfo.Name, ctx.target.Name, getShortTypeName(ctx.elemType, ctx.target))
			call.Fun = &ast.SelectorExpr{
				X:   ast.NewIdent("hwy"),
				Sel: ast.NewIdent(fullName),
			}
			// Keep args as [a]
		} else {
			// Integer types or non-archsimd: use method call a.Not()
			if len(call.Args) >= 1 {
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent(opInfo.Name),
				}
				call.Args = nil
			}
		}

	default:
		// Binary operations: hwy.Add(a, b) -> a.Add(b)
		if len(call.Args) >= 2 {
			call.Fun = &ast.SelectorExpr{
				X:   call.Args[0],
				Sel: ast.NewIdent(opInfo.Name),
			}
			call.Args = call.Args[1:]
		} else if len(call.Args) == 1 {
			// Other unary operations
			call.Fun = &ast.SelectorExpr{
				X:   call.Args[0],
				Sel: ast.NewIdent(opInfo.Name),
			}
			call.Args = nil
		}
	}
}

// transformToFunction converts hwy.LoadSlice(src) to archsimd.LoadFloat32x8Slice(src).
func transformToFunction(call *ast.CallExpr, funcName string, opInfo OpInfo, ctx *transformContext) {
	// Handle both SelectorExpr (hwy.Load) and IndexExpr (hwy.Zero[float32])
	var selExpr *ast.SelectorExpr
	var explicitTypeParam string // Explicit type parameter from hwy.Load[uint8] style calls
	switch fun := call.Fun.(type) {
	case *ast.SelectorExpr:
		selExpr = fun
	case *ast.IndexExpr:
		// For generic functions like hwy.Load[uint8]() or hwy.Zero[float32]()
		selExpr = fun.X.(*ast.SelectorExpr)
		// Extract explicit type parameter if it's a concrete type (not a generic type param like T)
		if typeIdent, ok := fun.Index.(*ast.Ident); ok {
			typeName := typeIdent.Name
			// Check if this is a concrete type, not a generic type parameter
			isTypeParam := false
			for _, tp := range ctx.typeParams {
				if typeName == tp.Name {
					isTypeParam = true
					break
				}
			}
			if !isTypeParam {
				// This is an explicit concrete type like uint8, float32, etc.
				explicitTypeParam = typeName
			}
		}
	default:
		return
	}

	if ctx.target.Name == "Fallback" {
		// For fallback, use the appropriate package
		if opInfo.SubPackage != "" {
			// Contrib functions use their subpackage with target suffix
			// e.g., contrib.Sigmoid -> math.BaseSigmoidVec_fallback
			selExpr.X = ast.NewIdent(opInfo.SubPackage)
			fullName := fmt.Sprintf("%s_%s%s", opInfo.Name, strings.ToLower(ctx.target.Name), getHwygenTypeSuffix(ctx.elemType))
			selExpr.Sel.Name = fullName
		} else {
			// Core ops use hwy package
			selExpr.X = ast.NewIdent("hwy")
			// Use opInfo.Name if it differs from the source funcName (e.g., ShiftAllRight -> ShiftRight)
			if opInfo.Name != "" {
				selExpr.Sel.Name = opInfo.Name
			} else {
				selExpr.Sel.Name = funcName
			}
		}
		return
	}

	// For Float16/BFloat16 on SIMD targets, use hwy package functions instead of archsimd calls.
	// archsimd doesn't have native support for half-precision types.
	if isHalfPrecisionType(ctx.elemType) {
		// For NEON target, convert Merge/IfThenElse to asm.IfThenElseFloat16/BFloat16
		if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
			if funcName == "Merge" && len(call.Args) >= 3 {
				asmFunc := "IfThenElseFloat16"
				if isBFloat16Type(ctx.elemType) {
					asmFunc = "IfThenElseBFloat16"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(asmFunc),
				}
				call.Args = []ast.Expr{call.Args[2], call.Args[0], call.Args[1]}
				return
			}
			if funcName == "IfThenElse" && len(call.Args) >= 3 {
				asmFunc := "IfThenElseFloat16"
				if isBFloat16Type(ctx.elemType) {
					asmFunc = "IfThenElseBFloat16"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(asmFunc),
				}
				return
			}
		}
		// For AVX promoted types, convert Merge/IfThenElse to method calls
		if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
			if funcName == "Merge" && len(call.Args) >= 3 {
				// hwy.Merge(yes, no, mask) -> yes.Merge(no, mask)
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent("Merge"),
				}
				call.Args = []ast.Expr{call.Args[1], call.Args[2]}
				return
			}
			if funcName == "IfThenElse" && len(call.Args) >= 3 {
				// hwy.IfThenElse(mask, yes, no) -> yes.Merge(no, mask)
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[1],
					Sel: ast.NewIdent("Merge"),
				}
				call.Args = []ast.Expr{call.Args[2], call.Args[0]}
				return
			}
		}
		// Handle Merge specially - needs argument reordering
		// hwy.Merge(yes, no, mask) -> hwy.IfThenElseF16(mask, yes, no)
		if funcName == "Merge" && len(call.Args) >= 3 {
			suffix := "F16"
			if isBFloat16Type(ctx.elemType) {
				suffix = "BF16"
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "IfThenElse" + suffix
			// Reorder: (yes, no, mask) -> (mask, yes, no)
			call.Args = []ast.Expr{call.Args[2], call.Args[0], call.Args[1]}
			return
		}
		// Handle IfThenElse - same signature as IfThenElseF16, no reordering needed
		// hwy.IfThenElse(mask, yes, no) -> hwy.IfThenElseF16(mask, yes, no)
		if funcName == "IfThenElse" && len(call.Args) >= 3 {
			suffix := "F16"
			if isBFloat16Type(ctx.elemType) {
				suffix = "BF16"
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "IfThenElse" + suffix
			// Args stay in same order
			return
		}

		if f16FuncName := getHalfPrecisionFuncName(funcName, ctx.elemType); f16FuncName != "" {
			// For comparison operations, check if operands are int32 - if so, keep generic hwy function
			if isComparisonOp(funcName) && len(call.Args) >= 2 {
				if isInt32Expr(call.Args[0], ctx) && isInt32Expr(call.Args[1], ctx) {
					if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
						// For AVX promoted, int32 variables are archsimd.Int32x8 - use method calls
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(funcName),
						}
						call.Args = call.Args[1:]
						return
					}
					// Keep as generic hwy.Equal, hwy.Greater, etc. for int32 operands
					selExpr.X = ast.NewIdent("hwy")
					selExpr.Sel.Name = funcName
					return
				}
			}
			// For NEON and AVX promoted targets, use method calls on asm types for arithmetic and comparison operations
			if (ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				switch funcName {
				case "Add", "Sub", "Mul", "Div", "Min", "Max":
					// hwy.AddF16(a, b) -> a.Add(b)
					if len(call.Args) >= 2 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(funcName),
						}
						call.Args = call.Args[1:]
						return
					}
				case "FMA", "MulAdd":
					// hwy.FMAF16(a, b, c) -> a.MulAdd(b, c)
					if len(call.Args) >= 3 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent("MulAdd"),
						}
						call.Args = call.Args[1:]
						return
					}
				case "Neg", "Abs", "Sqrt":
					// hwy.NegF16(a) -> a.Neg()
					if len(call.Args) >= 1 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(funcName),
						}
						call.Args = nil
						return
					}
				case "ReduceSum":
					// hwy.ReduceSumF16(v) -> v.ReduceSum()
					if len(call.Args) >= 1 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent("ReduceSum"),
						}
						call.Args = nil
						return
					}
				case "ReduceMax", "ReduceMin":
					// hwy.ReduceMaxF16(v) -> v.ReduceMax()
					if len(call.Args) >= 1 {
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(funcName),
						}
						call.Args = nil
						return
					}
				case "GreaterThan", "Greater", "LessThan", "Less",
					"GreaterEqual", "GreaterThanOrEqual", "LessEqual", "LessThanOrEqual",
					"Equal", "NotEqual":
					if len(call.Args) >= 2 {
						methodName := funcName
						if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
							// AVX promoted types use archsimd-style short names
							switch funcName {
							case "GreaterThan":
								methodName = "Greater"
							case "LessThan":
								methodName = "Less"
							case "GreaterThanOrEqual":
								methodName = "GreaterEqual"
							case "LessThanOrEqual":
								methodName = "LessEqual"
							}
						} else {
							// NEON uses long names
							switch funcName {
							case "Greater":
								methodName = "GreaterThan"
							case "Less":
								methodName = "LessThan"
							case "GreaterEqual":
								methodName = "GreaterThanOrEqual"
							case "LessEqual":
								methodName = "LessThanOrEqual"
							}
						}
						call.Fun = &ast.SelectorExpr{
							X:   call.Args[0],
							Sel: ast.NewIdent(methodName),
						}
						call.Args = call.Args[1:]
						return
					}
				}
			}
			// For non-NEON targets, transform to hwy.AddF16(a, b), hwy.MulF16(a, b), etc.
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = f16FuncName
			return
		}
		// For Load/Store/Set/Zero on F16/BF16, use asm types for NEON or generic hwy functions
		switch funcName {
		case "Load":
			// hwy.Load = fast pointer-based load
			// For NEON target, use concrete asm load functions with pointer access
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && len(call.Args) >= 1 {
				loadFuncName := "LoadFloat16x8Ptr"
				if isBFloat16Type(ctx.elemType) {
					loadFuncName = "LoadBFloat16x8Ptr"
				}
				// Transform: hwy.Load(slice) -> asm.LoadFloat16x8Ptr(unsafe.Pointer(&slice[0]))
				sliceArg := call.Args[0]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(loadFuncName),
				}
				// Wrap arg with unsafe.Pointer(&slice[0])
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{
							&ast.UnaryExpr{
								Op: token.AND,
								X: &ast.IndexExpr{
									X:     sliceArg,
									Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
								},
							},
						},
					},
				}
				return
			}
			// For AVX promoted types, use pointer-based asm load
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 1 {
				loadFuncName := "Load" + ctx.target.TypeMap[ctx.elemType] + "Ptr"
				sliceArg := call.Args[0]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(loadFuncName),
				}
				// unsafe.Pointer(&slice[0])
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{
							&ast.UnaryExpr{
								Op: token.AND,
								X: &ast.IndexExpr{
									X:     sliceArg,
									Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
								},
							},
						},
					},
				}
				return
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "Load"
			return
		case "LoadSlice":
			// hwy.LoadSlice = safe slice-based load
			// For NEON target, use slice-based asm load functions
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && len(call.Args) >= 1 {
				loadFuncName := "LoadFloat16x8Slice"
				if isBFloat16Type(ctx.elemType) {
					loadFuncName = "LoadBFloat16x8Slice"
				}
				// Transform: hwy.LoadSlice(slice) -> asm.LoadFloat16x8Slice(uint16Slice)
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(loadFuncName),
				}
				// Cast []hwy.Float16/[]hwy.BFloat16 -> []uint16
				call.Args[0] = halfPrecSliceToUint16(call.Args[0])
				return
			}
			// For AVX promoted types, use slice-based asm load
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 1 {
				loadFuncName := "Load" + ctx.target.TypeMap[ctx.elemType] + "Slice"
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(loadFuncName),
				}
				// Cast []hwy.Float16/[]hwy.BFloat16 -> []uint16
				call.Args[0] = halfPrecSliceToUint16(call.Args[0])
				return
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "LoadSlice"
			return
		case "Store":
			// hwy.Store = fast pointer-based store
			// For NEON target, convert to method call with pointer access
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && len(call.Args) >= 2 {
				// hwy.Store(v, dst) -> v.StorePtr(unsafe.Pointer(&dst[0]))
				vecArg := call.Args[0]
				sliceArg := call.Args[1]
				call.Fun = &ast.SelectorExpr{
					X:   vecArg,
					Sel: ast.NewIdent("StorePtr"),
				}
				// Wrap dst with unsafe.Pointer(&dst[0])
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{
							&ast.UnaryExpr{
								Op: token.AND,
								X: &ast.IndexExpr{
									X:     sliceArg,
									Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
								},
							},
						},
					},
				}
				return
			}
			// For AVX promoted types, convert to pointer-based method call: v.StorePtr(ptr)
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 2 {
				vecArg := call.Args[0]
				sliceArg := call.Args[1]
				call.Fun = &ast.SelectorExpr{
					X:   vecArg,
					Sel: ast.NewIdent("StorePtr"),
				}
				// unsafe.Pointer(&dst[0])
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{
							&ast.UnaryExpr{
								Op: token.AND,
								X: &ast.IndexExpr{
									X:     sliceArg,
									Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
								},
							},
						},
					},
				}
				return
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "Store"
			return
		case "StoreSlice":
			// hwy.StoreSlice = safe slice-based store
			// For NEON target, convert to slice-based method call
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && len(call.Args) >= 2 {
				// hwy.StoreSlice(v, dst) -> v.StoreSlice(uint16Slice)
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent("StoreSlice"),
				}
				// Cast []hwy.Float16/[]hwy.BFloat16 -> []uint16
				call.Args = []ast.Expr{halfPrecSliceToUint16(call.Args[1])}
				return
			}
			// For AVX promoted types, convert to slice-based method call: v.StoreSlice(dst)
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 2 {
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent("StoreSlice"),
				}
				// Cast []hwy.Float16/[]hwy.BFloat16 -> []uint16
				call.Args = []ast.Expr{halfPrecSliceToUint16(call.Args[1])}
				return
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "StoreSlice"
			return
		case "Set":
			// For NEON target, use concrete asm broadcast functions
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
				broadcastFuncName := "BroadcastFloat16x8"
				if isBFloat16Type(ctx.elemType) {
					broadcastFuncName = "BroadcastBFloat16x8"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(broadcastFuncName),
				}
				// Convert arg to uint16 - hwy.Float16/BFloat16 are uint16 aliases
				if len(call.Args) > 0 {
					call.Args[0] = &ast.CallExpr{
						Fun:  ast.NewIdent("uint16"),
						Args: []ast.Expr{call.Args[0]},
					}
				}
				return
			}
			// For AVX promoted types, use concrete asm broadcast functions
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				typeName := ctx.target.TypeMap[ctx.elemType]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent("Broadcast" + typeName),
				}
				if len(call.Args) > 0 {
					call.Args[0] = &ast.CallExpr{
						Fun:  ast.NewIdent("uint16"),
						Args: []ast.Expr{call.Args[0]},
					}
				}
				return
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "Set"
			// Note: For half-precision, argument wrapping is handled in
			// transformHalfPrecisionFallback after scalar variables are tracked.
			return
		case "Zero":
			// For NEON target, use concrete asm zero functions
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
				zeroFuncName := "ZeroFloat16x8"
				if isBFloat16Type(ctx.elemType) {
					zeroFuncName = "ZeroBFloat16x8"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(zeroFuncName),
				}
				// Remove type parameter args if present
				call.Args = nil
				return
			}
			// For AVX promoted types, use concrete asm zero functions
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				typeName := ctx.target.TypeMap[ctx.elemType]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent("Zero" + typeName),
				}
				call.Args = nil
				return
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "Zero"
			return
		case "RoundToEven":
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 1 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("RoundToEven"),
					}
					call.Args = nil
					return
				}
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "RoundToEven"
			return
		case "ConvertToInt32":
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 1 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("ConvertToInt32"),
					}
					call.Args = nil
					return
				}
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "ConvertToInt32"
			return
		case "ConvertExponentToFloat":
			// For Float16/BFloat16, use dedicated conversion functions
			if ctx.elemType == "hwy.Float16" {
				selExpr.X = ast.NewIdent("hwy")
				selExpr.Sel.Name = "ConvertToF16"
			} else {
				selExpr.X = ast.NewIdent("hwy")
				selExpr.Sel.Name = "ConvertToBF16"
			}
			// Strip the type parameter if present
			if indexExpr, ok := call.Fun.(*ast.IndexExpr); ok {
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent(selExpr.Sel.Name),
				}
				_ = indexExpr // used to strip type param
			}
			return
		case "Pow2":
			// For AVX promoted types: Pow2 operates on float32 internally
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 1 {
				pow2Func := fmt.Sprintf("Pow2_%s_F32x%d", ctx.target.Name, ctx.target.LanesFor("float32"))
				wrapFunc := ctx.target.TypeMap[ctx.elemType] + "FromFloat32x" + fmt.Sprintf("%d", ctx.target.LanesFor("float32"))
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(wrapFunc),
				}
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("hwy"),
							Sel: ast.NewIdent(pow2Func),
						},
						Args: call.Args,
					},
				}
				return
			}
			// Fallback/NEON: Pow2 needs a type parameter: hwy.Pow2[hwy.Float16](kInt)
			call.Fun = &ast.IndexExpr{
				X: &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent("Pow2"),
				},
				Index: ast.NewIdent(ctx.elemType),
			}
			return
		case "Const":
			// For NEON target with asm types, convert to asm.BroadcastFloat16x8/BFloat16x8
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
				broadcastFuncName := "BroadcastFloat16x8"
				if isBFloat16Type(ctx.elemType) {
					broadcastFuncName = "BroadcastBFloat16x8"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(broadcastFuncName),
				}
				// Convert arg: float64 literal -> uint16(hwy.Float32ToFloat16(float32(val)))
				// or uint16(hwy.Float32ToBFloat16(float32(val)))
				if len(call.Args) > 0 {
					convFunc := "Float32ToFloat16"
					if isBFloat16Type(ctx.elemType) {
						convFunc = "Float32ToBFloat16"
					}
					call.Args[0] = &ast.CallExpr{
						Fun: ast.NewIdent("uint16"),
						Args: []ast.Expr{
							&ast.CallExpr{
								Fun: &ast.SelectorExpr{
									X:   ast.NewIdent("hwy"),
									Sel: ast.NewIdent(convFunc),
								},
								Args: []ast.Expr{
									&ast.CallExpr{
										Fun:  ast.NewIdent("float32"),
										Args: []ast.Expr{call.Args[0]},
									},
								},
							},
						},
					}
				}
				return
			}
			// For AVX promoted types, convert to asm.BroadcastFloat16x8AVX2(...) etc.
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				typeName := ctx.target.TypeMap[ctx.elemType]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent("Broadcast" + typeName),
				}
				if len(call.Args) > 0 {
					convFunc := "Float32ToFloat16"
					if isBFloat16Type(ctx.elemType) {
						convFunc = "Float32ToBFloat16"
					}
					call.Args[0] = &ast.CallExpr{
						Fun: ast.NewIdent("uint16"),
						Args: []ast.Expr{
							&ast.CallExpr{
								Fun: &ast.SelectorExpr{
									X:   ast.NewIdent("hwy"),
									Sel: ast.NewIdent(convFunc),
								},
								Args: []ast.Expr{
									&ast.CallExpr{
										Fun:  ast.NewIdent("float32"),
										Args: []ast.Expr{call.Args[0]},
									},
								},
							},
						},
					}
				}
				return
			}
			// Keep hwy.Const[T] with type parameter for Fallback / skip targets
			return
		case "Not":
			// For NEON/AVX promoted targets with half-precision types, convert to method call: hwy.Not(v) -> v.Not()
			if (ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && isHalfPrecisionType(ctx.elemType)) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 1 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("Not"),
					}
					call.Args = nil
					return
				}
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = funcName
			return
		case "Xor":
			// For NEON/AVX promoted targets with half-precision types, convert to method call: hwy.Xor(a, b) -> a.Xor(b)
			if (ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && isHalfPrecisionType(ctx.elemType)) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 2 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("Xor"),
					}
					call.Args = call.Args[1:]
					return
				}
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = funcName
			return
		case "And":
			// For NEON/AVX promoted targets with half-precision types, convert to method call: hwy.And(a, b) -> a.And(b)
			if (ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && isHalfPrecisionType(ctx.elemType)) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 2 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("And"),
					}
					call.Args = call.Args[1:]
					return
				}
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = funcName
			return
		case "Or", "AndNot":
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 2 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent(funcName),
					}
					call.Args = call.Args[1:]
					return
				}
			}
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = funcName
			return
		case "NotEqual":
			// For AVX promoted types, use method call
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				if len(call.Args) >= 2 {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent("NotEqual"),
					}
					call.Args = call.Args[1:]
					return
				}
			}
			// For Fallback, keep as hwy.NotEqual(a, b)
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "NotEqual"
			return
		case "Pow":
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 2 {
				lanes := ctx.target.LanesFor("float32")
				asmType := ctx.target.TypeMap[ctx.elemType]
				wrapFunc := fmt.Sprintf("%sFromFloat32x%d", asmType, lanes)
				loadFunc := fmt.Sprintf("LoadFloat32x%dSlice", lanes)
				asF32Method := fmt.Sprintf("AsFloat32x%d", lanes)
				vecPkg := getVecPackageName(ctx.target)
				lanesStr := strconv.Itoa(lanes)
				call.Fun = genPowIIFE(asmType, wrapFunc, loadFunc, asF32Method, vecPkg, lanesStr, call.Args[0], call.Args[1])
				call.Args = nil
				return
			}
			// hwy.Pow on hwy.Vec[Float16/BFloat16] doesn't have a method form,
			// keep as hwy.Pow(base, exp)
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "Pow"
			return
		case "SignBit":
			// For NEON target with asm types, use asm.SignBitFloat16x8()/asm.SignBitBFloat16x8()
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
				signBitFuncName := "SignBitFloat16x8"
				if isBFloat16Type(ctx.elemType) {
					signBitFuncName = "SignBitBFloat16x8"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(signBitFuncName),
				}
				call.Args = nil
				return
			}
			// For AVX promoted types, use asm.SignBitFloat16x8AVX2() etc.
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
				signBitFuncName := "SignBit" + ctx.target.TypeMap[ctx.elemType]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(signBitFuncName),
				}
				call.Args = nil
				return
			}
			// For Fallback, SignBit needs a type parameter
			call.Fun = &ast.IndexExpr{
				X: &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent("SignBit"),
				},
				Index: ast.NewIdent(ctx.elemType),
			}
			return
		case "InterleaveLower", "InterleaveUpper":
			// For NEON or AVX promoted with half-precision types: hwy.InterleaveLower(a, b) -> a.InterleaveLower(b)
			if len(call.Args) >= 2 {
				if (ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && isHalfPrecisionType(ctx.elemType)) || isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
					call.Fun = &ast.SelectorExpr{
						X:   call.Args[0],
						Sel: ast.NewIdent(funcName),
					}
					call.Args = call.Args[1:]
					return
				}
			}
			// For Fallback, use generic hwy.InterleaveLower/InterleaveUpper
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = funcName
			return
		case "Load4":
			// For NEON target, use asm.Load4Float16x8/Load4BFloat16x8
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && len(call.Args) >= 1 {
				load4Func := "Load4Float16x8"
				if isBFloat16Type(ctx.elemType) {
					load4Func = "Load4BFloat16x8"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(load4Func),
				}
				sliceArg := call.Args[0]
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{
							&ast.UnaryExpr{
								Op: token.AND,
								X: &ast.IndexExpr{
									X:     sliceArg,
									Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
								},
							},
						},
					},
				}
				return
			}
			// For AVX promoted types, use asm.Load4Float16x8AVX2Slice etc.
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 1 {
				asmType := ctx.target.TypeMap[ctx.elemType]
				load4Func := fmt.Sprintf("Load4%sSlice", asmType)
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(load4Func),
				}
				call.Args = []ast.Expr{halfPrecSliceToUint16(call.Args[0])}
				return
			}
			// Fallback: use generic hwy.Load4 with type param
			call.Fun = &ast.IndexExpr{
				X: &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent("Load4"),
				},
				Index: ast.NewIdent(ctx.elemType),
			}
			return
		case "MaskAnd", "MaskOr":
			// For AVX promoted types, convert to method call: a.And(b)
			if isAVXPromotedHalfPrec(ctx.target, ctx.elemType) && len(call.Args) >= 2 {
				methodName := "And"
				if funcName == "MaskOr" {
					methodName = "Or"
				}
				call.Fun = &ast.SelectorExpr{
					X:   call.Args[0],
					Sel: ast.NewIdent(methodName),
				}
				call.Args = call.Args[1:]
				return
			}
			// For other targets, keep as hwy.MaskAnd/MaskOr
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = funcName
			return
		}
		// For other operations without F16/BF16 variants, fall through
	}

	// For SIMD targets, transform to package calls (archsimd for AVX, asm for NEON)
	var fullName string
	// Determine element type: explicit type param > function's default
	effectiveElemType := ctx.elemType
	if explicitTypeParam != "" {
		effectiveElemType = explicitTypeParam
	}
	vecTypeName := getVectorTypeName(effectiveElemType, ctx.target)
	pkgName := getVecPackageName(ctx.target)

	// Check if this op should be redirected to hwy wrappers (archsimd doesn't have it)
	if opInfo.Package == "hwy" && opInfo.SubPackage == "" {
		// Use hwy wrapper instead of archsimd
		// Try to infer lanes and element type from any argument (for operations like TableLookupBytes)
		shortTypeName := getShortTypeName(effectiveElemType, ctx.target)
		inferredLanes := 0
		inferredElemType := effectiveElemType
		for _, arg := range call.Args {
			if argIdent, ok := arg.(*ast.Ident); ok {
				if lanes, found := ctx.varVecLanes[argIdent.Name]; found {
					inferredLanes = lanes
					// Also check if we have an element type for this variable
					if elemType, hasType := ctx.varVecElemType[argIdent.Name]; hasType {
						inferredElemType = elemType
					}
					break // Use the first known lanes
				}
			}
		}
		if inferredLanes > 0 {
			shortTypeName = getShortTypeNameForLanes(inferredElemType, inferredLanes)
		} else if ctx.inferredFuncLanes > 0 {
			// Fall back to function-level inferred lanes (from Load calls)
			// Cap at target's max lanes to avoid generating invalid types (e.g., Uint8x32 on NEON)
			useLanes := ctx.inferredFuncLanes
			targetLanes := ctx.target.LanesFor(effectiveElemType)
			if useLanes > targetLanes {
				useLanes = targetLanes
			}
			shortTypeName = getShortTypeNameForLanes(effectiveElemType, useLanes)
		}
		fullName = fmt.Sprintf("%s_%s_%s", opInfo.Name, ctx.target.Name, shortTypeName)
		selExpr.X = ast.NewIdent("hwy")
		selExpr.Sel.Name = fullName
		// Strip the IndexExpr if call.Fun was hwy.Func[T]() - the wrapper doesn't use type params
		call.Fun = selExpr
		return
	}

	switch funcName {
	case "LoadSlice":
		// Check if we can determine the slice size from the argument
		// For example, hwy.LoadSlice(data[:16]) with uint8 should use Uint8x16, not Uint8x32
		loadVecTypeName := vecTypeName
		if len(call.Args) > 0 {
			sliceBytes := getSliceSize(call.Args[0])
			elemSize := elemTypeSize(effectiveElemType)
			targetLanes := ctx.target.LanesFor(effectiveElemType)
			if sliceBytes > 0 && elemSize > 0 {
				detectedLanes := sliceBytes / elemSize
				// Only use smaller type if detected lanes is less than target default
				// and is a valid vector size (power of 2, typically 2, 4, 8, 16, 32, 64)
				if detectedLanes < targetLanes && detectedLanes > 0 {
					loadVecTypeName = getVectorTypeNameForLanes(effectiveElemType, detectedLanes)
				}
			} else if ctx.inferredFuncLanes > 0 && ctx.inferredFuncLanes < targetLanes {
				// No explicit size, but we have inferred lanes from earlier in the function
				loadVecTypeName = getVectorTypeNameForLanes(effectiveElemType, ctx.inferredFuncLanes)
			}
		}
		fullName = fmt.Sprintf("Load%sSlice", loadVecTypeName)
		selExpr.X = ast.NewIdent(pkgName)
	case "Load":
		// hwy.LoadSlice(src) -> pointer-based load for performance (no bounds check)
		// For half-precision types on SIMD targets
		if isHalfPrecisionType(effectiveElemType) {
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && len(call.Args) >= 1 {
				// NEON: use asm.LoadFloat16x8Ptr(unsafe.Pointer(&slice[0]))
				loadFuncName := "LoadFloat16x8Ptr"
				if isBFloat16Type(effectiveElemType) {
					loadFuncName = "LoadBFloat16x8Ptr"
				}
				sliceArg := call.Args[0]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(loadFuncName),
				}
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{
							&ast.UnaryExpr{
								Op: token.AND,
								X: &ast.IndexExpr{
									X:     sliceArg,
									Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
								},
							},
						},
					},
				}
				return
			}
			// AVX2/AVX512: use asm load functions with pointer-based access (no bounds check)
			if isAVXPromotedHalfPrec(ctx.target, effectiveElemType) && len(call.Args) >= 1 {
				loadFuncName := "Load" + ctx.target.TypeMap[effectiveElemType] + "Ptr"
				sliceArg := call.Args[0]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(loadFuncName),
				}
				// unsafe.Pointer(&slice[0])
				addrExpr := optimizeSliceToPointer(sliceArg)
				call.Args = []ast.Expr{
					&ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   ast.NewIdent("unsafe"),
							Sel: ast.NewIdent("Pointer"),
						},
						Args: []ast.Expr{addrExpr},
					},
				}
				return
			}
			// Fallback: keep hwy.LoadSlice() for half-precision
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "Load"
			return
		}

		if ctx.target.Name == "AVX2" || ctx.target.Name == "AVX512" {
			// For AVX targets, use unsafe pointer cast to avoid bounds checks
			// archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[idx])))
			lanes := ctx.target.LanesFor(effectiveElemType)
			fullName = fmt.Sprintf("Load%s", vecTypeName)
			selExpr.X = ast.NewIdent(pkgName)

			// Transform argument to pointer cast
			if len(call.Args) > 0 {
				src := call.Args[0]
				// unsafe.Pointer(&src[idx]) - optimized to avoid &src[i:][0]
				addrExpr := optimizeSliceToPointer(src)
				ptr := &ast.CallExpr{
					Fun: &ast.SelectorExpr{
						X:   ast.NewIdent("unsafe"),
						Sel: ast.NewIdent("Pointer"),
					},
					Args: []ast.Expr{addrExpr},
				}
				// (*[lanes]T)(ptr)
				cast := &ast.CallExpr{
					Fun: &ast.ParenExpr{
						X: &ast.StarExpr{
							X: &ast.ArrayType{
								Len: &ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(lanes)},
								Elt: ast.NewIdent(effectiveElemType),
							},
						},
					},
					Args: []ast.Expr{ptr},
				}
				call.Args[0] = cast
			}
		} else if ctx.target.Name == "NEON" {
			// For NEON, use asm pointer cast to avoid bounds checks
			// asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&src[idx])))
			lanes := ctx.target.LanesFor(effectiveElemType)
			fullName = fmt.Sprintf("Load%s", vecTypeName)
			selExpr.X = ast.NewIdent(pkgName)

			// Transform argument to pointer cast
			if len(call.Args) > 0 {
				src := call.Args[0]
				addrExpr := optimizeSliceToPointer(src)
				ptr := &ast.CallExpr{
					Fun: &ast.SelectorExpr{
						X:   ast.NewIdent("unsafe"),
						Sel: ast.NewIdent("Pointer"),
					},
					Args: []ast.Expr{addrExpr},
				}
				// (*[lanes]T)(ptr)
				cast := &ast.CallExpr{
					Fun: &ast.ParenExpr{
						X: &ast.StarExpr{
							X: &ast.ArrayType{
								Len: &ast.BasicLit{Kind: token.INT, Value: strconv.Itoa(lanes)},
								Elt: ast.NewIdent(effectiveElemType),
							},
						},
					},
					Args: []ast.Expr{ptr},
				}
				call.Args[0] = cast
			}
		} else {
			// Fallback: keep generic hwy.Load
			selExpr.X = ast.NewIdent("hwy")
			selExpr.Sel.Name = "Load"
			return
		}
	case "Store":
		// For Fallback (IsMethod: false), use generic hwy.Store
		// NEON/AVX use IsMethod: true, handled in transformToMethod
		selExpr.X = ast.NewIdent("hwy")
		selExpr.Sel.Name = "Store"
		return
	case "Load4":
		// For Vec types (Float16/BFloat16), use hwy wrapper or asm function
		if strings.HasPrefix(vecTypeName, "Vec") || strings.HasPrefix(vecTypeName, "hwy.Vec") {
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON && isHalfPrecisionType(effectiveElemType) {
				// NEON half-precision: use asm.Load4Float16x8/Load4BFloat16x8
				load4Func := "Load4Float16x8"
				if isBFloat16Type(effectiveElemType) {
					load4Func = "Load4BFloat16x8"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(load4Func),
				}
				// Transform arg: slice -> unsafe.Pointer(&slice[0])
				if len(call.Args) > 0 {
					sliceArg := call.Args[0]
					call.Args = []ast.Expr{
						&ast.CallExpr{
							Fun: &ast.SelectorExpr{
								X:   ast.NewIdent("unsafe"),
								Sel: ast.NewIdent("Pointer"),
							},
							Args: []ast.Expr{
								&ast.UnaryExpr{
									Op: token.AND,
									X: &ast.IndexExpr{
										X:     sliceArg,
										Index: &ast.BasicLit{Kind: token.INT, Value: "0"},
									},
								},
							},
						},
					}
				}
				return
			}
			if isAVXPromotedHalfPrec(ctx.target, effectiveElemType) {
				// AVX promoted half-precision: use asm.Load4Float16x8AVX2Slice etc.
				asmType := ctx.target.TypeMap[effectiveElemType]
				load4Func := fmt.Sprintf("Load4%sSlice", asmType)
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(load4Func),
				}
				// Transform arg: []hwy.Float16 -> []uint16
				if len(call.Args) > 0 {
					call.Args = []ast.Expr{halfPrecSliceToUint16(call.Args[0])}
				}
				return
			}
			fullName = fmt.Sprintf("Load4_%s_Vec", ctx.target.Name)
			selExpr.X = ast.NewIdent("hwy")
		} else {
			// For NEON: asm.Load4Float32x4Slice (single ld1 instruction)
			// For AVX2/512/Fallback: handled by hwy wrapper at line 2094-2100
			fullName = fmt.Sprintf("Load4%sSlice", vecTypeName)
			selExpr.X = ast.NewIdent(pkgName)
		}
	case "Set", "Const":
		// Both Set and Const broadcast a scalar value to all lanes
		fullName = fmt.Sprintf("Broadcast%s", vecTypeName)
		selExpr.X = ast.NewIdent(pkgName)
	case "Zero":
		if opInfo.Package == "special" {
			// archsimd doesn't have Zero*, use Broadcast with 0
			fullName = fmt.Sprintf("Broadcast%s", vecTypeName)
			selExpr.X = ast.NewIdent(pkgName)
			// Add 0 as argument
			call.Args = []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: "0"}}
		} else {
			fullName = fmt.Sprintf("Zero%s", vecTypeName)
			selExpr.X = ast.NewIdent(pkgName)
		}
	case "SlideUpLanes":
		// For NEON: hwy.SlideUpLanes(v, offset) -> asm.SlideUpLanesFloat32x4(v, offset)
		// For AVX2/AVX512: hwy.SlideUpLanes(v, offset) -> hwy.SlideUpLanes_AVX2_F32x8(v, offset)
		if ctx.target.Name == "AVX2" || ctx.target.Name == "AVX512" {
			shortTypeName := getShortTypeName(ctx.elemType, ctx.target)
			fullName = fmt.Sprintf("SlideUpLanes_%s_%s", ctx.target.Name, shortTypeName)
			selExpr.X = ast.NewIdent("hwy")
		} else {
			fullName = fmt.Sprintf("SlideUpLanes%s", vecTypeName)
			selExpr.X = ast.NewIdent(pkgName)
		}
	case "SlideDownLanes":
		// For NEON: hwy.SlideDownLanes(v, offset) -> asm.SlideDownLanesFloat32x4(v, offset)
		// For AVX2/AVX512: hwy.SlideDownLanes(v, offset) -> hwy.SlideDownLanes_AVX2_F32x8(v, offset)
		if ctx.target.Name == "AVX2" || ctx.target.Name == "AVX512" {
			shortTypeName := getShortTypeName(ctx.elemType, ctx.target)
			fullName = fmt.Sprintf("SlideDownLanes_%s_%s", ctx.target.Name, shortTypeName)
			selExpr.X = ast.NewIdent("hwy")
		} else {
			fullName = fmt.Sprintf("SlideDownLanes%s", vecTypeName)
			selExpr.X = ast.NewIdent(pkgName)
		}
	case "InsertLane":
		// hwy.InsertLane(v, idx, val) -> asm.InsertLaneFloat32x4(v, idx, val)
		fullName = fmt.Sprintf("InsertLane%s", vecTypeName)
		selExpr.X = ast.NewIdent(pkgName)
	case "MaskLoad":
		fullName = fmt.Sprintf("MaskLoad%sSlice", vecTypeName)
		selExpr.X = ast.NewIdent(pkgName)
	case "Compress":
		// Use hwy wrapper if configured
		if opInfo.Package == "hwy" {
			fullName = fmt.Sprintf("%s_%s_%s", opInfo.Name, ctx.target.Name, getShortTypeName(ctx.elemType, ctx.target))
			selExpr.X = ast.NewIdent("hwy")
		} else {
			// Compress returns (Vec, int). Maps to CompressKeysF32x4, etc.
			switch ctx.elemType {
			case "float32":
				fullName = "CompressKeysF32x4"
			case "float64":
				fullName = "CompressKeysF64x2"
			case "int32":
				fullName = "CompressKeysI32x4"
			case "int64":
				fullName = "CompressKeysI64x2"
			case "uint32":
				fullName = "CompressKeysU32x4"
			case "uint64":
				fullName = "CompressKeysU64x2"
			default:
				fullName = "CompressKeysF32x4"
			}
			selExpr.X = ast.NewIdent(pkgName)
		}
	case "CompressStore":
		// Use hwy wrapper if configured
		if opInfo.Package == "hwy" {
			fullName = fmt.Sprintf("%s_%s_%s", opInfo.Name, ctx.target.Name, getShortTypeName(ctx.elemType, ctx.target))
			selExpr.X = ast.NewIdent("hwy")
		} else {
			// CompressStore has type-specific versions: CompressStore (float32), CompressStoreFloat64, etc.
			switch ctx.elemType {
			case "float32":
				fullName = "CompressStore"
			case "float64":
				fullName = "CompressStoreFloat64"
			case "int32":
				fullName = "CompressStoreInt32"
			case "int64":
				fullName = "CompressStoreInt64"
			case "uint32":
				fullName = "CompressStoreUint32"
			case "uint64":
				fullName = "CompressStoreUint64"
			default:
				fullName = "CompressStore"
			}
			selExpr.X = ast.NewIdent(pkgName)
		}
	case "FirstN":
		// Use hwy wrapper if configured
		if opInfo.Package == "hwy" {
			fullName = fmt.Sprintf("%s_%s_%s", opInfo.Name, ctx.target.Name, getShortTypeName(ctx.elemType, ctx.target))
			selExpr.X = ast.NewIdent("hwy")
		} else {
			// FirstN returns a mask type: Int32x4 for 4-lane, Int64x2 for 2-lane
			switch ctx.elemType {
			case "float32":
				fullName = "FirstN"
			case "float64":
				fullName = "FirstNFloat64"
			case "int32", "uint32":
				fullName = "FirstN" // Int32x4 mask for 32-bit types
			case "int64", "uint64":
				fullName = "FirstNInt64" // Int64x2 mask for 64-bit types
			default:
				fullName = "FirstN"
			}
			selExpr.X = ast.NewIdent(pkgName)
		}
	case "IfThenElse":
		// Use hwy wrapper if configured
		if opInfo.Package == "hwy" {
			fullName = fmt.Sprintf("%s_%s_%s", opInfo.Name, ctx.target.Name, getShortTypeName(ctx.elemType, ctx.target))
			selExpr.X = ast.NewIdent("hwy")
		} else {
			// IfThenElse has type-specific versions for NEON
			switch ctx.elemType {
			case "float32":
				fullName = "IfThenElse"
			case "float64":
				fullName = "IfThenElseFloat64"
			case "int32":
				fullName = "IfThenElseInt32"
			case "int64":
				fullName = "IfThenElseInt64"
			default:
				fullName = "IfThenElse"
			}
			selExpr.X = ast.NewIdent(pkgName)
		}
	case "AllTrue":
		// AllTrue has type-specific versions for inlining:
		// AllTrueVal for Int32x4 masks, AllTrueValFloat64 for Int64x2 masks
		switch ctx.elemType {
		case "float32", "int32":
			fullName = "AllTrueVal"
		case "float64", "int64":
			fullName = "AllTrueValFloat64"
		case "uint32":
			fullName = "AllTrueValUint32"
		case "uint64":
			fullName = "AllTrueValUint64"
		default:
			fullName = "AllTrueVal"
		}
		selExpr.X = ast.NewIdent(pkgName)
	case "AllFalse":
		// AllFalse has type-specific versions for inlining:
		// AllFalseVal for Int32x4 masks, AllFalseValFloat64 for Int64x2 masks
		switch ctx.elemType {
		case "float32", "int32":
			fullName = "AllFalseVal"
		case "float64", "int64":
			fullName = "AllFalseValFloat64"
		case "uint32":
			fullName = "AllFalseValUint32"
		case "uint64":
			fullName = "AllFalseValUint64"
		default:
			fullName = "AllFalseVal"
		}
		selExpr.X = ast.NewIdent(pkgName)
	case "SignBit":
		// SignBit has type-specific versions for NEON: SignBitFloat32x4, SignBitFloat64x2
		// For AVX2/AVX512, archsimd.SignBit() is generic
		if ctx.target.Name == "NEON" {
			switch ctx.elemType {
			case "float32":
				fullName = "SignBitFloat32x4"
			case "float64":
				fullName = "SignBitFloat64x2"
			default:
				fullName = "SignBitFloat32x4"
			}
		} else {
			fullName = "SignBit"
		}
		selExpr.X = ast.NewIdent(pkgName)
	case "Iota":
		// Iota needs target-specific handling since archsimd doesn't have a generic Iota.
		// NEON: type-specific asm functions (IotaFloat32x4, IotaFloat64x2, etc.)
		// AVX2/AVX512: hwy wrapper functions (Iota_AVX2_F32x8, Iota_AVX512_F32x16, etc.)
		// Float16/BFloat16 on any target: hwy.Iota[T]() generic function
		if isHalfPrecisionType(effectiveElemType) {
			if ctx.target.Name == "NEON" && !ctx.skipHalfPrecNEON {
				// NEON: use asm.IotaFloat16x8() / asm.IotaBFloat16x8()
				iotaFunc := "IotaFloat16x8"
				if isBFloat16Type(effectiveElemType) {
					iotaFunc = "IotaBFloat16x8"
				}
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(iotaFunc),
				}
				call.Args = nil
				return
			}
			// AVX2/AVX512: use asm.IotaFloat16x8AVX2() etc.
			if isAVXPromotedHalfPrec(ctx.target, effectiveElemType) {
				iotaFunc := "Iota" + ctx.target.TypeMap[effectiveElemType]
				call.Fun = &ast.SelectorExpr{
					X:   ast.NewIdent("asm"),
					Sel: ast.NewIdent(iotaFunc),
				}
				call.Args = nil
				return
			}
			// Fallback: use hwy.Iota[T]() generic function
			call.Fun = &ast.IndexExpr{
				X: &ast.SelectorExpr{
					X:   ast.NewIdent("hwy"),
					Sel: ast.NewIdent("Iota"),
				},
				Index: ast.NewIdent(ctx.elemType),
			}
			return
		}
		if ctx.target.Name == "NEON" {
			switch ctx.elemType {
			case "float32":
				fullName = "IotaFloat32x4"
			case "float64":
				fullName = "IotaFloat64x2"
			case "uint32":
				fullName = "IotaUint32x4"
			case "uint64":
				fullName = "IotaUint64x2"
			default:
				fullName = "Iota"
			}
			selExpr.X = ast.NewIdent(pkgName)
		} else if ctx.target.VecPackage == "archsimd" {
			// AVX2/AVX512: use hwy.Iota_{target}_{shortType}()
			shortTypeName := getShortTypeName(effectiveElemType, ctx.target)
			fullName = fmt.Sprintf("Iota_%s_%s", ctx.target.Name, shortTypeName)
			selExpr.X = ast.NewIdent("hwy")
		} else {
			fullName = opInfo.Name
			selExpr.X = ast.NewIdent(pkgName)
		}
	case "MaskNot":
		// MaskNot(mask) -> mask.Xor(allTrue)
		// where allTrue = one.Equal(one) (comparing 1.0 == 1.0 gives all-true mask)
		if opInfo.Package == "special" && len(call.Args) >= 1 {
			vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)
			mask := call.Args[0]

			// Create pkg.Broadcast*(1.0) for float types or 1 for int types
			var oneLit ast.Expr
			if ctx.elemType == "float32" || ctx.elemType == "float64" {
				oneLit = &ast.BasicLit{Kind: token.FLOAT, Value: "1.0"}
			} else {
				oneLit = &ast.BasicLit{Kind: token.INT, Value: "1"}
			}
			oneCall := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   ast.NewIdent(pkgName),
					Sel: ast.NewIdent("Broadcast" + vecTypeName),
				},
				Args: []ast.Expr{oneLit},
			}
			// Create one.Equal(one) to get all-true mask
			allTrue := &ast.CallExpr{
				Fun: &ast.SelectorExpr{
					X:   oneCall,
					Sel: ast.NewIdent("Equal"),
				},
				Args: []ast.Expr{cloneExpr(oneCall)},
			}
			// Create mask.Xor(allTrue) to invert
			call.Fun = &ast.SelectorExpr{
				X:   mask,
				Sel: ast.NewIdent("Xor"),
			}
			call.Args = []ast.Expr{allTrue}
		}
		return // Don't set fullName, we've already transformed the call
	case "ShiftRight", "ShiftLeft", "ShiftAllRight", "ShiftAllLeft":
		// archsimd's ShiftAllRight/ShiftAllLeft expect uint64, but hwy uses int.
		// After function-to-method transformation, the shift is the last arg.
		// Wrap it in a uint64() cast for archsimd targets.
		if ctx.target.VecPackage == "archsimd" && len(call.Args) >= 1 {
			lastIdx := len(call.Args) - 1
			call.Args[lastIdx] = &ast.CallExpr{
				Fun:  ast.NewIdent("uint64"),
				Args: []ast.Expr{call.Args[lastIdx]},
			}
		}
		fullName = opInfo.Name
		selExpr.X = ast.NewIdent(pkgName)
	case "ConvertExponentToFloat":
		// Convert Vec[int32] to Vec[T] for the target float type
		// For native float types, transform to e.ConvertToFloat32() method call
		if len(call.Args) >= 1 {
			var methodName string
			switch ctx.elemType {
			case "float32":
				methodName = "ConvertToFloat32"
			case "float64":
				methodName = "ConvertToFloat64"
			default:
				// Half-precision handled earlier in the isHalfPrecisionType block
				methodName = "ConvertToFloat32"
			}
			// Transform hwy.ConvertExponentToFloat[T](e) to e.ConvertToFloat32()
			call.Fun = &ast.SelectorExpr{
				X:   call.Args[0],
				Sel: ast.NewIdent(methodName),
			}
			call.Args = nil
		}
		return
	default:
		// For contrib functions (SubPackage), use hwygen's naming convention:
		// lowercase target, type suffix only for non-default types
		// e.g., math.BaseExpVec_avx2, math.BaseExpVec_avx2_Float64
		if opInfo.SubPackage != "" {
			fullName = fmt.Sprintf("%s_%s%s", opInfo.Name, strings.ToLower(ctx.target.Name), getHwygenTypeSuffix(ctx.elemType))
			selExpr.X = ast.NewIdent(opInfo.SubPackage) // math, vec, matvec, algo
		} else if opInfo.Package == "hwy" {
			// Core ops from hwy package (e.g., hwy.Sqrt_AVX2_F32x8)
			fullName = fmt.Sprintf("%s_%s_%s", opInfo.Name, ctx.target.Name, getShortTypeName(ctx.elemType, ctx.target))
			selExpr.X = ast.NewIdent("hwy")
		} else {
			fullName = opInfo.Name
			selExpr.X = ast.NewIdent(pkgName)
		}
	}

	selExpr.Sel.Name = fullName

	// If call.Fun is an IndexExpr (from explicit type param like hwy.Load[uint8]),
	// strip the IndexExpr since asm/archsimd package functions don't use type params
	if _, ok := call.Fun.(*ast.IndexExpr); ok {
		call.Fun = selExpr
	}
}

// getVecPackageName returns the package name for vector types based on target.
// Returns "archsimd" for AVX targets, "asm" for NEON.
func getVecPackageName(target Target) string {
	switch target.VecPackage {
	case "archsimd":
		return "archsimd"
	case "asm":
		return "asm"
	default:
		return "archsimd" // default for compatibility
	}
}

// getShortTypeName returns the short type name like F32x8 for contrib functions.
func getShortTypeName(elemType string, target Target) string {
	lanes := target.LanesFor(elemType)
	return getShortTypeNameForLanes(elemType, lanes)
}

// getShortTypeNameForLanes returns the short type name for a specific lane count.
func getShortTypeNameForLanes(elemType string, lanes int) string {
	switch elemType {
	case "float32":
		return fmt.Sprintf("F32x%d", lanes)
	case "float64":
		return fmt.Sprintf("F64x%d", lanes)
	case "int32":
		return fmt.Sprintf("I32x%d", lanes)
	case "int64":
		return fmt.Sprintf("I64x%d", lanes)
	case "uint8":
		return fmt.Sprintf("Uint8x%d", lanes)
	case "uint16":
		return fmt.Sprintf("Uint16x%d", lanes)
	case "uint32":
		return fmt.Sprintf("Uint32x%d", lanes)
	case "uint64":
		return fmt.Sprintf("Uint64x%d", lanes)
	default:
		return "Vec"
	}
}

// getHwygenTypeSuffix returns the type suffix used by hwygen for generated functions.
// float32 is the default (no suffix), other types get _Float64, _Int32, _Int64.
func getHwygenTypeSuffix(elemType string) string {
	switch elemType {
	case "float32":
		return "" // default type, no suffix
	case "float64":
		return "_Float64"
	case "int32":
		return "_Int32"
	case "int64":
		return "_Int64"
	default:
		return ""
	}
}

// transformGenDecl transforms variable declarations with generic types.
func transformGenDecl(decl *ast.GenDecl, ctx *transformContext) {
	if decl.Tok != token.VAR && decl.Tok != token.CONST {
		return
	}

	for _, spec := range decl.Specs {
		valueSpec, ok := spec.(*ast.ValueSpec)
		if !ok {
			continue
		}

		// Transform type if present
		if valueSpec.Type != nil {
			typeStr := exprToString(valueSpec.Type)
			// First specialize generic type parameters (T -> float32)
			specialized := specializeType(typeStr, ctx.typeParams, ctx.elemType)
			// Then transform hwy.Vec[float32] -> asm.Float32x4 for SIMD targets
			specialized = specializeVecType(specialized, ctx.elemType, ctx.target, ctx.skipHalfPrecNEON)
			if specialized != typeStr {
				valueSpec.Type = parseTypeExpr(specialized)
			}
		}
	}
}

// transformAssignStmt transforms assignments, particularly for loop stride calculations
// and hoisting hwy.Set calls with constant values.
func transformAssignStmt(stmt *ast.AssignStmt, ctx *transformContext) {
	// For fallback, don't replace NumLanes with a constant - keep it dynamic
	if ctx.target.Name == "Fallback" {
		return
	}

	// For NEON target, detect accumulator patterns and use in-place operations.
	// Pattern: acc = v.MulAdd(a, acc) -> v.MulAddAcc(a, &acc)
	// This avoids return value allocation overhead on ARM64.
	if ctx.target.Name == "NEON" {
		if transformed := tryTransformToInPlace(stmt, ctx); transformed {
			return
		}
	}

	// Look for v.NumElements(), hwy.Lanes[T](), or similar and replace with constant
	for i, rhs := range stmt.Rhs {
		if call, ok := rhs.(*ast.CallExpr); ok {
			// Check for hwy.Lanes[T]() or hwy.NumLanes[T]() - IndexExpr wrapping SelectorExpr
			if indexExpr, ok := call.Fun.(*ast.IndexExpr); ok {
				if sel, ok := indexExpr.X.(*ast.SelectorExpr); ok {
					if pkgIdent, ok := sel.X.(*ast.Ident); ok {
						if pkgIdent.Name == "hwy" && (sel.Sel.Name == "Lanes" || sel.Sel.Name == "MaxLanes" || sel.Sel.Name == "NumLanes") {
							// Extract the actual type parameter from hwy.NumLanes[T]()
							// Use it instead of ctx.elemType to get correct lane count
							effectiveElemType := ctx.elemType
							if typeIdent, ok := indexExpr.Index.(*ast.Ident); ok {
								effectiveElemType = typeIdent.Name
							}
							// Replace with constant lane count for the actual type parameter
							lanes := ctx.target.LanesFor(effectiveElemType)
							stmt.Rhs[i] = &ast.BasicLit{
								Kind:  token.INT,
								Value: strconv.Itoa(lanes),
							}
							// Track the variable name
							if len(stmt.Lhs) > i {
								if ident, ok := stmt.Lhs[i].(*ast.Ident); ok {
									ctx.lanesVars[ident.Name] = true
								}
							}
							continue
						}
					}
				}
			}
			// Check for v.NumElements() or v.NumLanes()
			if sel, ok := call.Fun.(*ast.SelectorExpr); ok {
				if sel.Sel.Name == "NumElements" || sel.Sel.Name == "NumLanes" {
					// Try to look up the element type of the vector variable
					// If the receiver is a variable we've tracked from a Load call,
					// use its element type instead of the function's default
					effectiveElemType := ctx.elemType
					if varIdent, ok := sel.X.(*ast.Ident); ok {
						if varElemType, ok := ctx.varVecElemType[varIdent.Name]; ok {
							effectiveElemType = varElemType
						}
					}
					// Replace with constant lane count
					lanes := ctx.target.LanesFor(effectiveElemType)
					stmt.Rhs[i] = &ast.BasicLit{
						Kind:  token.INT,
						Value: strconv.Itoa(lanes),
					}
					// Track the variable name so we can recognize it in loop strides
					if len(stmt.Lhs) > i {
						if ident, ok := stmt.Lhs[i].(*ast.Ident); ok {
							ctx.lanesVars[ident.Name] = true
						}
					}
				}
			}
		}

		// Check for make([]T, ...) calls
		if call, ok := rhs.(*ast.CallExpr); ok {
			if ident, ok := call.Fun.(*ast.Ident); ok && ident.Name == "make" {
				if len(call.Args) >= 2 {
					// Check if first arg is []T (slice type)
					if arrayType, ok := call.Args[0].(*ast.ArrayType); ok {
						if arrayType.Len == nil { // It's a slice, not an array
							// Specialize the element type (T -> float32/float64)
							elemTypeStr := exprToString(arrayType.Elt)
							specializedType := specializeType(elemTypeStr, ctx.typeParams, ctx.elemType)
							// Also specialize hwy.Vec[float32] -> asm.Float32x4 for SIMD targets
							specializedType = specializeVecType(specializedType, ctx.elemType, ctx.target, ctx.skipHalfPrecNEON)

							// Check if second arg is a lanes variable or literal for stack array optimization
							var lanesCount int
							switch sizeArg := call.Args[1].(type) {
							case *ast.Ident:
								if ctx.lanesVars[sizeArg.Name] {
									lanesCount = ctx.target.LanesFor(ctx.elemType)
								}
							case *ast.BasicLit:
								if sizeArg.Kind == token.INT {
									lanesCount, _ = strconv.Atoi(sizeArg.Value)
								}
							}

							if lanesCount > 0 {
								// Replace make([]T, lanes) with [lanes]T{} (zero-valued array literal)
								stmt.Rhs[i] = &ast.CompositeLit{
									Type: &ast.ArrayType{
										Len: &ast.BasicLit{
											Kind:  token.INT,
											Value: strconv.Itoa(lanesCount),
										},
										Elt: parseTypeExpr(specializedType),
									},
								}
								// Track this variable as a stack array
								if len(stmt.Lhs) > i {
									if ident, ok := stmt.Lhs[i].(*ast.Ident); ok {
										ctx.stackArrayVars[ident.Name] = true
									}
								}
							} else if elemTypeStr != specializedType {
								// Just replace T with concrete type in make call
								arrayType.Elt = parseTypeExpr(specializedType)
							}
						}
					}
				}
			}
		}

		// Check for hwy.Set[T](constant) calls that can be hoisted
		if hoistedName := tryHoistSetCall(stmt, i, rhs, ctx); hoistedName != "" {
			// Replace RHS with reference to hoisted variable
			stmt.Rhs[i] = ast.NewIdent(hoistedName)
		}
	}
}

// findMaxLoadSizeForElemType scans the function body for hwy.Load[T](slice) calls
// and returns the maximum slice size found for the given element type.
// This is used to determine the appropriate vector width for constant hoisting.
func findMaxLoadSizeForElemType(body *ast.BlockStmt, elemType string) int {
	maxSize := 0
	ast.Inspect(body, func(n ast.Node) bool {
		call, ok := n.(*ast.CallExpr)
		if !ok {
			return true
		}
		// Check for hwy.Load[T](slice) pattern
		indexExpr, ok := call.Fun.(*ast.IndexExpr)
		if !ok {
			return true
		}
		selExpr, ok := indexExpr.X.(*ast.SelectorExpr)
		if !ok {
			return true
		}
		ident, ok := selExpr.X.(*ast.Ident)
		if !ok || ident.Name != "hwy" || selExpr.Sel.Name != "LoadSlice" {
			return true
		}
		// Check type parameter matches
		typeIdent, ok := indexExpr.Index.(*ast.Ident)
		if !ok || typeIdent.Name != elemType {
			return true
		}
		// Get slice size from argument
		if len(call.Args) == 1 {
			if size := getSliceSize(call.Args[0]); size > 0 && size > maxSize {
				maxSize = size
			}
		}
		return true
	})
	return maxSize
}

// convertBlankAssignToExprStmt walks a block statement and replaces any
// "_ = expr" assignments with just "expr" as an expression statement.
// This is needed because tryTransformToInPlace converts assignments like
// "acc = v.MulAdd(a, acc)" to "_ = v.MulAddAcc(a, &acc)", but MulAddAcc
// returns void, making "_ = voidFunc()" invalid Go.
func convertBlankAssignToExprStmt(block *ast.BlockStmt) {
	if block == nil {
		return
	}
	for i, stmt := range block.List {
		switch s := stmt.(type) {
		case *ast.AssignStmt:
			// Check for _ = expr pattern where expr is a call (void function)
			if len(s.Lhs) == 1 && len(s.Rhs) == 1 {
				if ident, ok := s.Lhs[0].(*ast.Ident); ok && ident.Name == "_" {
					// Only convert if the RHS is a function/method call
					// (bounds check hints like _ = slice[i] must stay as-is)
					if _, isCall := s.Rhs[0].(*ast.CallExpr); isCall {
						block.List[i] = &ast.ExprStmt{X: s.Rhs[0]}
					}
				}
			}
		case *ast.BlockStmt:
			convertBlankAssignToExprStmt(s)
		case *ast.IfStmt:
			convertBlankAssignToExprStmt(s.Body)
			if s.Else != nil {
				if elseBlock, ok := s.Else.(*ast.BlockStmt); ok {
					convertBlankAssignToExprStmt(elseBlock)
				}
			}
		case *ast.ForStmt:
			convertBlankAssignToExprStmt(s.Body)
		case *ast.RangeStmt:
			convertBlankAssignToExprStmt(s.Body)
		}
	}
}

// tryTransformToInPlace detects accumulator patterns and transforms them to in-place operations.
// Pattern: acc = v.MulAdd(a, acc) -> v.MulAddAcc(a, &acc)
// This only applies to NEON target where in-place operations avoid allocation overhead.
// Returns true if the statement was transformed.
func tryTransformToInPlace(stmt *ast.AssignStmt, ctx *transformContext) bool {
	// Only handle simple assignments with one LHS and one RHS
	if len(stmt.Lhs) != 1 || len(stmt.Rhs) != 1 {
		return false
	}

	// LHS must be an identifier (the accumulator variable)
	lhsIdent, ok := stmt.Lhs[0].(*ast.Ident)
	if !ok {
		return false
	}
	accName := lhsIdent.Name

	// RHS must be a method call
	call, ok := stmt.Rhs[0].(*ast.CallExpr)
	if !ok {
		return false
	}

	// Get the method name
	sel, ok := call.Fun.(*ast.SelectorExpr)
	if !ok {
		return false
	}
	methodName := sel.Sel.Name

	// Check if this operation has an in-place variant
	var inPlaceOp OpInfo
	var foundInPlace bool

	// When the call is a function call like hwy.MulAdd(vA, vB, vC) that will be converted
	// to a method call vA.MulAdd(vB, vC), the first arg becomes receiver, shifting indices.
	// Detect this by checking if the call is hwy.* (function) vs receiver.Method() (already method).
	accArgAdjustment := 0
	if pkgIdent, ok := sel.X.(*ast.Ident); ok {
		// hwy.MulAdd(...) is a function call - after transformation, arg[0] becomes receiver
		if pkgIdent.Name == "hwy" || pkgIdent.Name == "asm" {
			accArgAdjustment = 1
		}
	}

	for opName, opInfo := range ctx.target.OpMap {
		if opInfo.InPlaceOf == methodName {
			// Check if the last argument is the same as LHS (accumulator pattern)
			if len(call.Args) > 0 {
				lastArg := call.Args[len(call.Args)-1]
				if argIdent, ok := lastArg.(*ast.Ident); ok && argIdent.Name == accName {
					// Verify AccArg matches the actual last argument index after transformation.
					// For half-precision NEON: hwy.MulAdd(vA, vB, vC) -> vA.MulAddAcc(vB, &vC)
					// Original: 3 args, lastArg at index 2
					// After transformation: 2 args (vB, &vC), lastArg at index 1
					// MulAddAcc.AccArg = 1, so check: 1 == (3-1) - 1 = 1 
					expectedAccArg := len(call.Args) - 1 - accArgAdjustment
					if opInfo.AccArg == expectedAccArg {
						inPlaceOp = opInfo
						inPlaceOp.Name = opName // Use the in-place op name
						foundInPlace = true
						break
					}
				}
			}
		}
	}

	if !foundInPlace {
		return false
	}

	// Transform: acc = v.MulAdd(a, acc) -> v.MulAddAcc(a, &acc)
	// The receiver stays the same, we change the method name and wrap the last arg with &

	// Change method name to in-place version
	sel.Sel.Name = inPlaceOp.Name

	// Wrap the accumulator argument with &
	lastIdx := len(call.Args) - 1
	call.Args[lastIdx] = &ast.UnaryExpr{
		Op: token.AND,
		X:  call.Args[lastIdx],
	}

	// Remove the assignment - convert to expression statement
	// We need to replace the AssignStmt with an ExprStmt in the parent
	// Since we can't easily do that here, we'll use a workaround:
	// Set the LHS to a blank identifier and the RHS to the call
	// This isn't ideal, but Go will optimize away the blank assignment
	stmt.Lhs[0] = ast.NewIdent("_")

	return true
}

// tryHoistSetCall checks if an expression is a hwy.Set[T](constant) call
// and if so, registers it for hoisting and returns the hoisted variable name.
func tryHoistSetCall(stmt *ast.AssignStmt, rhsIndex int, rhs ast.Expr, ctx *transformContext) string {
	call, ok := rhs.(*ast.CallExpr)
	if !ok {
		return ""
	}

	// Check for hwy.Set[T](arg) pattern - could be IndexExpr wrapping SelectorExpr
	var selExpr *ast.SelectorExpr
	var typeParam string
	switch fun := call.Fun.(type) {
	case *ast.IndexExpr:
		// hwy.Set[T](arg)
		selExpr, ok = fun.X.(*ast.SelectorExpr)
		if !ok {
			return ""
		}
		// Extract the type parameter
		if typeIdent, ok := fun.Index.(*ast.Ident); ok {
			typeParam = typeIdent.Name
		}
	case *ast.SelectorExpr:
		// hwy.Set(arg) - non-generic, shouldn't happen but handle it
		selExpr = fun
	default:
		return ""
	}

	// Verify it's hwy.Set or hwy.Const
	ident, ok := selExpr.X.(*ast.Ident)
	if !ok || ident.Name != "hwy" {
		return ""
	}
	isConst := selExpr.Sel.Name == "Const"
	if selExpr.Sel.Name != "Set" && !isConst {
		return ""
	}

	// Determine the actual element type for this call
	actualElemType := ctx.elemType
	if !isConst && typeParam == "int32" {
		actualElemType = "int32"
		// For non-AVX-promoted half-precision types, don't hoist int32 constants to native SIMD types
		// because hwy.ConvertToInt32 returns hwy.Vec[int32], not native SIMD types.
		// Keeping them as hwy.Set[int32] ensures type compatibility.
		// For AVX promoted half-precision, ConvertToInt32 returns archsimd.Int32x8/Int32x16,
		// so hoisting as archsimd.BroadcastInt32xN is correct.
		if isHalfPrecisionType(ctx.elemType) && !isAVXPromotedHalfPrec(ctx.target, ctx.elemType) {
			return ""
		}
	}
	// For all half-precision types (NEON, AVX promoted, Fallback), skip hoisting float constants.
	// NEON and Fallback: constants stay as inline hwy.Set/Const calls.
	// AVX promoted: the inline Setasm.Broadcast transformation produces the correct promoted type.
	// Int32 constants for AVX promoted are handled above (line 5139) and are fine to hoist.
	if isHalfPrecisionType(ctx.elemType) && actualElemType != "int32" {
		return ""
	}

	// Check if the argument is a constant (literal or type conversion of constant)
	if len(call.Args) != 1 {
		return ""
	}
	arg := call.Args[0]
	constValue := extractConstantValue(arg, actualElemType, ctx)
	if constValue == "" {
		return ""
	}

	// Get the local variable name being assigned
	if rhsIndex >= len(stmt.Lhs) {
		return ""
	}
	localIdent, ok := stmt.Lhs[rhsIndex].(*ast.Ident)
	if !ok {
		return ""
	}
	localVarName := localIdent.Name

	// Generate unique hoisted variable name (include target to avoid conflicts)
	// For int32 constants, we need separate versions for f32 and f64 functions
	// because they have different lane counts
	elemSuffix := "f32"
	if ctx.elemType == "float64" {
		elemSuffix = "f64"
	}
	if actualElemType == "int32" {
		// Include parent element type in suffix for proper lane matching
		if ctx.elemType == "float64" {
			elemSuffix = "i32_f64"
		} else {
			elemSuffix = "i32_f32"
		}
	}
	hoistedName := fmt.Sprintf("%s_%s_%s_%s", ctx.funcName, ctx.target.Name, localVarName, elemSuffix)

	// Get vector type and broadcast function for this target
	// For int32 types used in float operations, match the lane count of the parent element type
	// If inferredFuncLanes is set and smaller than target width, use it to match Load sizes
	var useLanes int
	if actualElemType == "int32" || actualElemType == "int64" {
		// Int32/int64 constants used in float functions should match the parent type's lane count
		// e.g., int32 constants in float64 functions need 2 lanes on NEON, not 4
		useLanes = ctx.target.LanesFor(ctx.elemType)
	} else {
		targetLanes := ctx.target.LanesFor(actualElemType)
		useLanes = targetLanes
		if ctx.inferredFuncLanes > 0 && ctx.inferredFuncLanes < targetLanes {
			useLanes = ctx.inferredFuncLanes
		}
	}
	vecTypeName := getVectorTypeNameForLanes(actualElemType, useLanes)
	pkgName := getVecPackageName(ctx.target)
	broadcastFunc := fmt.Sprintf("%s.Broadcast%s", pkgName, vecTypeName)

	// Register the hoisted constant
	ctx.hoistedConsts[localVarName] = HoistedConst{
		VarName:   hoistedName,
		Value:     constValue,
		VecType:   vecTypeName,
		Broadcast: broadcastFunc,
	}

	return hoistedName
}

// extractConstantValue extracts the string representation of a constant value.
// Returns empty string if the expression is not a constant.
// The elemType parameter is used to add type conversion when needed.
// The ctx is used to check if a variable is locally-defined (not a constant).
func extractConstantValue(expr ast.Expr, elemType string, ctx *transformContext) string {
	switch e := expr.(type) {
	case *ast.BasicLit:
		// Literal like 1.0, 0.5, etc.
		return e.Value
	case *ast.UnaryExpr:
		// Handle negative literals like -1.0
		if e.Op == token.SUB {
			if inner := extractConstantValueRaw(e.X, ctx); inner != "" {
				return "-" + inner
			}
		}
	case *ast.CallExpr:
		// Type conversion like T(1.0) or float32(sigmoidC1)
		// Get the inner value without adding another type conversion
		if len(e.Args) == 1 {
			inner := extractConstantValueRaw(e.Args[0], ctx)
			if inner != "" {
				// Add the target type conversion
				return fmt.Sprintf("%s(%s)", elemType, inner)
			}
		}
	case *ast.Ident:
		// Variable reference - only hoist if it's NOT a local variable
		name := e.Name
		if ctx.localVars[name] {
			// This is a locally-defined variable, not a package-level constant
			return ""
		}
		if isLikelyConstant(name) {
			// Add type conversion in case the var type differs from target type
			return fmt.Sprintf("%s(%s)", elemType, name)
		}
	}
	return ""
}

// extractConstantValueRaw extracts the raw constant value without type conversion.
func extractConstantValueRaw(expr ast.Expr, ctx *transformContext) string {
	switch e := expr.(type) {
	case *ast.BasicLit:
		return e.Value
	case *ast.UnaryExpr:
		if e.Op == token.SUB {
			if inner := extractConstantValueRaw(e.X, ctx); inner != "" {
				return "-" + inner
			}
		}
	case *ast.Ident:
		name := e.Name
		// Skip if it's a locally-defined variable
		if ctx != nil && ctx.localVars[name] {
			return ""
		}
		if isLikelyConstant(name) {
			return name
		}
	}
	return ""
}

// isLikelyConstant checks if a name looks like a package-level constant.
// This is a heuristic - constants typically have specific naming patterns.
// Note: This function is only called AFTER checking that the name is not
// in ctx.localVars, so locally-defined variables are already excluded.
func isLikelyConstant(name string) bool {
	// Skip very common short names that are almost never constants
	skipNames := map[string]bool{
		"i": true, "j": true, "k": true, "ii": true, "jj": true,
		"x": true, "y": true, "z": true, "n": true, "m": true,
		"a": true, "b": true, "c": true, "v": true, "w": true,
		"err": true, "ok": true,
	}
	if skipNames[name] {
		return false
	}

	// Constants typically:
	// 1. Contain digits (e.g., sigmoidC1, ln2Hi, exp2bias, c1, c2)
	// 2. Are all uppercase (e.g., PI, MAX_VALUE)
	hasDigit := false
	hasLower := false
	hasUpper := false
	for _, r := range name {
		if r >= '0' && r <= '9' {
			hasDigit = true
		} else if r >= 'a' && r <= 'z' {
			hasLower = true
		} else if r >= 'A' && r <= 'Z' {
			hasUpper = true
		}
	}

	// Accept if it has a digit (like sigmoidC1, ln2Hi)
	if hasDigit {
		return true
	}

	// Accept if all uppercase (like PI, MAX_VALUE)
	if hasUpper && !hasLower {
		return true
	}

	// Reject short lowercase names that look like local variables
	if len(name) <= 4 && hasLower {
		return false
	}

	// Accept longer camelCase names that look like package-level constants
	// (e.g., sigmoidScale, expBias, tanhClamp)
	return len(name) > 4
}

// matchesLoopIterator checks if a for loop uses the given iterator name.
// It checks the init statement, condition, and post statement.
func matchesLoopIterator(forStmt *ast.ForStmt, iteratorName string) bool {
	// Check init statement: for ii := 0
	if forStmt.Init != nil {
		if assign, ok := forStmt.Init.(*ast.AssignStmt); ok {
			for _, lhs := range assign.Lhs {
				if ident, ok := lhs.(*ast.Ident); ok && ident.Name == iteratorName {
					return true
				}
			}
		}
	}

	// Check condition: ii < size, ii+N <= size, or (ii+N)+M <= size
	if forStmt.Cond != nil {
		if binExpr, ok := forStmt.Cond.(*ast.BinaryExpr); ok {
			// Check LHS directly (ii < size)
			if ident, ok := binExpr.X.(*ast.Ident); ok && ident.Name == iteratorName {
				return true
			}
			// Check LHS if it's a binary expression (ii+N <= size)
			if innerBin, ok := binExpr.X.(*ast.BinaryExpr); ok {
				if ident, ok := innerBin.X.(*ast.Ident); ok && ident.Name == iteratorName {
					return true
				}
				// Check deeper nesting: (ii+N)+M <= size (after transformForStmt wraps condition)
				if deeperBin, ok := innerBin.X.(*ast.BinaryExpr); ok {
					if ident, ok := deeperBin.X.(*ast.Ident); ok && ident.Name == iteratorName {
						return true
					}
				}
			}
		}
	}

	// Check post statement: ii += lanes or ii++
	if forStmt.Post != nil {
		if assignStmt, ok := forStmt.Post.(*ast.AssignStmt); ok {
			for _, lhs := range assignStmt.Lhs {
				if ident, ok := lhs.(*ast.Ident); ok && ident.Name == iteratorName {
					return true
				}
			}
		}
		if incDecStmt, ok := forStmt.Post.(*ast.IncDecStmt); ok {
			if ident, ok := incDecStmt.X.(*ast.Ident); ok && ident.Name == iteratorName {
				return true
			}
		}
	}

	return false
}

// insertTailHandling adds scalar tail handling after the vectorized loop.
func insertTailHandling(body *ast.BlockStmt, loopInfo *LoopInfo, elemType string, target Target, funcName string, params []Param, typeParams []TypeParam) {
	if body == nil || loopInfo == nil {
		return
	}

	// For fallback, no tail handling needed - callers must provide inputs >= vector width
	if target.Name == "Fallback" {
		return
	}

	// Count SIMD loops that use the same iterator. If there are multiple SIMD loops,
	// the function has a multi-phase algorithm (e.g., Normalize: accumulate then scale)
	// and automatic tail handling would break the data dependencies between phases.
	// In such cases, the template must handle tails manually.
	simdLoopCount := 0
	for _, stmt := range body.List {
		if forStmt, ok := stmt.(*ast.ForStmt); ok {
			if matchesLoopIterator(forStmt, loopInfo.Iterator) && isSimdStyleLoop(forStmt, loopInfo) {
				simdLoopCount++
			}
		}
	}
	if simdLoopCount > 1 {
		// Multiple SIMD loops - don't insert automatic tail handling
		return
	}

	// Find the SIMD loop that uses loopInfo.Iterator as its iterator.
	// This ensures we don't add tail handling after unrelated loops (e.g., scalar loops).
	var loopIdx int = -1
	var mainLoop *ast.ForStmt
	for i, stmt := range body.List {
		if forStmt, ok := stmt.(*ast.ForStmt); ok {
			// Check if this loop's iterator matches loopInfo.Iterator
			if matchesLoopIterator(forStmt, loopInfo.Iterator) {
				loopIdx = i
				mainLoop = forStmt
				break
			}
		}
	}

	if mainLoop == nil || loopIdx < 0 {
		return
	}

	// Declare the iterator before the loop so it's in scope for the tail
	// Change: for ii := 0; ... to: ii := 0; for ; ...
	var initStmt ast.Stmt
	if mainLoop.Init != nil {
		initStmt = mainLoop.Init
		mainLoop.Init = nil
	}

	// Build tail handling that calls the fallback function for remaining elements
	// if ii < size {
	//     BaseSigmoid_fallback(in[ii:size], out[ii:size])
	// }
	fallbackFuncName := funcName + "_fallback"
	// Add type suffix for non-float32 types only for generic functions
	// (matches how generator.go names functions in generator.go:100-102)
	if elemType != "float32" && len(typeParams) > 0 {
		fallbackFuncName = fallbackFuncName + "_" + typeNameToSuffix(elemType)
	}

	// Build arguments for the fallback call
	// For slice parameters: param[ii:size]
	// For non-slice parameters: pass as-is
	var callArgs []ast.Expr
	for _, param := range params {
		if strings.HasPrefix(param.Type, "[]") {
			// Create param[ii:size] for slice parameters
			sliceExpr := &ast.SliceExpr{
				X:    ast.NewIdent(param.Name),
				Low:  ast.NewIdent(loopInfo.Iterator),
				High: ast.NewIdent(loopInfo.End),
			}
			callArgs = append(callArgs, sliceExpr)
		} else {
			// Pass non-slice parameters as-is
			callArgs = append(callArgs, ast.NewIdent(param.Name))
		}
	}

	// Create the fallback call: BasePoly2_fallback(x[ii:size], c0, c1, c2, result[ii:size])
	fallbackCall := &ast.CallExpr{
		Fun:  ast.NewIdent(fallbackFuncName),
		Args: callArgs,
	}

	// Wrap in if statement: if ii < size { ... }
	tailIf := &ast.IfStmt{
		Cond: &ast.BinaryExpr{
			X:  ast.NewIdent(loopInfo.Iterator),
			Op: token.LSS,
			Y:  ast.NewIdent(loopInfo.End),
		},
		Body: &ast.BlockStmt{
			List: []ast.Stmt{
				&ast.ExprStmt{X: fallbackCall},
			},
		},
	}

	// Insert init statement, main loop, and tail handling
	// Check if the next statement is a scalar tail loop that can be replaced by fallback
	nextIdx := loopIdx + 1
	canReplaceTailLoop := false
	if nextIdx < len(body.List) {
		if isScalarTailLoop(body.List[nextIdx], loopInfo.Iterator, loopInfo.End) {
			canReplaceTailLoop = true
			nextIdx++ // Skip the scalar tail loop (it will be replaced by fallback call)
		}
	}

	newStmts := make([]ast.Stmt, 0, len(body.List)+2)
	newStmts = append(newStmts, body.List[:loopIdx]...)
	if initStmt != nil {
		newStmts = append(newStmts, initStmt)
	}
	newStmts = append(newStmts, mainLoop)
	// Only add the fallback call if we're replacing the scalar tail loop.
	// If the tail loop uses external variables (like 'scale' computed from full array),
	// we must keep the original loop which correctly uses those variables.
	if canReplaceTailLoop {
		newStmts = append(newStmts, tailIf)
	}
	newStmts = append(newStmts, body.List[nextIdx:]...)
	body.List = newStmts
}

// isScalarTailLoop checks if a statement is a scalar tail loop that should be
// replaced by the fallback call. A scalar tail loop has the form:
//
//	for ; i < n; i++ { ... }
//
// where i is the iterator and n is the end variable from the SIMD loop.
// Returns false if the loop body assigns to local variables (other than indexed
// array elements), as these indicate state that the fallback cannot handle.
func isScalarTailLoop(stmt ast.Stmt, iterator, end string) bool {
	forStmt, ok := stmt.(*ast.ForStmt)
	if !ok {
		return false
	}

	// Scalar tail loops have no Init (the iterator is already declared)
	if forStmt.Init != nil {
		return false
	}

	// Check condition: i < n
	cond, ok := forStmt.Cond.(*ast.BinaryExpr)
	if !ok || cond.Op != token.LSS {
		return false
	}

	// Left side should be the iterator
	leftIdent, ok := cond.X.(*ast.Ident)
	if !ok || leftIdent.Name != iterator {
		return false
	}

	// Right side should be the end variable (can be identifier like "n" or call like "len(dst)")
	if exprToString(cond.Y) != end {
		return false
	}

	// Check post: i++ (increment expression)
	post, ok := forStmt.Post.(*ast.IncDecStmt)
	if !ok || post.Tok != token.INC {
		return false
	}

	postIdent, ok := post.X.(*ast.Ident)
	if !ok || postIdent.Name != iterator {
		return false
	}

	// Check if the loop body assigns to local variables (not array elements).
	// If so, this loop has state that the fallback cannot handle correctly.
	// Example: "prev = src[i]" indicates state tracking that needs the manual loop.
	if hasLocalVariableAssignment(forStmt.Body, iterator) {
		return false
	}

	// Check if the loop body uses external variables (not just the iterator and arrays).
	// If so, those variables were computed from the full input and the fallback would
	// recalculate them incorrectly from just the tail.
	// Example: "dst[i] *= scale" uses external variable "scale" computed from full array.
	if usesExternalVariables(forStmt.Body, iterator) {
		return false
	}

	return true
}

// hasLocalVariableAssignment checks if a block contains assignments to local
// variables (identifiers) rather than just indexed array/slice elements.
// Assignments like "prev = src[i]" return true.
// Assignments like "dst[i] = x" return false (these are array element assignments).
func hasLocalVariableAssignment(body *ast.BlockStmt, iterator string) bool {
	if body == nil {
		return false
	}

	hasLocalAssign := false
	ast.Inspect(body, func(n ast.Node) bool {
		assign, ok := n.(*ast.AssignStmt)
		if !ok {
			return true
		}

		for _, lhs := range assign.Lhs {
			// Check if this is an assignment to a plain identifier (not array index)
			if ident, ok := lhs.(*ast.Ident); ok {
				// Skip the iterator variable itself
				if ident.Name != iterator {
					hasLocalAssign = true
					return false
				}
			}
		}
		return true
	})

	return hasLocalAssign
}

// usesExternalVariables checks if a loop body uses variables that were defined
// outside the loop (excluding the iterator and slice/array variables used in index expressions).
// For example, "dst[i] *= scale" uses external variable "scale".
// The fallback function would recalculate such variables from just the tail, which is wrong.
func usesExternalVariables(body *ast.BlockStmt, iterator string) bool {
	if body == nil {
		return false
	}

	// Collect identifiers that are OK to use:
	// 1. Slices/arrays being indexed (function parameters)
	// 2. Identifiers that are part of selector expressions (package.Func, obj.Method)
	okIdents := make(map[*ast.Ident]bool)

	ast.Inspect(body, func(n ast.Node) bool {
		switch expr := n.(type) {
		case *ast.IndexExpr:
			// Mark the slice/array being indexed as OK
			if ident, ok := expr.X.(*ast.Ident); ok {
				okIdents[ident] = true
			}
		case *ast.SelectorExpr:
			// Mark both parts of selector expressions as OK
			// e.g., hwy.Float32ToFloat16 or dst[i].Float32()
			if ident, ok := expr.X.(*ast.Ident); ok {
				okIdents[ident] = true
			}
			okIdents[expr.Sel] = true
		case *ast.CallExpr:
			// Mark function name in direct calls as OK
			if ident, ok := expr.Fun.(*ast.Ident); ok {
				okIdents[ident] = true
			}
		}
		return true
	})

	hasExternal := false
	ast.Inspect(body, func(n ast.Node) bool {
		ident, ok := n.(*ast.Ident)
		if !ok {
			return true
		}

		// Skip if already marked as OK
		if okIdents[ident] {
			return true
		}

		name := ident.Name

		// Skip the iterator variable
		if name == iterator {
			return true
		}

		// Skip built-in identifiers
		builtins := map[string]bool{
			"true": true, "false": true, "nil": true,
			"int": true, "int8": true, "int16": true, "int32": true, "int64": true,
			"uint": true, "uint8": true, "uint16": true, "uint32": true, "uint64": true,
			"float32": true, "float64": true, "complex64": true, "complex128": true,
			"string": true, "bool": true, "byte": true, "rune": true,
			"len": true, "cap": true, "make": true, "new": true, "append": true,
			"copy": true, "delete": true, "panic": true, "recover": true, "close": true,
			"print": true, "println": true,
		}
		if builtins[name] {
			return true
		}

		// Skip blank identifier
		if name == "_" {
			return true
		}

		// This is an external variable - flag it
		hasExternal = true
		return false
	})

	return hasExternal
}

// isSimdStyleLoop checks if a for loop appears to be a SIMD-style loop (as opposed
// to a scalar tail loop). SIMD loops typically have:
// - A condition like i+lanes <= len(dst) (not i < len)
// - A stride like i += lanes (not i++)
func isSimdStyleLoop(forStmt *ast.ForStmt, loopInfo *LoopInfo) bool {
	if forStmt == nil || forStmt.Cond == nil || forStmt.Post == nil {
		return false
	}

	// Check condition: should be i+lanes <= len (not i < len)
	cond, ok := forStmt.Cond.(*ast.BinaryExpr)
	if !ok {
		return false
	}

	// SIMD loop condition is typically <= (not <)
	// Or it's < with a +lanes on the left side
	if cond.Op == token.LEQ {
		return true
	}

	// Check if left side is i+lanes (binary expr with +)
	if cond.Op == token.LSS {
		if _, ok := cond.X.(*ast.BinaryExpr); ok {
			// i+lanes < len pattern
			return true
		}
	}

	// Check post: should be i += lanes (not i++)
	switch post := forStmt.Post.(type) {
	case *ast.AssignStmt:
		// i += lanes
		if post.Tok == token.ADD_ASSIGN {
			return true
		}
	case *ast.IncDecStmt:
		// i++ is NOT a SIMD loop
		return false
	}

	return false
}

// specializeType replaces generic type parameters with concrete types.
// For SIMD targets, also transforms hwy.Vec[T] to archsimd/asm vector types.
func specializeType(typeStr string, typeParams []TypeParam, elemType string) string {
	// First, identify which type parameters are element types vs interface types
	elementTypeParams := make(map[string]bool)
	interfaceTypeParams := make(map[string]string) // maps param name to constraint

	for _, tp := range typeParams {
		// Element type constraints
		if strings.Contains(tp.Constraint, "Lanes") ||
			strings.Contains(tp.Constraint, "Floats") ||
			strings.Contains(tp.Constraint, "Integers") ||
			strings.Contains(tp.Constraint, "SignedInts") ||
			strings.Contains(tp.Constraint, "UnsignedInts") {
			elementTypeParams[tp.Name] = true
		} else {
			// Interface constraint (like Predicate[T])
			interfaceTypeParams[tp.Name] = tp.Constraint
		}
	}

	// Replace element type parameters and hwy.Vec[T]/hwy.Mask[T]
	for _, tp := range typeParams {
		if elementTypeParams[tp.Name] {
			// Replace hwy.Vec[T] with concrete vector type placeholder
			typeStr = strings.ReplaceAll(typeStr, "hwy.Vec["+tp.Name+"]", "hwy.Vec["+elemType+"]")
			// Replace hwy.Mask[T] with concrete mask type placeholder
			typeStr = strings.ReplaceAll(typeStr, "hwy.Mask["+tp.Name+"]", "hwy.Mask["+elemType+"]")
			// Replace []T with []float32, etc.
			typeStr = strings.ReplaceAll(typeStr, "[]"+tp.Name, "[]"+elemType)
			// Replace standalone T with concrete type
			typeStr = replaceTypeParam(typeStr, tp.Name, elemType)
		}
	}

	// For interface type parameters, specialize the generic type within the constraint
	// e.g., Predicate[T] -> Predicate[float32]
	for paramName, constraint := range interfaceTypeParams {
		// Check if this parameter's type is exactly its constraint (e.g., "P" -> "Predicate[T]")
		if typeStr == paramName {
			// Specialize the constraint's type parameters
			specializedConstraint := constraint
			for _, tp := range typeParams {
				if elementTypeParams[tp.Name] {
					specializedConstraint = strings.ReplaceAll(specializedConstraint, "["+tp.Name+"]", "["+elemType+"]")
				}
			}
			typeStr = specializedConstraint
		}
	}

	return typeStr
}

// replaceTypeParam replaces a type parameter name with a concrete type,
// being careful to only replace it when it appears as a standalone type
// (not as part of another identifier).
func replaceTypeParam(typeStr, paramName, elemType string) string {
	// Simple approach: replace when the parameter appears alone or as a type argument
	// This handles cases like "T", "[T]", "[]T", "func(T)"
	result := typeStr

	// Replace [T] with [elemType]
	result = strings.ReplaceAll(result, "["+paramName+"]", "["+elemType+"]")

	// Replace T when it's the whole string
	if result == paramName {
		return elemType
	}

	// Replace T in slice types []T
	result = strings.ReplaceAll(result, "[]"+paramName, "[]"+elemType)

	// Replace T in function types and map value types - look for patterns like "T)" or "T," or "(T" or "]T"
	// This is a simple heuristic that works for most cases
	for _, suffix := range []string{")", ",", " ", ""} {
		for _, prefix := range []string{"(", ",", " ", "]"} {
			old := prefix + paramName + suffix
			new := prefix + elemType + suffix
			result = strings.ReplaceAll(result, old, new)
		}
	}

	return result
}

// complexHalfPrecOps lists hwy.* operations that cannot be converted to asm.Float16x8/BFloat16x8
// method calls. Functions using these must stay on the generic hwy.Vec[T] path for half-precision NEON.
var complexHalfPrecOps = map[string]bool{
	"RoundToEven":            true,
	"ConvertToInt32":         true,
	"ConvertToFloat32":       true,
	"Pow2":                   true,
	"GetExponent":            true,
	"GetMantissa":            true,
	"ConvertExponentToFloat": true,
	"Equal":                  true,
	"MaskAnd":                true,
	"Pow":                    true,
}

// externalGenericHalfPrecPkgs lists package names whose Base*Vec functions use the
// generic hwy.Vec[T] path for half-precision NEON (and thus callers must also use it).
var externalGenericHalfPrecPkgs = map[string]bool{
	"math": true,
}

// NeedsGenericHalfPrecisionPath scans a function body for hwy.* calls that cannot be
// converted to asm types for NEON half-precision, or references to external package
// functions (like math.BaseExpVec) that use the generic path. Returns true if the
// function should use the generic hwy.Vec[T] path instead of asm.Float16x8/BFloat16x8.
func NeedsGenericHalfPrecisionPath(body *ast.BlockStmt) bool {
	found := false
	ast.Inspect(body, func(n ast.Node) bool {
		if found {
			return false
		}
		// Check all selector expressions (not just call targets) for function value references
		// like math.BaseExpVec passed as argument
		if sel, ok := n.(*ast.SelectorExpr); ok {
			if ident, ok := sel.X.(*ast.Ident); ok {
				if ident.Name == "hwy" {
					if complexHalfPrecOps[sel.Sel.Name] {
						found = true
						return false
					}
				}
				// Detect references to external package functions (e.g., math.BaseExpVec)
				if externalGenericHalfPrecPkgs[ident.Name] && strings.HasPrefix(sel.Sel.Name, "Base") {
					found = true
					return false
				}
			}
		}
		return true
	})
	return found
}

// CollectBaseFuncCalls returns the set of Base* function names called from a function body.
func CollectBaseFuncCalls(body *ast.BlockStmt) map[string]bool {
	calls := make(map[string]bool)
	ast.Inspect(body, func(n ast.Node) bool {
		call, ok := n.(*ast.CallExpr)
		if !ok {
			return true
		}
		// Check direct Base* calls: BaseFoo(args)
		if ident, ok := call.Fun.(*ast.Ident); ok {
			if strings.HasPrefix(ident.Name, "Base") {
				calls[ident.Name] = true
			}
		}
		// Check generic Base* calls: BaseFoo[T](args)
		if indexExpr, ok := call.Fun.(*ast.IndexExpr); ok {
			if ident, ok := indexExpr.X.(*ast.Ident); ok {
				if strings.HasPrefix(ident.Name, "Base") {
					calls[ident.Name] = true
				}
			}
		}
		return true
	})
	return calls
}

// hasFuncParamWithVecType checks if any function parameter is a function type
// containing hwy.Vec. Such functions must use the generic half-precision path
// because callers from other files may pass generic-typed callbacks.
// For example: func BaseApply[T hwy.Floats](in, out []T, fn func(hwy.Vec[T]) hwy.Vec[T])
func hasFuncParamWithVecType(params []Param) bool {
	for _, p := range params {
		if strings.HasPrefix(p.Type, "func(") && strings.Contains(p.Type, "hwy.Vec[") {
			return true
		}
	}
	return false
}

// ComputeGenericHalfPrecFuncs computes the set of function names that need the generic
// hwy.Vec[T] path for half-precision NEON, including transitive dependencies through
// Base* function calls.
func ComputeGenericHalfPrecFuncs(funcs []ParsedFunc) map[string]bool {
	// Pass 1: identify functions that directly use complex ops or have
	// function-typed parameters with hwy.Vec (cross-file compatibility)
	genericFuncs := make(map[string]bool)
	callGraph := make(map[string]map[string]bool) // caller -> set of Base* callees

	for _, pf := range funcs {
		if NeedsGenericHalfPrecisionPath(pf.Body) || hasFuncParamWithVecType(pf.Params) {
			genericFuncs[pf.Name] = true
		}
		callGraph[pf.Name] = CollectBaseFuncCalls(pf.Body)
	}

	// Pass 2: propagate transitively in both directions:
	// - If a function calls a generic-path function, the caller must also be on the generic path
	//   (because it will receive hwy.Vec[T] return values)
	// - If a generic-path function calls a non-generic function, the callee must also be on the
	//   generic path (because it will receive hwy.Vec[T] arguments from the generic caller)
	changed := true
	for changed {
		changed = false
		for _, pf := range funcs {
			if genericFuncs[pf.Name] {
				// Propagate to callees: generic caller forces callees to be generic
				for callee := range callGraph[pf.Name] {
					if !genericFuncs[callee] {
						genericFuncs[callee] = true
						changed = true
					}
				}
				continue
			}
			// Propagate to callers: if any callee is generic, caller must be too
			for callee := range callGraph[pf.Name] {
				if genericFuncs[callee] {
					genericFuncs[pf.Name] = true
					changed = true
					break
				}
			}
		}
	}

	return genericFuncs
}

// specializeVecType transforms hwy.Vec[elemType] and hwy.Mask[elemType] to concrete archsimd/asm types.
// For example: hwy.Vec[float32] -> archsimd.Float32x8 (for AVX2)
//
//	hwy.Mask[float32] -> archsimd.Int32x8 (for AVX2)
//
// If skipHalfPrec is true, half-precision types on NEON are NOT converted to asm types,
// keeping them on the generic hwy.Vec[T] path (used for functions with complex ops like RoundToEven).
func specializeVecType(typeStr string, elemType string, target Target, skipHalfPrec ...bool) string {
	if target.Name == "Fallback" {
		// For fallback, keep hwy.Vec[float32], hwy.Mask[float32] etc.
		return typeStr
	}

	// For Float16/BFloat16 on NEON, convert to concrete asm types (Float16x8, BFloat16x8).
	// For Float16/BFloat16 on AVX2/AVX512, convert to promoted asm types that wrap archsimd.Float32x{8,16}.
	// On Fallback, keep hwy.Vec[hwy.Float16].
	if isHalfPrecisionType(elemType) {
		if target.Name == "NEON" && !(len(skipHalfPrec) > 0 && skipHalfPrec[0]) {
			asmType := "asm.Float16x8"
			if isBFloat16Type(elemType) {
				asmType = "asm.BFloat16x8"
			}
			vecPlaceholder := "hwy.Vec[" + elemType + "]"
			typeStr = strings.ReplaceAll(typeStr, vecPlaceholder, asmType)
			// Also handle Mask types for half-precision on NEON
			maskPlaceholder := "hwy.Mask[" + elemType + "]"
			typeStr = strings.ReplaceAll(typeStr, maskPlaceholder, "asm.Uint16x8")
			return typeStr
		}
		if target.Name == "AVX2" || target.Name == "AVX512" {
			asmType := "asm." + target.TypeMap[elemType]
			vecPlaceholder := "hwy.Vec[" + elemType + "]"
			typeStr = strings.ReplaceAll(typeStr, vecPlaceholder, asmType)
			// Half-precision on AVX uses Mask32x{8,16} since underlying data is float32
			maskPlaceholder := "hwy.Mask[" + elemType + "]"
			lanes := target.LanesFor(elemType)
			maskType := fmt.Sprintf("archsimd.Mask32x%d", lanes)
			typeStr = strings.ReplaceAll(typeStr, maskPlaceholder, maskType)
			return typeStr
		}
		return typeStr
	}

	pkgName := target.VecPackage
	if pkgName == "" {
		pkgName = "archsimd" // default
	}

	// Transform hwy.Vec[elemType]
	vecPlaceholder := "hwy.Vec[" + elemType + "]"
	if strings.Contains(typeStr, vecPlaceholder) {
		vecTypeName, ok := target.TypeMap[elemType]
		if ok {
			concreteType := pkgName + "." + vecTypeName
			typeStr = strings.ReplaceAll(typeStr, vecPlaceholder, concreteType)
		}
	}

	// Transform hwy.Mask[elemType] to integer vector type (masks are represented as integer vectors)
	maskPlaceholder := "hwy.Mask[" + elemType + "]"
	if strings.Contains(typeStr, maskPlaceholder) {
		maskTypeName := getMaskTypeName(elemType, target)
		if maskTypeName != "" {
			concreteMaskType := pkgName + "." + maskTypeName
			typeStr = strings.ReplaceAll(typeStr, maskPlaceholder, concreteMaskType)
		}
	}

	return typeStr
}

// getMaskTypeName returns the mask type name for a given element type and target.
// For archsimd (AVX2/AVX512), masks are dedicated Mask32xN or Mask64xN types.
// For NEON and fallback, masks may use integer vector types.
func getMaskTypeName(elemType string, target Target) string {
	lanes := target.LanesFor(elemType)
	// For archsimd targets, use proper Mask types
	if target.VecPackage == "archsimd" {
		switch elemType {
		case "float32", "int32", "uint32":
			return fmt.Sprintf("Mask32x%d", lanes)
		case "float64", "int64", "uint64":
			return fmt.Sprintf("Mask64x%d", lanes)
		default:
			// Half-precision on AVX uses float32 promoted storage, so Mask32xN
			if isHalfPrecisionType(elemType) {
				return fmt.Sprintf("Mask32x%d", lanes)
			}
			return ""
		}
	}
	// For other targets (NEON, fallback), use integer vector types
	switch elemType {
	case "float32":
		return fmt.Sprintf("Int32x%d", lanes)
	case "float64":
		return fmt.Sprintf("Int64x%d", lanes)
	case "int32", "uint32":
		return fmt.Sprintf("Int32x%d", lanes)
	case "int64", "uint64":
		return fmt.Sprintf("Int64x%d", lanes)
	default:
		return ""
	}
}

// getVectorTypeName returns the vector type name for archsimd functions.
func getVectorTypeName(elemType string, target Target) string {
	lanes := target.LanesFor(elemType)
	return getVectorTypeNameForLanes(elemType, lanes)
}

// getVectorTypeNameForLanes returns the vector type name for a specific lane count.
func getVectorTypeNameForLanes(elemType string, lanes int) string {
	switch elemType {
	case "float32":
		return fmt.Sprintf("Float32x%d", lanes)
	case "float64":
		return fmt.Sprintf("Float64x%d", lanes)
	case "int32":
		return fmt.Sprintf("Int32x%d", lanes)
	case "int64":
		return fmt.Sprintf("Int64x%d", lanes)
	case "uint8":
		return fmt.Sprintf("Uint8x%d", lanes)
	case "uint16":
		return fmt.Sprintf("Uint16x%d", lanes)
	case "uint32":
		return fmt.Sprintf("Uint32x%d", lanes)
	case "uint64":
		return fmt.Sprintf("Uint64x%d", lanes)
	default:
		return "Vec"
	}
}

// getSliceSize extracts the size from a slice expression like data[:16], data[0:16], or data[1:17].
// Returns 0 if the size cannot be determined.
func getSliceSize(expr ast.Expr) int {
	sliceExpr, ok := expr.(*ast.SliceExpr)
	if !ok {
		return 0
	}
	// Need a high bound to determine size
	if sliceExpr.High == nil {
		return 0
	}
	highLit, ok := sliceExpr.High.(*ast.BasicLit)
	if !ok || highLit.Kind != token.INT {
		return 0
	}
	high, err := strconv.Atoi(highLit.Value)
	if err != nil {
		return 0
	}
	// If there's a low bound, subtract it from high to get actual size
	// For [:N] or [0:N], size is N
	// For [1:17], size is 17-1=16
	low := 0
	if sliceExpr.Low != nil {
		lowLit, ok := sliceExpr.Low.(*ast.BasicLit)
		if !ok || lowLit.Kind != token.INT {
			return 0 // Non-literal low bound, can't determine effective size
		}
		low, err = strconv.Atoi(lowLit.Value)
		if err != nil {
			return 0
		}
	}
	return high - low
}

// elemTypeSize returns the size in bytes of an element type.
func elemTypeSize(elemType string) int {
	switch elemType {
	case "float32", "int32", "uint32":
		return 4
	case "float64", "int64", "uint64":
		return 8
	case "uint8", "int8":
		return 1
	case "uint16", "int16":
		return 2
	default:
		return 0
	}
}

// getVectorTypeNameForInt returns the vector type name for int types used
// in float operations. The lane count matches the parent element type.
// For example, int32 in a float64 function needs Int32x2 (matching Float64x2 lanes).
func getVectorTypeNameForInt(intType, parentElemType string, target Target) string {
	if intType != "int32" && intType != "int64" {
		// Non-integer type, use regular logic
		return getVectorTypeName(intType, target)
	}

	// Match lanes to parent element type
	lanes := target.LanesFor(parentElemType)
	switch intType {
	case "int32":
		return fmt.Sprintf("Int32x%d", lanes)
	case "int64":
		return fmt.Sprintf("Int64x%d", lanes)
	default:
		return getVectorTypeName(intType, target)
	}
}

// parseTypeExpr converts a type string back to an AST expression.
func parseTypeExpr(typeStr string) ast.Expr {
	// Handle slice types
	if strings.HasPrefix(typeStr, "[]") {
		return &ast.ArrayType{
			Elt: parseTypeExpr(typeStr[2:]),
		}
	}

	// Handle array types like [4]uint32 or [16]uint8
	// Must check before generic types since both use brackets
	if strings.HasPrefix(typeStr, "[") {
		closeBracket := strings.Index(typeStr, "]")
		if closeBracket > 0 {
			sizeStr := typeStr[1:closeBracket]
			elemType := typeStr[closeBracket+1:]
			// Check if it's an array type (size is a number) vs generic (size is a type)
			if _, err := strconv.Atoi(sizeStr); err == nil {
				return &ast.ArrayType{
					Len: &ast.BasicLit{Kind: token.INT, Value: sizeStr},
					Elt: parseTypeExpr(elemType),
				}
			}
		}
	}

	// Handle pointer types
	if strings.HasPrefix(typeStr, "*") {
		return &ast.StarExpr{
			X: parseTypeExpr(typeStr[1:]),
		}
	}

	// Handle function types like func(archsimd.Float32x8) archsimd.Float32x8
	if strings.HasPrefix(typeStr, "func(") {
		return parseFuncType(typeStr)
	}

	// Handle generic types like hwy.Vec[float32] or Vec[float32]
	if bracketIdx := strings.Index(typeStr, "["); bracketIdx >= 0 {
		closeBracket := strings.LastIndex(typeStr, "]")
		if closeBracket > bracketIdx {
			baseType := typeStr[:bracketIdx]
			typeArg := typeStr[bracketIdx+1 : closeBracket]

			// Parse the base type (could be pkg.Type or just Type)
			baseExpr := parseTypeExpr(baseType)

			// Create IndexExpr for the generic instantiation
			return &ast.IndexExpr{
				X:     baseExpr,
				Index: parseTypeExpr(typeArg),
			}
		}
	}

	// Handle qualified names (pkg.Type)
	if before, after, ok := strings.Cut(typeStr, "."); ok {
		return &ast.SelectorExpr{
			X:   ast.NewIdent(before),
			Sel: ast.NewIdent(after),
		}
	}

	// Simple identifier
	return ast.NewIdent(typeStr)
}

// parseFuncType parses a function type string like "func(archsimd.Float32x8) archsimd.Float32x8"
func parseFuncType(typeStr string) *ast.FuncType {
	// Find the matching closing paren for the params
	parenDepth := 0
	paramsEnd := -1
	for i := 5; i < len(typeStr); i++ { // Start after "func("
		switch typeStr[i] {
		case '(':
			parenDepth++
		case ')':
			if parenDepth == 0 {
				paramsEnd = i
				break
			}
			parenDepth--
		}
		if paramsEnd >= 0 {
			break
		}
	}
	if paramsEnd < 0 {
		// Malformed, return empty func type
		return &ast.FuncType{}
	}

	// Extract params string (between "func(" and ")")
	paramsStr := typeStr[5:paramsEnd]

	// Extract results string (after ")")
	resultsStr := strings.TrimSpace(typeStr[paramsEnd+1:])
	// Remove surrounding parens from results if present
	if strings.HasPrefix(resultsStr, "(") && strings.HasSuffix(resultsStr, ")") {
		resultsStr = resultsStr[1 : len(resultsStr)-1]
	}

	// Parse params
	var params []*ast.Field
	if paramsStr != "" {
		for _, paramType := range splitTypeList(paramsStr) {
			paramType = strings.TrimSpace(paramType)
			if paramType != "" {
				params = append(params, &ast.Field{
					Type: parseTypeExpr(paramType),
				})
			}
		}
	}

	// Parse results
	var results []*ast.Field
	if resultsStr != "" {
		for _, resultType := range splitTypeList(resultsStr) {
			resultType = strings.TrimSpace(resultType)
			if resultType != "" {
				results = append(results, &ast.Field{
					Type: parseTypeExpr(resultType),
				})
			}
		}
	}

	funcType := &ast.FuncType{
		Params: &ast.FieldList{List: params},
	}
	if len(results) > 0 {
		funcType.Results = &ast.FieldList{List: results}
	}
	return funcType
}

// splitTypeList splits a comma-separated type list, respecting nested brackets and parens.
func splitTypeList(s string) []string {
	var parts []string
	depth := 0
	start := 0
	for i, c := range s {
		switch c {
		case '(', '[':
			depth++
		case ')', ']':
			depth--
		case ',':
			if depth == 0 {
				parts = append(parts, s[start:i])
				start = i + 1
			}
		}
	}
	if start < len(s) {
		parts = append(parts, s[start:])
	}
	return parts
}

// cloneBlockStmt creates a deep copy of a block statement.
func cloneBlockStmt(block *ast.BlockStmt) *ast.BlockStmt {
	if block == nil {
		return nil
	}

	newBlock := &ast.BlockStmt{
		List: make([]ast.Stmt, len(block.List)),
	}

	for i, stmt := range block.List {
		newBlock.List[i] = cloneStmt(stmt)
	}

	return newBlock
}

// cloneStmt creates a deep copy of a statement.
func cloneStmt(stmt ast.Stmt) ast.Stmt {
	if stmt == nil {
		return nil
	}

	switch s := stmt.(type) {
	case *ast.ExprStmt:
		return &ast.ExprStmt{X: cloneExpr(s.X)}
	case *ast.AssignStmt:
		return cloneAssignStmt(s)
	case *ast.DeclStmt:
		return &ast.DeclStmt{Decl: cloneDecl(s.Decl)}
	case *ast.ReturnStmt:
		return cloneReturnStmt(s)
	case *ast.ForStmt:
		return cloneForStmt(s)
	case *ast.IfStmt:
		return cloneIfStmt(s)
	case *ast.IncDecStmt:
		return &ast.IncDecStmt{X: cloneExpr(s.X), Tok: s.Tok}
	case *ast.BranchStmt:
		return &ast.BranchStmt{Tok: s.Tok, Label: s.Label}
	case *ast.BlockStmt:
		return cloneBlockStmt(s)
	case *ast.RangeStmt:
		return &ast.RangeStmt{
			Key:   cloneExpr(s.Key),
			Value: cloneExpr(s.Value),
			Tok:   s.Tok,
			X:     cloneExpr(s.X),
			Body:  cloneBlockStmt(s.Body),
		}
	case *ast.SwitchStmt:
		return &ast.SwitchStmt{
			Init: cloneStmt(s.Init),
			Tag:  cloneExpr(s.Tag),
			Body: cloneBlockStmt(s.Body),
		}
	case *ast.TypeSwitchStmt:
		return &ast.TypeSwitchStmt{
			Init:   cloneStmt(s.Init),
			Assign: cloneStmt(s.Assign),
			Body:   cloneBlockStmt(s.Body),
		}
	case *ast.CaseClause:
		// For default clause, List is nil; preserve that
		var exprs []ast.Expr
		if len(s.List) > 0 {
			exprs = make([]ast.Expr, len(s.List))
			for i, e := range s.List {
				exprs[i] = cloneExpr(e)
			}
		}
		stmts := make([]ast.Stmt, len(s.Body))
		for i, st := range s.Body {
			stmts[i] = cloneStmt(st)
		}
		return &ast.CaseClause{
			List: exprs,
			Body: stmts,
		}
	default:
		// For other statement types, return as-is
		return stmt
	}
}

// genPowIIFE generates an IIFE that computes element-wise Pow for AVX promoted half-precision types.
// It generates: func() asm.Type { var _powBase, _powExp [N]float32; base.AsFloat32xN().StoreSlice(_powBase[:]); ...; return asm.TypeFromFloat32xN(archsimd.LoadFloat32xNSlice(_powBase[:])) }
func genPowIIFE(asmType, wrapFunc, loadFunc, asF32Method, vecPkg, lanesStr string, baseArg, expArg ast.Expr) *ast.FuncLit {
	// base.AsFloat32xN().StoreSlice(_powBase[:])
	storeBase := &ast.ExprStmt{X: &ast.CallExpr{
		Fun: &ast.SelectorExpr{
			X: &ast.CallExpr{
				Fun: &ast.SelectorExpr{X: cloneExpr(baseArg), Sel: ast.NewIdent(asF32Method)},
			},
			Sel: ast.NewIdent("StoreSlice"),
		},
		Args: []ast.Expr{&ast.SliceExpr{X: ast.NewIdent("_powBase")}},
	}}
	// exp.AsFloat32xN().StoreSlice(_powExp[:])
	storeExp := &ast.ExprStmt{X: &ast.CallExpr{
		Fun: &ast.SelectorExpr{
			X: &ast.CallExpr{
				Fun: &ast.SelectorExpr{X: cloneExpr(expArg), Sel: ast.NewIdent(asF32Method)},
			},
			Sel: ast.NewIdent("StoreSlice"),
		},
		Args: []ast.Expr{&ast.SliceExpr{X: ast.NewIdent("_powExp")}},
	}}

	stmts := []ast.Stmt{
		// var _powBase, _powExp [N]float32
		&ast.DeclStmt{Decl: &ast.GenDecl{Tok: token.VAR, Specs: []ast.Spec{
			&ast.ValueSpec{
				Names: []*ast.Ident{ast.NewIdent("_powBase"), ast.NewIdent("_powExp")},
				Type: &ast.ArrayType{
					Len: &ast.BasicLit{Kind: token.INT, Value: lanesStr},
					Elt: ast.NewIdent("float32"),
				},
			},
		}}},
		storeBase,
		storeExp,
		// for _powI := range _powBase { _powBase[_powI] = float32(math.Pow(...)) }
		&ast.RangeStmt{
			Key: ast.NewIdent("_powI"), Tok: token.DEFINE, X: ast.NewIdent("_powBase"),
			Body: &ast.BlockStmt{List: []ast.Stmt{
				&ast.AssignStmt{
					Lhs: []ast.Expr{&ast.IndexExpr{X: ast.NewIdent("_powBase"), Index: ast.NewIdent("_powI")}},
					Tok: token.ASSIGN,
					Rhs: []ast.Expr{&ast.CallExpr{Fun: ast.NewIdent("float32"), Args: []ast.Expr{
						&ast.CallExpr{
							Fun: &ast.SelectorExpr{X: ast.NewIdent("stdmath"), Sel: ast.NewIdent("Pow")},
							Args: []ast.Expr{
								&ast.CallExpr{Fun: ast.NewIdent("float64"), Args: []ast.Expr{&ast.IndexExpr{X: ast.NewIdent("_powBase"), Index: ast.NewIdent("_powI")}}},
								&ast.CallExpr{Fun: ast.NewIdent("float64"), Args: []ast.Expr{&ast.IndexExpr{X: ast.NewIdent("_powExp"), Index: ast.NewIdent("_powI")}}},
							},
						},
					}}},
				},
			}},
		},
		// return asm.TypeFromFloat32xN(archsimd.LoadFloat32xNSlice(_powBase[:]))
		&ast.ReturnStmt{Results: []ast.Expr{
			&ast.CallExpr{
				Fun: &ast.SelectorExpr{X: ast.NewIdent("asm"), Sel: ast.NewIdent(wrapFunc)},
				Args: []ast.Expr{&ast.CallExpr{
					Fun:  &ast.SelectorExpr{X: ast.NewIdent(vecPkg), Sel: ast.NewIdent(loadFunc)},
					Args: []ast.Expr{&ast.SliceExpr{X: ast.NewIdent("_powBase")}},
				}},
			},
		}},
	}

	return &ast.FuncLit{
		Type: &ast.FuncType{
			Results: &ast.FieldList{List: []*ast.Field{{
				Type: &ast.SelectorExpr{X: ast.NewIdent("asm"), Sel: ast.NewIdent(asmType)},
			}}},
		},
		Body: &ast.BlockStmt{List: stmts},
	}
}

// cloneExpr creates a deep copy of an expression.
func cloneExpr(expr ast.Expr) ast.Expr {
	return cloneExprWithDepth(expr, 0)
}

const maxCloneDepth = 1000

func cloneExprWithDepth(expr ast.Expr, depth int) ast.Expr {
	if expr == nil {
		return nil
	}
	if depth > maxCloneDepth {
		panic(fmt.Sprintf("cloneExpr: max depth %d exceeded, expression type: %T", maxCloneDepth, expr))
	}

	switch e := expr.(type) {
	case *ast.Ident:
		return &ast.Ident{Name: e.Name}
	case *ast.BasicLit:
		return &ast.BasicLit{Kind: e.Kind, Value: e.Value}
	case *ast.SelectorExpr:
		return &ast.SelectorExpr{
			X:   cloneExprWithDepth(e.X, depth+1),
			Sel: ast.NewIdent(e.Sel.Name),
		}
	case *ast.CallExpr:
		args := make([]ast.Expr, len(e.Args))
		for i, arg := range e.Args {
			args[i] = cloneExprWithDepth(arg, depth+1)
		}
		return &ast.CallExpr{
			Fun:      cloneExprWithDepth(e.Fun, depth+1),
			Args:     args,
			Ellipsis: e.Ellipsis,
		}
	case *ast.BinaryExpr:
		return &ast.BinaryExpr{
			X:  cloneExprWithDepth(e.X, depth+1),
			Op: e.Op,
			Y:  cloneExprWithDepth(e.Y, depth+1),
		}
	case *ast.UnaryExpr:
		return &ast.UnaryExpr{
			Op: e.Op,
			X:  cloneExprWithDepth(e.X, depth+1),
		}
	case *ast.ParenExpr:
		return &ast.ParenExpr{X: cloneExprWithDepth(e.X, depth+1)}
	case *ast.IndexExpr:
		return &ast.IndexExpr{
			X:     cloneExprWithDepth(e.X, depth+1),
			Index: cloneExprWithDepth(e.Index, depth+1),
		}
	case *ast.SliceExpr:
		return &ast.SliceExpr{
			X:      cloneExprWithDepth(e.X, depth+1),
			Low:    cloneExprWithDepth(e.Low, depth+1),
			High:   cloneExprWithDepth(e.High, depth+1),
			Max:    cloneExprWithDepth(e.Max, depth+1),
			Slice3: e.Slice3,
		}
	case *ast.StarExpr:
		return &ast.StarExpr{X: cloneExprWithDepth(e.X, depth+1)}
	case *ast.TypeAssertExpr:
		return &ast.TypeAssertExpr{
			X:    cloneExprWithDepth(e.X, depth+1),
			Type: cloneExprWithDepth(e.Type, depth+1),
		}
	case *ast.ArrayType:
		return &ast.ArrayType{
			Len: cloneExprWithDepth(e.Len, depth+1),
			Elt: cloneExprWithDepth(e.Elt, depth+1),
		}
	case *ast.CompositeLit:
		elts := make([]ast.Expr, len(e.Elts))
		for i, elt := range e.Elts {
			elts[i] = cloneExprWithDepth(elt, depth+1)
		}
		return &ast.CompositeLit{
			Type: cloneExprWithDepth(e.Type, depth+1),
			Elts: elts,
		}
	default:
		// For unsupported types, return as-is (may cause issues for complex expressions)
		return expr
	}
}

// cloneAssignStmt clones an assignment statement.
func cloneAssignStmt(stmt *ast.AssignStmt) *ast.AssignStmt {
	newStmt := &ast.AssignStmt{
		Lhs: make([]ast.Expr, len(stmt.Lhs)),
		Rhs: make([]ast.Expr, len(stmt.Rhs)),
		Tok: stmt.Tok,
	}
	for i, lhs := range stmt.Lhs {
		newStmt.Lhs[i] = cloneExpr(lhs)
	}
	for i, rhs := range stmt.Rhs {
		newStmt.Rhs[i] = cloneExpr(rhs)
	}
	return newStmt
}

// cloneDecl clones a declaration.
func cloneDecl(decl ast.Decl) ast.Decl {
	if decl == nil {
		return nil
	}

	switch d := decl.(type) {
	case *ast.GenDecl:
		newSpecs := make([]ast.Spec, len(d.Specs))
		for i, spec := range d.Specs {
			newSpecs[i] = cloneSpec(spec)
		}
		return &ast.GenDecl{
			Tok:   d.Tok,
			Specs: newSpecs,
		}
	default:
		return decl
	}
}

// cloneSpec clones a declaration spec (e.g., variable declaration).
func cloneSpec(spec ast.Spec) ast.Spec {
	if spec == nil {
		return nil
	}

	switch s := spec.(type) {
	case *ast.ValueSpec:
		var newValues []ast.Expr
		if len(s.Values) > 0 {
			newValues = make([]ast.Expr, len(s.Values))
			for i, v := range s.Values {
				newValues[i] = cloneExpr(v)
			}
		}
		newNames := make([]*ast.Ident, len(s.Names))
		for i, n := range s.Names {
			newNames[i] = &ast.Ident{Name: n.Name}
		}
		return &ast.ValueSpec{
			Names:  newNames,
			Type:   cloneExpr(s.Type),
			Values: newValues,
		}
	default:
		return spec
	}
}

// cloneReturnStmt clones a return statement.
func cloneReturnStmt(stmt *ast.ReturnStmt) *ast.ReturnStmt {
	newStmt := &ast.ReturnStmt{
		Results: make([]ast.Expr, len(stmt.Results)),
	}
	for i, result := range stmt.Results {
		newStmt.Results[i] = cloneExpr(result)
	}
	return newStmt
}

// cloneForStmt clones a for loop.
func cloneForStmt(stmt *ast.ForStmt) *ast.ForStmt {
	return &ast.ForStmt{
		Init: cloneStmt(stmt.Init),
		Cond: cloneExpr(stmt.Cond),
		Post: cloneStmt(stmt.Post),
		Body: cloneBlockStmt(stmt.Body),
	}
}

// cloneIfStmt clones an if statement.
func cloneIfStmt(stmt *ast.IfStmt) *ast.IfStmt {
	return &ast.IfStmt{
		Init: cloneStmt(stmt.Init),
		Cond: cloneExpr(stmt.Cond),
		Body: cloneBlockStmt(stmt.Body),
		Else: cloneStmt(stmt.Else),
	}
}

// buildResults builds the return type list for a function.
func (pf *ParsedFunc) buildResults(elemType string) *ast.FieldList {
	if len(pf.Returns) == 0 {
		return nil
	}

	fieldList := &ast.FieldList{
		List: make([]*ast.Field, 0, len(pf.Returns)),
	}

	for _, ret := range pf.Returns {
		retType := specializeType(ret.Type, pf.TypeParams, elemType)
		field := &ast.Field{
			Type: parseTypeExpr(retType),
		}
		if ret.Name != "" {
			field.Names = []*ast.Ident{ast.NewIdent(ret.Name)}
		}
		fieldList.List = append(fieldList.List, field)
	}

	return fieldList
}

// buildResultsWithTarget builds the return type list with target-specific Vec types.
func (pf *ParsedFunc) buildResultsWithTarget(elemType string, target Target, skipHalfPrec ...bool) *ast.FieldList {
	if len(pf.Returns) == 0 {
		return nil
	}

	skip := len(skipHalfPrec) > 0 && skipHalfPrec[0]
	fieldList := &ast.FieldList{
		List: make([]*ast.Field, 0, len(pf.Returns)),
	}

	for _, ret := range pf.Returns {
		retType := specializeType(ret.Type, pf.TypeParams, elemType)
		// Transform hwy.Vec[T] to concrete vector types for SIMD targets
		retType = specializeVecType(retType, elemType, target, skip)
		field := &ast.Field{
			Type: parseTypeExpr(retType),
		}
		if ret.Name != "" {
			field.Names = []*ast.Ident{ast.NewIdent(ret.Name)}
		}
		fieldList.List = append(fieldList.List, field)
	}

	return fieldList
}

// postProcessSIMD walks the AST and replaces NumLanes() calls with constants
// and transforms ReduceSum() calls to store+sum patterns.
func postProcessSIMD(node ast.Node, ctx *transformContext) {
	if node == nil {
		return
	}

	defaultLanes := ctx.target.LanesFor(ctx.elemType)
	vecTypeName := getVectorTypeName(ctx.elemType, ctx.target)

	// Walk all statements and expressions, replacing as needed
	ast.Inspect(node, func(n ast.Node) bool {
		switch stmt := n.(type) {
		case *ast.IfStmt:
			// Replace comparisons like: remaining >= v.NumLanes()
			if binExpr, ok := stmt.Cond.(*ast.BinaryExpr); ok {
				replaceNumLanesInExpr(binExpr, defaultLanes, ctx)
			}
		case *ast.AssignStmt:
			// Replace: sum += v.ReduceSum() or sum += hwy.ReduceSum(v)
			// Skip for Float16/BFloat16 - hwy.Vec doesn't have StoreSlice(),
			// and hwy.ReduceSumF16/BF16 work directly.
			// Also skip if target has native ReduceSum support (e.g., NEON has v.ReduceSum() method)
			hasNativeReduceSum := false
			if opInfo, ok := ctx.target.OpMap["ReduceSum"]; ok {
				// Native if it's a method with no package prefix (direct method on vector type)
				hasNativeReduceSum = opInfo.Package == "" && opInfo.IsMethod
			}
			if !isHalfPrecisionType(ctx.elemType) && !hasNativeReduceSum {
				for i, rhs := range stmt.Rhs {
					if call, ok := rhs.(*ast.CallExpr); ok {
						if isReduceSumCall(call) {
							// Transform to store + sum pattern
							stmt.Rhs[i] = createReduceSumExpr(call, defaultLanes, vecTypeName, ctx.elemType)
						}
					}
				}
			}
		case *ast.ExprStmt:
			// Handle standalone expressions if needed
		}
		return true
	})
}

// replaceNumLanesInExpr replaces v.NumLanes() with a constant in a binary expression.
// It uses the context to look up the actual element type of vector variables.
func replaceNumLanesInExpr(binExpr *ast.BinaryExpr, defaultLanes int, ctx *transformContext) {
	// Check RHS
	if call, ok := binExpr.Y.(*ast.CallExpr); ok {
		if lanes := getLanesForNumLanesCall(call, defaultLanes, ctx); lanes > 0 {
			binExpr.Y = &ast.BasicLit{
				Kind:  token.INT,
				Value: strconv.Itoa(lanes),
			}
		}
	}
	// Check LHS (less common but possible)
	if call, ok := binExpr.X.(*ast.CallExpr); ok {
		if lanes := getLanesForNumLanesCall(call, defaultLanes, ctx); lanes > 0 {
			binExpr.X = &ast.BasicLit{
				Kind:  token.INT,
				Value: strconv.Itoa(lanes),
			}
		}
	}
}

// getLanesForNumLanesCall returns the lane count for a NumLanes() call,
// taking into account the actual element type of the vector variable.
// Returns 0 if the call is not a NumLanes call.
func getLanesForNumLanesCall(call *ast.CallExpr, defaultLanes int, ctx *transformContext) int {
	sel, ok := call.Fun.(*ast.SelectorExpr)
	if !ok {
		return 0
	}
	if sel.Sel.Name != "NumLanes" && sel.Sel.Name != "NumElements" {
		return 0
	}
	// Try to look up the element type of the vector variable
	if varIdent, ok := sel.X.(*ast.Ident); ok {
		if varElemType, ok := ctx.varVecElemType[varIdent.Name]; ok {
			return ctx.target.LanesFor(varElemType)
		}
	}
	return defaultLanes
}

// isReduceSumCall checks if a call expression is v.ReduceSum(), hwy.ReduceSum(v),
// or the F16/BF16 variants (ReduceSumF16, ReduceSumBF16).
func isReduceSumCall(call *ast.CallExpr) bool {
	sel, ok := call.Fun.(*ast.SelectorExpr)
	if !ok {
		return false
	}
	name := sel.Sel.Name
	return name == "ReduceSum" || name == "ReduceSumF16" || name == "ReduceSumBF16" ||
		name == "ReduceMin" || name == "ReduceMax"
}

// createReduceSumExpr creates an expression that stores the vector and sums elements.
// For now, we generate a function call that we'll define in a helper.
// Actually, archsimd vectors don't have a built-in ReduceSum, so we need to
// generate inline code that stores to temp and sums.
// Since we can't inject statements here, we'll generate a compound expression.
func createReduceSumExpr(call *ast.CallExpr, lanes int, vecTypeName, elemType string) ast.Expr {
	// Get the vector argument
	var vecExpr ast.Expr
	if sel, ok := call.Fun.(*ast.SelectorExpr); ok {
		if ident, ok := sel.X.(*ast.Ident); ok {
			// Check if it's a package name (hwy, asm) or a vector variable
			// Package names are lowercase and known; vectors are variables
			if ident.Name != "hwy" && ident.Name != "asm" && ident.Name != "archsimd" {
				// It's v.ReduceSum() - the receiver is the vector
				vecExpr = sel.X
			}
		}
	}
	if vecExpr == nil && len(call.Args) > 0 {
		// It's hwy.ReduceSum(v) or hwy.ReduceSumF16(v) - first arg is the vector
		vecExpr = call.Args[0]
	}
	if vecExpr == nil {
		return call // Can't transform, leave as-is
	}

	// Generate a function call to a helper we'll need to add
	// For now, generate inline reduction: func() T { var t [N]T; v.StoreSlice(t[:]); return t[0]+t[1]+... }()
	// This is verbose but works without injecting statements

	// Build: t[0] + t[1] + ... + t[lanes-1]
	var sumExpr ast.Expr
	for i := range lanes {
		indexExpr := &ast.IndexExpr{
			X: ast.NewIdent("_simd_temp"),
			Index: &ast.BasicLit{
				Kind:  token.INT,
				Value: strconv.Itoa(i),
			},
		}
		if sumExpr == nil {
			sumExpr = indexExpr
		} else {
			sumExpr = &ast.BinaryExpr{
				X:  sumExpr,
				Op: token.ADD,
				Y:  indexExpr,
			}
		}
	}

	// Build the function literal:
	// func() elemType {
	//     var _simd_temp [lanes]elemType
	//     vec.StoreSlice(_simd_temp[:])
	//     return t[0] + t[1] + ...
	// }()
	funcLit := &ast.FuncLit{
		Type: &ast.FuncType{
			Results: &ast.FieldList{
				List: []*ast.Field{
					{Type: ast.NewIdent(elemType)},
				},
			},
		},
		Body: &ast.BlockStmt{
			List: []ast.Stmt{
				// var _simd_temp [lanes]elemType
				&ast.DeclStmt{
					Decl: &ast.GenDecl{
						Tok: token.VAR,
						Specs: []ast.Spec{
							&ast.ValueSpec{
								Names: []*ast.Ident{ast.NewIdent("_simd_temp")},
								Type: &ast.ArrayType{
									Len: &ast.BasicLit{
										Kind:  token.INT,
										Value: strconv.Itoa(lanes),
									},
									Elt: ast.NewIdent(elemType),
								},
							},
						},
					},
				},
				// vec.StoreSlice(_simd_temp[:])
				&ast.ExprStmt{
					X: &ast.CallExpr{
						Fun: &ast.SelectorExpr{
							X:   vecExpr,
							Sel: ast.NewIdent("StoreSlice"),
						},
						Args: []ast.Expr{
							&ast.SliceExpr{
								X: ast.NewIdent("_simd_temp"),
							},
						},
					},
				},
				// return sum expression
				&ast.ReturnStmt{
					Results: []ast.Expr{sumExpr},
				},
			},
		},
	}

	// Call the function literal immediately
	return &ast.CallExpr{
		Fun: funcLit,
	}
}

// filterConditionalBlocks filters statements based on //hwy:if, //hwy:else, //hwy:endif directives.
// It returns a new BlockStmt with only the statements that match the current target and element type.
// The original AST is not modified.
func filterConditionalBlocks(body *ast.BlockStmt, blocks []ConditionalBlock, fset *token.FileSet, targetName, elemType string) *ast.BlockStmt {
	if body == nil || len(blocks) == 0 {
		return body
	}

	// Create a new block with filtered statements
	newBody := &ast.BlockStmt{
		Lbrace: body.Lbrace,
		Rbrace: body.Rbrace,
	}

	for _, stmt := range body.List {
		// Get the line number of this statement
		stmtLine := fset.Position(stmt.Pos()).Line

		// Check if this statement is within any conditional block
		included := true
		for _, block := range blocks {
			if stmtLine > block.StartLine && stmtLine < block.EndLine {
				// Statement is within this conditional block
				conditionMatches := block.ParsedCondition.Evaluate(targetName, elemType)

				if block.ElseLine > 0 {
					// Block has an else clause
					if stmtLine < block.ElseLine {
						// Statement is in the "if" part
						included = conditionMatches
					} else {
						// Statement is in the "else" part
						included = !conditionMatches
					}
				} else {
					// No else clause - include only if condition matches
					included = conditionMatches
				}
				break // Found the innermost containing block
			}
		}

		if included {
			// Recursively filter nested blocks (e.g., for statements, if statements)
			filteredStmt := filterNestedConditionalBlocks(stmt, blocks, fset, targetName, elemType)
			newBody.List = append(newBody.List, filteredStmt)
		}
	}

	return newBody
}

// filterNestedConditionalBlocks recursively filters conditional blocks within nested statements.
func filterNestedConditionalBlocks(stmt ast.Stmt, blocks []ConditionalBlock, fset *token.FileSet, targetName, elemType string) ast.Stmt {
	switch s := stmt.(type) {
	case *ast.BlockStmt:
		return filterConditionalBlocks(s, blocks, fset, targetName, elemType)
	case *ast.IfStmt:
		newIf := *s // shallow copy
		if s.Body != nil {
			newIf.Body = filterConditionalBlocks(s.Body, blocks, fset, targetName, elemType)
		}
		if s.Else != nil {
			newIf.Else = filterNestedConditionalBlocks(s.Else, blocks, fset, targetName, elemType)
		}
		return &newIf
	case *ast.ForStmt:
		newFor := *s // shallow copy
		if s.Body != nil {
			newFor.Body = filterConditionalBlocks(s.Body, blocks, fset, targetName, elemType)
		}
		return &newFor
	case *ast.RangeStmt:
		newRange := *s // shallow copy
		if s.Body != nil {
			newRange.Body = filterConditionalBlocks(s.Body, blocks, fset, targetName, elemType)
		}
		return &newRange
	case *ast.SwitchStmt:
		newSwitch := *s // shallow copy
		if s.Body != nil {
			newSwitch.Body = filterConditionalBlocks(s.Body, blocks, fset, targetName, elemType)
		}
		return &newSwitch
	case *ast.TypeSwitchStmt:
		newSwitch := *s // shallow copy
		if s.Body != nil {
			newSwitch.Body = filterConditionalBlocks(s.Body, blocks, fset, targetName, elemType)
		}
		return &newSwitch
	case *ast.SelectStmt:
		newSelect := *s // shallow copy
		if s.Body != nil {
			newSelect.Body = filterConditionalBlocks(s.Body, blocks, fset, targetName, elemType)
		}
		return &newSelect
	default:
		return stmt
	}
}

// resolveTypeSpecificConst resolves type-specific constant references.
// It supports two patterns:
//
// Pattern 1 (base name): "expC0" -> "expC0_f32" or "expC0_f64"
//   - Looks up base name in typeSpecificConsts map
//   - Resolves to variant matching target element type
//
// Pattern 2 (suffix swap): "expC0_f32" -> "expC0_f64"
//   - Detects existing type suffix in the name
//   - Swaps to suffix matching target element type
//   - This allows base files to be compilable while hwygen adjusts for other types
func resolveTypeSpecificConst(name string, ctx *transformContext) string {
	targetSuffix := GetTypeSuffix(ctx.elemType)

	// Pattern 1: Check if this is a base name with type-specific variants
	if ctx.typeSpecificConsts != nil {
		if tsc, ok := ctx.typeSpecificConsts[name]; ok {
			if resolved, exists := tsc.Variants[targetSuffix]; exists {
				return resolved
			}
			// Fallback: if no exact match, try f32 for Float16/BFloat16 (compute type)
			if targetSuffix == "f16" || targetSuffix == "bf16" {
				if resolved, exists := tsc.Variants["f32"]; exists {
					return resolved
				}
			}
		}
	}

	// Pattern 2: Check if name already has a type suffix that needs swapping
	for _, suffix := range typeSuffixes {
		if before, ok := strings.CutSuffix(name, suffix); ok {
			// Extract base name and swap suffix
			baseName := before
			newSuffix := "_" + targetSuffix

			// Only swap if target suffix is different
			if suffix != newSuffix {
				return baseName + newSuffix
			}
			return name // Same suffix, no change needed
		}
	}

	return name
}

// transformIdentifiers walks the AST and resolves type-specific constant references
// and type parameter substitutions.
// This handles both Pattern 1 (base name lookup) and Pattern 2 (suffix swapping).
func transformIdentifiers(node ast.Node, ctx *transformContext) {
	if node == nil {
		return
	}

	ast.Inspect(node, func(n ast.Node) bool {
		switch expr := n.(type) {
		case *ast.Ident:
			// First check if it's a type parameter that should be replaced
			for _, tp := range ctx.typeParams {
				if expr.Name == tp.Name {
					expr.Name = ctx.elemType
					return true
				}
			}
			// Otherwise check if it's a constant reference
			resolved := resolveTypeSpecificConst(expr.Name, ctx)
			if resolved != expr.Name {
				expr.Name = resolved
			}
		case *ast.SelectorExpr:
			// Rename math.X to stdmath.X to avoid package name conflict
			// since generated files are in the math package but need stdlib math.
			// Only rename if "math" actually refers to the stdlib "math" import,
			// not a local variable or a different package aliased as "math".
			if ident, ok := expr.X.(*ast.Ident); ok && ident.Name == "math" {
				if importPath, isImport := ctx.imports[ident.Name]; isImport && importPath == "math" {
					ident.Name = "stdmath"
				}
			}
		}
		return true
	})
}

// convertStackArrayUsages converts stack array variable usages to slice expressions.
// For example, if buf is a stack array, convert:
//   - copy(buf, ...) -> copy(buf[:], ...)
//   - archsimd.LoadFloat32x8Slice(buf) -> archsimd.LoadFloat32x8Slice(buf[:])
//   - v.StoreSlice(buf) -> v.StoreSlice(buf[:])
func convertStackArrayUsages(node ast.Node, ctx *transformContext) {
	if node == nil {
		return
	}

	ast.Inspect(node, func(n ast.Node) bool {
		call, ok := n.(*ast.CallExpr)
		if !ok {
			return true
		}

		// Check each argument
		for i, arg := range call.Args {
			// Skip if it's already a slice expression
			if _, ok := arg.(*ast.SliceExpr); ok {
				continue
			}

			// Check if the argument is a stack array variable
			if ident, ok := arg.(*ast.Ident); ok {
				if ctx.stackArrayVars[ident.Name] {
					// Replace buf with buf[:]
					call.Args[i] = &ast.SliceExpr{
						X: ident,
					}
				}
			}
		}

		return true
	})
}

// transformFuncRefArgs transforms function references passed as arguments.
// For example: BaseApply(in, out, math.BaseExpVec)
// The math.BaseExpVec should become math.BaseExpVec_avx2 for SIMD targets,
// or math.BaseExpVec_fallback for fallback targets.
func transformFuncRefArgs(call *ast.CallExpr, ctx *transformContext) {
	for i, arg := range call.Args {
		// Handle package.BaseFuncName (SelectorExpr)
		if sel, ok := arg.(*ast.SelectorExpr); ok {
			if ident, ok := sel.X.(*ast.Ident); ok {
				// Check if it's a contrib package with a Base* function
				switch ident.Name {
				case "math", "vec", "matvec", "matmul", "algo", "image", "bitpack", "sort":
					if strings.HasPrefix(sel.Sel.Name, "Base") {
						// Transform math.BaseExpVec to math.BaseExpVec_avx2
						suffix := ctx.target.Suffix()
						if ctx.elemType == "float64" {
							suffix = suffix + "_Float64"
						} else if isFloat16Type(ctx.elemType) {
							suffix = suffix + "_Float16"
						} else if isBFloat16Type(ctx.elemType) {
							suffix = suffix + "_BFloat16"
						}
						sel.Sel.Name = sel.Sel.Name + suffix
					}
				}
			}
		}

		// Handle package.BaseFuncName[T] (IndexExpr wrapping SelectorExpr)
		if indexExpr, ok := arg.(*ast.IndexExpr); ok {
			if sel, ok := indexExpr.X.(*ast.SelectorExpr); ok {
				if ident, ok := sel.X.(*ast.Ident); ok {
					switch ident.Name {
					case "math", "vec", "matvec", "matmul", "algo", "image", "bitpack", "sort":
						if strings.HasPrefix(sel.Sel.Name, "Base") {
							// Transform to non-generic version with suffix
							suffix := ctx.target.Suffix()
							if ctx.elemType == "float64" {
								suffix = suffix + "_Float64"
							} else if isFloat16Type(ctx.elemType) {
								suffix = suffix + "_Float16"
							} else if isBFloat16Type(ctx.elemType) {
								suffix = suffix + "_BFloat16"
							}
							// Replace the IndexExpr with just the SelectorExpr (strip type param)
							sel.Sel.Name = sel.Sel.Name + suffix
							call.Args[i] = sel
						}
					}
				}
			}
		}

		// Handle local BaseFuncName[T] (IndexExpr wrapping Ident)
		if indexExpr, ok := arg.(*ast.IndexExpr); ok {
			if ident, ok := indexExpr.X.(*ast.Ident); ok {
				if strings.HasPrefix(ident.Name, "Base") {
					// Transform BaseFunc[T] to BaseFunc_avx2
					suffix := ctx.target.Suffix()
					if ctx.elemType == "float64" {
						suffix = suffix + "_Float64"
					} else if isFloat16Type(ctx.elemType) {
						suffix = suffix + "_Float16"
					} else if isBFloat16Type(ctx.elemType) {
						suffix = suffix + "_BFloat16"
					}
					// Replace the IndexExpr with just the Ident
					call.Args[i] = ast.NewIdent(ident.Name + suffix)
				}
			}
		}
	}
}

// hasPredicateParam returns true if the function has a predicate-type parameter
// (i.e., a type parameter with a non-Lanes constraint like Predicate[T]).
func hasPredicateParam(pf *ParsedFunc) bool {
	for _, tp := range pf.TypeParams {
		// Skip element type constraints
		if strings.Contains(tp.Constraint, "Lanes") ||
			strings.Contains(tp.Constraint, "Floats") ||
			strings.Contains(tp.Constraint, "Integers") ||
			strings.Contains(tp.Constraint, "SignedInts") ||
			strings.Contains(tp.Constraint, "UnsignedInts") {
			continue
		}
		// This is likely a predicate or other interface type param
		return true
	}
	return false
}

// generateScalarPredicateBody generates a scalar loop body for predicate functions
// in fallback mode. Returns nil if this function doesn't need scalar generation.
func generateScalarPredicateBody(pf *ParsedFunc, elemType string) *ast.BlockStmt {
	// Map function names to their scalar implementations
	switch pf.Name {
	case "BaseAll":
		return generateScalarAll(pf, elemType)
	case "BaseAny":
		return generateScalarAny(pf, elemType)
	case "BaseNone":
		return generateScalarNone(pf, elemType)
	case "BaseFindIf":
		return generateScalarFindIf(pf, elemType)
	case "BaseCountIf":
		return generateScalarCountIf(pf, elemType)
	default:
		return nil
	}
}

// generateScalarAll generates: for _, v := range slice { if !pred.Test(v) { return false } } return true
func generateScalarAll(pf *ParsedFunc, elemType string) *ast.BlockStmt {
	sliceParam := pf.Params[0].Name
	predParam := pf.Params[1].Name

	return &ast.BlockStmt{
		List: []ast.Stmt{
			// for _, v := range slice { if !pred.Test(v) { return false } }
			&ast.RangeStmt{
				Key:   ast.NewIdent("_"),
				Value: ast.NewIdent("v"),
				Tok:   token.DEFINE,
				X:     ast.NewIdent(sliceParam),
				Body: &ast.BlockStmt{
					List: []ast.Stmt{
						&ast.IfStmt{
							Cond: &ast.UnaryExpr{
								Op: token.NOT,
								X: &ast.CallExpr{
									Fun: &ast.SelectorExpr{
										X:   ast.NewIdent(predParam),
										Sel: ast.NewIdent("Test"),
									},
									Args: []ast.Expr{ast.NewIdent("v")},
								},
							},
							Body: &ast.BlockStmt{
								List: []ast.Stmt{
									&ast.ReturnStmt{
										Results: []ast.Expr{ast.NewIdent("false")},
									},
								},
							},
						},
					},
				},
			},
			// return true
			&ast.ReturnStmt{
				Results: []ast.Expr{ast.NewIdent("true")},
			},
		},
	}
}

// generateScalarAny generates: for _, v := range slice { if pred.Test(v) { return true } } return false
func generateScalarAny(pf *ParsedFunc, elemType string) *ast.BlockStmt {
	sliceParam := pf.Params[0].Name
	predParam := pf.Params[1].Name

	return &ast.BlockStmt{
		List: []ast.Stmt{
			&ast.RangeStmt{
				Key:   ast.NewIdent("_"),
				Value: ast.NewIdent("v"),
				Tok:   token.DEFINE,
				X:     ast.NewIdent(sliceParam),
				Body: &ast.BlockStmt{
					List: []ast.Stmt{
						&ast.IfStmt{
							Cond: &ast.CallExpr{
								Fun: &ast.SelectorExpr{
									X:   ast.NewIdent(predParam),
									Sel: ast.NewIdent("Test"),
								},
								Args: []ast.Expr{ast.NewIdent("v")},
							},
							Body: &ast.BlockStmt{
								List: []ast.Stmt{
									&ast.ReturnStmt{
										Results: []ast.Expr{ast.NewIdent("true")},
									},
								},
							},
						},
					},
				},
			},
			&ast.ReturnStmt{
				Results: []ast.Expr{ast.NewIdent("false")},
			},
		},
	}
}

// generateScalarNone generates: return !BaseAny_fallback...(slice, pred)
func generateScalarNone(pf *ParsedFunc, elemType string) *ast.BlockStmt {
	sliceParam := pf.Params[0].Name
	predParam := pf.Params[1].Name

	// Build the function name: BaseAny_fallback or BaseAny_fallback_Float64, etc.
	funcName := "BaseAny_fallback"
	switch elemType {
	case "float64":
		funcName += "_Float64"
	case "int32":
		funcName += "_Int32"
	case "int64":
		funcName += "_Int64"
	case "uint32":
		funcName += "_Uint32"
	case "uint64":
		funcName += "_Uint64"
	}

	return &ast.BlockStmt{
		List: []ast.Stmt{
			&ast.ReturnStmt{
				Results: []ast.Expr{
					&ast.UnaryExpr{
						Op: token.NOT,
						X: &ast.CallExpr{
							Fun:  ast.NewIdent(funcName),
							Args: []ast.Expr{ast.NewIdent(sliceParam), ast.NewIdent(predParam)},
						},
					},
				},
			},
		},
	}
}

// generateScalarFindIf generates: for i, v := range slice { if pred.Test(v) { return i } } return -1
func generateScalarFindIf(pf *ParsedFunc, elemType string) *ast.BlockStmt {
	sliceParam := pf.Params[0].Name
	predParam := pf.Params[1].Name

	return &ast.BlockStmt{
		List: []ast.Stmt{
			&ast.RangeStmt{
				Key:   ast.NewIdent("i"),
				Value: ast.NewIdent("v"),
				Tok:   token.DEFINE,
				X:     ast.NewIdent(sliceParam),
				Body: &ast.BlockStmt{
					List: []ast.Stmt{
						&ast.IfStmt{
							Cond: &ast.CallExpr{
								Fun: &ast.SelectorExpr{
									X:   ast.NewIdent(predParam),
									Sel: ast.NewIdent("Test"),
								},
								Args: []ast.Expr{ast.NewIdent("v")},
							},
							Body: &ast.BlockStmt{
								List: []ast.Stmt{
									&ast.ReturnStmt{
										Results: []ast.Expr{ast.NewIdent("i")},
									},
								},
							},
						},
					},
				},
			},
			&ast.ReturnStmt{
				Results: []ast.Expr{
					&ast.UnaryExpr{Op: token.SUB, X: &ast.BasicLit{Kind: token.INT, Value: "1"}},
				},
			},
		},
	}
}

// generateScalarCountIf generates: count := 0; for _, v := range slice { if pred.Test(v) { count++ } } return count
func generateScalarCountIf(pf *ParsedFunc, elemType string) *ast.BlockStmt {
	sliceParam := pf.Params[0].Name
	predParam := pf.Params[1].Name

	return &ast.BlockStmt{
		List: []ast.Stmt{
			// count := 0
			&ast.AssignStmt{
				Lhs: []ast.Expr{ast.NewIdent("count")},
				Tok: token.DEFINE,
				Rhs: []ast.Expr{&ast.BasicLit{Kind: token.INT, Value: "0"}},
			},
			// for _, v := range slice { if pred.Test(v) { count++ } }
			&ast.RangeStmt{
				Key:   ast.NewIdent("_"),
				Value: ast.NewIdent("v"),
				Tok:   token.DEFINE,
				X:     ast.NewIdent(sliceParam),
				Body: &ast.BlockStmt{
					List: []ast.Stmt{
						&ast.IfStmt{
							Cond: &ast.CallExpr{
								Fun: &ast.SelectorExpr{
									X:   ast.NewIdent(predParam),
									Sel: ast.NewIdent("Test"),
								},
								Args: []ast.Expr{ast.NewIdent("v")},
							},
							Body: &ast.BlockStmt{
								List: []ast.Stmt{
									&ast.IncDecStmt{
										X:   ast.NewIdent("count"),
										Tok: token.INC,
									},
								},
							},
						},
					},
				},
			},
			// return count
			&ast.ReturnStmt{
				Results: []ast.Expr{ast.NewIdent("count")},
			},
		},
	}
}

// inlineHelperCalls recursively inlines local helper function calls in a block.
// Local helpers are non-Base* functions defined in the same file that use hwy operations.
// This ensures the entire code path gets specialized for each target architecture.
func inlineHelperCalls(block *ast.BlockStmt, ctx *transformContext) {
	if block == nil || ctx.allFuncs == nil {
		return
	}

	// Process statements in reverse order so we can safely replace them
	for i := 0; i < len(block.List); i++ {
		stmt := block.List[i]

		// Check for expression statement that is a helper call
		exprStmt, ok := stmt.(*ast.ExprStmt)
		if !ok {
			// Recursively process nested blocks
			inlineHelperCallsInStmt(stmt, ctx)
			continue
		}

		// Check if this is a call expression
		callExpr, ok := exprStmt.X.(*ast.CallExpr)
		if !ok {
			continue
		}

		// Get the function name being called
		var funcName string
		switch fun := callExpr.Fun.(type) {
		case *ast.Ident:
			funcName = fun.Name
		case *ast.IndexExpr:
			// Generic call like func[T](...)
			if ident, ok := fun.X.(*ast.Ident); ok {
				funcName = ident.Name
			}
		}

		if funcName == "" {
			continue
		}

		// Skip Base* functions - they're handled separately with target suffix
		if hasBasePrefix(funcName) {
			continue
		}

		// Check if this is a local helper we can inline
		helper, exists := ctx.allFuncs[funcName]
		if !exists {
			continue
		}

		// Skip if helper has no hwy operations (pure scalar helper)
		if len(helper.HwyCalls) == 0 && !hasHwyLanesConstraint(helper.TypeParams) {
			continue
		}

		// Inline the helper
		inlinedStmts := inlineHelper(helper, callExpr, ctx)
		if inlinedStmts == nil {
			continue
		}

		// Replace the call statement with the inlined statements
		// Wrap in a BlockStmt to keep variable scope contained
		block.List[i] = &ast.BlockStmt{List: inlinedStmts}
	}
}

// inlineHelperCallsInStmt recursively processes statements to find nested helper calls.
func inlineHelperCallsInStmt(stmt ast.Stmt, ctx *transformContext) {
	if stmt == nil {
		return
	}

	switch s := stmt.(type) {
	case *ast.BlockStmt:
		inlineHelperCalls(s, ctx)
	case *ast.ForStmt:
		inlineHelperCalls(s.Body, ctx)
	case *ast.IfStmt:
		inlineHelperCalls(s.Body, ctx)
		if s.Else != nil {
			if elseBlock, ok := s.Else.(*ast.BlockStmt); ok {
				inlineHelperCalls(elseBlock, ctx)
			} else if elseIf, ok := s.Else.(*ast.IfStmt); ok {
				inlineHelperCallsInStmt(elseIf, ctx)
			}
		}
	case *ast.RangeStmt:
		inlineHelperCalls(s.Body, ctx)
	case *ast.SwitchStmt:
		inlineHelperCalls(s.Body, ctx)
	}
}

// inlineHelper transforms a helper function and returns its body statements with
// parameters substituted with actual arguments.
func inlineHelper(helper *ParsedFunc, call *ast.CallExpr, ctx *transformContext) []ast.Stmt {
	if helper.Body == nil || len(helper.Body.List) == 0 {
		return nil
	}

	// Clone the helper's body to avoid modifying the original
	clonedBody := cloneBlockStmt(helper.Body)

	// Build parameter -> argument mapping
	paramMap := make(map[string]ast.Expr)
	for i, param := range helper.Params {
		if i < len(call.Args) {
			paramMap[param.Name] = call.Args[i]
		}
	}

	// Create a unique suffix for this inline site to avoid variable conflicts
	ctx.inlineCounter++
	suffix := fmt.Sprintf("_%d", ctx.inlineCounter)

	// Collect local variables defined in the helper to rename them
	localVars := make(map[string]bool)
	collectLocalVariablesFromBlock(clonedBody, localVars)

	// Substitute parameters and rename local variables
	substituteAndRename(clonedBody, paramMap, localVars, suffix)

	// Now transform the cloned body for the current target/elemType
	// Create a mini-context for transforming the helper
	helperCtx := &transformContext{
		target:                  ctx.target,
		elemType:                ctx.elemType,
		typeParams:              helper.TypeParams,
		loopInfo:                helper.LoopInfo,
		lanesVars:               make(map[string]bool),
		localVars:               make(map[string]bool),
		stackArrayVars:          make(map[string]bool),
		hoistedConsts:           ctx.hoistedConsts, // Share hoisted consts
		funcName:                ctx.funcName,
		typeSpecificConsts:      ctx.typeSpecificConsts,
		conditionalBlocks:       ctx.conditionalBlocks,
		fset:                    ctx.fset,
		imports:                 ctx.imports,
		varTypes:                make(map[string]string),
		halfPrecisionSlices:     make(map[string]bool),
		halfPrecisionScalarVars: make(map[string]bool),
		varVecLanes:             make(map[string]int),
		varVecElemType:          make(map[string]string),
		allFuncs:                ctx.allFuncs,
		inlineCounter:           ctx.inlineCounter,
	}

	// Copy relevant tracking from parent context
	maps.Copy(helperCtx.halfPrecisionSlices, ctx.halfPrecisionSlices)

	// Transform the helper body - same transformations as the main function
	transformIdentifiers(clonedBody, helperCtx)
	transformNode(clonedBody, helperCtx)

	// Recursively inline any nested helper calls
	inlineHelperCalls(clonedBody, helperCtx)

	// Update parent context's inline counter
	ctx.inlineCounter = helperCtx.inlineCounter

	return clonedBody.List
}

// collectLocalVariablesFromBlock collects all variable names defined in a block.
func collectLocalVariablesFromBlock(block *ast.BlockStmt, vars map[string]bool) {
	if block == nil {
		return
	}

	ast.Inspect(block, func(n ast.Node) bool {
		switch node := n.(type) {
		case *ast.AssignStmt:
			if node.Tok == token.DEFINE {
				for _, lhs := range node.Lhs {
					if ident, ok := lhs.(*ast.Ident); ok {
						vars[ident.Name] = true
					}
				}
			}
		case *ast.DeclStmt:
			if genDecl, ok := node.Decl.(*ast.GenDecl); ok {
				if genDecl.Tok == token.VAR {
					for _, spec := range genDecl.Specs {
						if valueSpec, ok := spec.(*ast.ValueSpec); ok {
							for _, name := range valueSpec.Names {
								vars[name.Name] = true
							}
						}
					}
				}
			}
		case *ast.RangeStmt:
			if ident, ok := node.Key.(*ast.Ident); ok {
				vars[ident.Name] = true
			}
			if ident, ok := node.Value.(*ast.Ident); ok {
				vars[ident.Name] = true
			}
		case *ast.ForStmt:
			if assignStmt, ok := node.Init.(*ast.AssignStmt); ok {
				if assignStmt.Tok == token.DEFINE {
					for _, lhs := range assignStmt.Lhs {
						if ident, ok := lhs.(*ast.Ident); ok {
							vars[ident.Name] = true
						}
					}
				}
			}
		}
		return true
	})
}

// substituteAndRename walks the AST and:
// 1. Renames local variables with a unique suffix to avoid conflicts
// 2. Replaces parameter references with actual argument expressions
func substituteAndRename(block *ast.BlockStmt, paramMap map[string]ast.Expr, localVars map[string]bool, suffix string) {
	// First pass: rename local variables
	ast.Inspect(block, func(n ast.Node) bool {
		if ident, ok := n.(*ast.Ident); ok {
			// Check if this is a local variable - rename with suffix
			if localVars[ident.Name] {
				ident.Name = ident.Name + suffix
			}
		}
		return true
	})

	// Second pass: perform parameter substitution
	substituteParams(block, paramMap)
}

// substituteParams replaces parameter identifiers with their argument expressions.
// It uses a post-order traversal approach to avoid visiting newly-inserted nodes,
// which could cause infinite expansion if replacement expressions contain identifiers
// that match parameter names.
func substituteParams(node ast.Node, paramMap map[string]ast.Expr) {
	substituteParamsPostOrder(node, paramMap)
}

// substituteParamsPostOrder does a depth-first post-order traversal,
// processing children before parents to avoid re-visiting modified nodes.
func substituteParamsPostOrder(node ast.Node, paramMap map[string]ast.Expr) {
	if node == nil {
		return
	}

	// First, recursively process all children
	switch n := node.(type) {
	case *ast.BlockStmt:
		for _, stmt := range n.List {
			substituteParamsPostOrder(stmt, paramMap)
		}
	case *ast.ExprStmt:
		substituteParamsPostOrder(n.X, paramMap)
	case *ast.AssignStmt:
		for _, expr := range n.Lhs {
			substituteParamsPostOrder(expr, paramMap)
		}
		for _, expr := range n.Rhs {
			substituteParamsPostOrder(expr, paramMap)
		}
	case *ast.DeclStmt:
		substituteParamsPostOrder(n.Decl, paramMap)
	case *ast.GenDecl:
		for _, spec := range n.Specs {
			substituteParamsPostOrder(spec, paramMap)
		}
	case *ast.ValueSpec:
		for _, val := range n.Values {
			substituteParamsPostOrder(val, paramMap)
		}
	case *ast.IfStmt:
		substituteParamsPostOrder(n.Init, paramMap)
		substituteParamsPostOrder(n.Cond, paramMap)
		substituteParamsPostOrder(n.Body, paramMap)
		substituteParamsPostOrder(n.Else, paramMap)
	case *ast.ForStmt:
		substituteParamsPostOrder(n.Init, paramMap)
		substituteParamsPostOrder(n.Cond, paramMap)
		substituteParamsPostOrder(n.Post, paramMap)
		substituteParamsPostOrder(n.Body, paramMap)
	case *ast.RangeStmt:
		substituteParamsPostOrder(n.Key, paramMap)
		substituteParamsPostOrder(n.Value, paramMap)
		substituteParamsPostOrder(n.X, paramMap)
		substituteParamsPostOrder(n.Body, paramMap)
	case *ast.ReturnStmt:
		for _, expr := range n.Results {
			substituteParamsPostOrder(expr, paramMap)
		}
	case *ast.IncDecStmt:
		substituteParamsPostOrder(n.X, paramMap)
	case *ast.SwitchStmt:
		substituteParamsPostOrder(n.Init, paramMap)
		substituteParamsPostOrder(n.Tag, paramMap)
		substituteParamsPostOrder(n.Body, paramMap)
	case *ast.TypeSwitchStmt:
		substituteParamsPostOrder(n.Init, paramMap)
		substituteParamsPostOrder(n.Assign, paramMap)
		substituteParamsPostOrder(n.Body, paramMap)
	case *ast.CaseClause:
		for _, expr := range n.List {
			substituteParamsPostOrder(expr, paramMap)
		}
		for _, stmt := range n.Body {
			substituteParamsPostOrder(stmt, paramMap)
		}
	case *ast.BranchStmt:
		// nothing to recurse into
	case *ast.CallExpr:
		substituteParamsPostOrder(n.Fun, paramMap)
		for _, arg := range n.Args {
			substituteParamsPostOrder(arg, paramMap)
		}
	case *ast.BinaryExpr:
		substituteParamsPostOrder(n.X, paramMap)
		substituteParamsPostOrder(n.Y, paramMap)
	case *ast.UnaryExpr:
		substituteParamsPostOrder(n.X, paramMap)
	case *ast.IndexExpr:
		substituteParamsPostOrder(n.X, paramMap)
		substituteParamsPostOrder(n.Index, paramMap)
	case *ast.SliceExpr:
		substituteParamsPostOrder(n.X, paramMap)
		substituteParamsPostOrder(n.Low, paramMap)
		substituteParamsPostOrder(n.High, paramMap)
		substituteParamsPostOrder(n.Max, paramMap)
	case *ast.SelectorExpr:
		substituteParamsPostOrder(n.X, paramMap)
	case *ast.ParenExpr:
		substituteParamsPostOrder(n.X, paramMap)
	case *ast.StarExpr:
		substituteParamsPostOrder(n.X, paramMap)
	case *ast.CompositeLit:
		for _, elt := range n.Elts {
			substituteParamsPostOrder(elt, paramMap)
		}
	case *ast.KeyValueExpr:
		substituteParamsPostOrder(n.Key, paramMap)
		substituteParamsPostOrder(n.Value, paramMap)
	case *ast.TypeAssertExpr:
		substituteParamsPostOrder(n.X, paramMap)
	case *ast.Ident, *ast.BasicLit:
		// leaf nodes, nothing to recurse into
	}

	// Now, perform substitutions at this node level (post-order)
	switch parent := node.(type) {
	case *ast.CallExpr:
		for i, arg := range parent.Args {
			if ident, ok := arg.(*ast.Ident); ok {
				if replacement, isParam := paramMap[ident.Name]; isParam {
					parent.Args[i] = cloneExpr(replacement)
				}
			}
		}
	case *ast.BinaryExpr:
		if ident, ok := parent.X.(*ast.Ident); ok {
			if replacement, isParam := paramMap[ident.Name]; isParam {
				parent.X = cloneExpr(replacement)
			}
		}
		if ident, ok := parent.Y.(*ast.Ident); ok {
			if replacement, isParam := paramMap[ident.Name]; isParam {
				parent.Y = cloneExpr(replacement)
			}
		}
	case *ast.IndexExpr:
		if ident, ok := parent.X.(*ast.Ident); ok {
			if replacement, isParam := paramMap[ident.Name]; isParam {
				parent.X = cloneExpr(replacement)
			}
		}
		if ident, ok := parent.Index.(*ast.Ident); ok {
			if replacement, isParam := paramMap[ident.Name]; isParam {
				parent.Index = cloneExpr(replacement)
			}
		}
	case *ast.SliceExpr:
		if ident, ok := parent.X.(*ast.Ident); ok {
			if replacement, isParam := paramMap[ident.Name]; isParam {
				parent.X = cloneExpr(replacement)
			}
		}
		if ident, ok := parent.Low.(*ast.Ident); ok {
			if replacement, isParam := paramMap[ident.Name]; isParam {
				parent.Low = cloneExpr(replacement)
			}
		}
		if ident, ok := parent.High.(*ast.Ident); ok {
			if replacement, isParam := paramMap[ident.Name]; isParam {
				parent.High = cloneExpr(replacement)
			}
		}
	case *ast.UnaryExpr:
		if ident, ok := parent.X.(*ast.Ident); ok {
			if replacement, isParam := paramMap[ident.Name]; isParam {
				parent.X = cloneExpr(replacement)
			}
		}
	case *ast.StarExpr:
		// Handle pointer dereference: *bitPos where bitPos is a pointer parameter
		if ident, ok := parent.X.(*ast.Ident); ok {
			if replacement, isParam := paramMap[ident.Name]; isParam {
				parent.X = cloneExpr(replacement)
			}
		}
	case *ast.AssignStmt:
		for i, rhs := range parent.Rhs {
			if ident, ok := rhs.(*ast.Ident); ok {
				if replacement, isParam := paramMap[ident.Name]; isParam {
					parent.Rhs[i] = cloneExpr(replacement)
				}
			}
		}
	case *ast.ReturnStmt:
		for i, result := range parent.Results {
			if ident, ok := result.(*ast.Ident); ok {
				if replacement, isParam := paramMap[ident.Name]; isParam {
					parent.Results[i] = cloneExpr(replacement)
				}
			}
		}
	}
}
