package main

import (
	"bytes"
	"fmt"
	"go/ast"
	"os"
	"path/filepath"
	"strings"
)

// CEmitter generates C code with SIMD intrinsics from Go hwy.* functions.
// The generated C code can be compiled with GOAT to produce Go assembly.
// Supports NEON, AVX2, and AVX-512 targets with f16/bf16/f32/f64 element types.
type CEmitter struct {
	pkgName  string
	elemType string // "float32", "float64", "hwy.Float16", "hwy.BFloat16"
	target   Target
	profile  *CIntrinsicProfile // target+type specific intrinsics (nil = use legacy if/else)
}

// NewCEmitter creates a new C emitter for the given element type.
func NewCEmitter(pkgName, elemType string, target Target) *CEmitter {
	return &CEmitter{
		pkgName:  pkgName,
		elemType: elemType,
		target:   target,
	}
}

// EmitC generates C code for a SIMD array operation.
// The generated function processes entire arrays using target-specific SIMD.
// Returns the path to the generated C file.
func (e *CEmitter) EmitC(pf *ParsedFunc, outPath string) (string, error) {
	var buf bytes.Buffer

	// Determine include and target label from profile
	include := "#include <arm_neon.h>"
	targetLabel := "ARM64 NEON"
	targetSuffix := "neon"
	archSuffix := "arm64"
	if e.profile != nil {
		include = e.profile.Include
		targetLabel = e.profile.TargetName + " " + e.profile.ElemType
		targetSuffix = strings.ToLower(e.profile.TargetName)
		if e.profile.GoatTarget == "amd64" {
			archSuffix = "amd64"
		}
	}

	// File header
	fmt.Fprintf(&buf, "// Generated by hwygen -c. DO NOT EDIT.\n")
	fmt.Fprintf(&buf, "// %s for %s\n\n", pf.Name, targetLabel)
	fmt.Fprintf(&buf, "%s\n\n", include)

	// Generate the function
	if err := e.emitCFunction(&buf, pf); err != nil {
		return "", fmt.Errorf("emit C function: %w", err)
	}

	// Determine output filename
	suffix := e.typeSuffix()
	filename := filepath.Join(outPath, fmt.Sprintf("%s_c_%s_%s_%s.c", strings.ToLower(pf.Name), suffix, targetSuffix, archSuffix))

	if err := os.WriteFile(filename, buf.Bytes(), 0644); err != nil {
		return "", fmt.Errorf("write C file: %w", err)
	}

	fmt.Printf("Generated %s\n", filename)
	return filename, nil
}

// emitCFunction generates a single bulk function.
func (e *CEmitter) emitCFunction(buf *bytes.Buffer, pf *ParsedFunc) error {
	funcName := e.cFuncName(pf.Name)
	cType := e.cType()
	vecType := e.vecType()
	lanes := e.lanes()

	// Function signature following GOAT conventions:
	// void func_name(cType *input, cType *result, long *len)
	fmt.Fprintf(buf, "// %s: processes entire array with NEON SIMD\n", funcName)
	fmt.Fprintf(buf, "void %s(%s *input, %s *result, long *len) {\n", funcName, cType, cType)
	fmt.Fprintf(buf, "    long n = *len;\n")
	fmt.Fprintf(buf, "    long i = 0;\n\n")

	// Emit constants
	e.emitConstants(buf, pf)

	// Main SIMD loop - process multiple vectors at once for better throughput
	fmt.Fprintf(buf, "    // Process %d elements at a time (4 vectors)\n", lanes*4)
	fmt.Fprintf(buf, "    for (; i + %d < n; i += %d) {\n", lanes*4-1, lanes*4)

	// Load 4 vectors
	for j := range 4 {
		fmt.Fprintf(buf, "        %s x%d = %s(input + i + %d);\n", vecType, j, e.loadIntrinsic(), j*lanes)
	}
	fmt.Fprintf(buf, "\n")

	// Emit the computation for each vector
	for j := range 4 {
		e.emitVecComputation(buf, pf, j, "        ")
	}

	// Store results
	fmt.Fprintf(buf, "\n")
	for j := range 4 {
		fmt.Fprintf(buf, "        %s(result + i + %d, res%d);\n", e.storeIntrinsic(), j*lanes, j)
	}
	fmt.Fprintf(buf, "    }\n\n")

	// Single vector loop for remaining elements
	fmt.Fprintf(buf, "    // Process %d elements at a time\n", lanes)
	fmt.Fprintf(buf, "    for (; i + %d < n; i += %d) {\n", lanes-1, lanes)
	fmt.Fprintf(buf, "        %s x = %s(input + i);\n", vecType, e.loadIntrinsic())
	e.emitVecComputationSingle(buf, pf, "        ")
	fmt.Fprintf(buf, "        %s(result + i, res);\n", e.storeIntrinsic())
	fmt.Fprintf(buf, "    }\n\n")

	// Scalar tail - use NEON for single elements
	fmt.Fprintf(buf, "    // Scalar remainder using NEON for consistency\n")
	fmt.Fprintf(buf, "    for (; i < n; i++) {\n")
	fmt.Fprintf(buf, "        %s xv = input[i];\n", cType)
	fmt.Fprintf(buf, "        %s x = %s(xv);\n", vecType, e.dupIntrinsic())
	e.emitVecComputationSingle(buf, pf, "        ")
	fmt.Fprintf(buf, "        result[i] = %s(res, 0);\n", e.getLaneIntrinsic())
	fmt.Fprintf(buf, "    }\n")

	fmt.Fprintf(buf, "}\n")
	return nil
}

// emitConstants emits the constant declarations needed for the function.
func (e *CEmitter) emitConstants(buf *bytes.Buffer, pf *ParsedFunc) {
	vecType := e.vecType()

	// Based on the function, emit appropriate constants
	// This is a simplified version - a full implementation would analyze the AST
	switch {
	case strings.Contains(pf.Name, "Exp"):
		e.emitExpConstants(buf, vecType)
	case strings.Contains(pf.Name, "Sigmoid"):
		e.emitSigmoidConstants(buf, vecType)
	case strings.Contains(pf.Name, "Erf"):
		e.emitErfConstants(buf, vecType)
	}
	fmt.Fprintf(buf, "\n")
}

// emitExpConstants emits constants for exp computation.
// Float64 constants use hex bit patterns to avoid clang placing them in rodata sections.
func (e *CEmitter) emitExpConstants(buf *bytes.Buffer, vecType string) {
	if e.elemType == "float32" {
		fmt.Fprintf(buf, "    // Exp constants (f32)\n")
		fmt.Fprintf(buf, "    %s invLn2 = vreinterpretq_f32_s32(vdupq_n_s32(0x3FB8AA3B));\n", vecType)
		fmt.Fprintf(buf, "    %s ln2Hi = vreinterpretq_f32_s32(vdupq_n_s32(0x3F317200));\n", vecType)
		fmt.Fprintf(buf, "    %s ln2Lo = vreinterpretq_f32_s32(vdupq_n_s32(0xB95E8083));\n", vecType)
		fmt.Fprintf(buf, "    %s overflow = vreinterpretq_f32_s32(vdupq_n_s32(0x42B17218));\n", vecType)
		fmt.Fprintf(buf, "    %s underflow = vreinterpretq_f32_s32(vdupq_n_s32(0xC2AEAC50));\n", vecType)
		fmt.Fprintf(buf, "    %s c1 = vdupq_n_f32(1.0f);\n", vecType)
		fmt.Fprintf(buf, "    %s c2 = vdupq_n_f32(0.5f);\n", vecType)
		fmt.Fprintf(buf, "    %s c3 = vdupq_n_f32(0.16666666666666666f);\n", vecType)
		fmt.Fprintf(buf, "    %s c4 = vdupq_n_f32(0.041666666666666664f);\n", vecType)
		fmt.Fprintf(buf, "    %s c5 = vdupq_n_f32(0.008333333333333333f);\n", vecType)
		fmt.Fprintf(buf, "    %s c6 = vdupq_n_f32(0.001388888888888889f);\n", vecType)
		fmt.Fprintf(buf, "    %s zero = vdupq_n_f32(0.0f);\n", vecType)
		fmt.Fprintf(buf, "    %s inf = vdupq_n_f32(1.0f / 0.0f);\n", vecType)
		fmt.Fprintf(buf, "    int32x4_t bias = vdupq_n_s32(127);\n")
	} else {
		// Use volatile + hex bit patterns to prevent clang from hoisting to rodata
		// GOAT cannot handle rodata sections, so we must force inline constant construction
		fmt.Fprintf(buf, "    // Exp constants (f64) - volatile prevents rodata hoisting\n")
		fmt.Fprintf(buf, "    volatile %s invLn2 = vreinterpretq_f64_s64(vdupq_n_s64(0x3FF71547652B82FELL)); // 1.4426950408889634\n", vecType)
		fmt.Fprintf(buf, "    volatile %s ln2Hi = vreinterpretq_f64_s64(vdupq_n_s64(0x3FE62E42FEFA39EFLL)); // 0.6931471805599453\n", vecType)
		fmt.Fprintf(buf, "    volatile %s ln2Lo = vreinterpretq_f64_s64(vdupq_n_s64(0xBDE8BFBE8E7BCD5ELL)); // -1.9082149292705877e-10\n", vecType)
		fmt.Fprintf(buf, "    volatile %s overflow = vreinterpretq_f64_s64(vdupq_n_s64(0x40862E42FEFA39EFLL)); // 709.0\n", vecType)
		fmt.Fprintf(buf, "    volatile %s underflow = vreinterpretq_f64_s64(vdupq_n_s64(0xC0862E42FEFA39EFLL)); // -709.0\n", vecType)
		fmt.Fprintf(buf, "    volatile %s c1 = vreinterpretq_f64_s64(vdupq_n_s64(0x3FF0000000000000LL)); // 1.0\n", vecType)
		fmt.Fprintf(buf, "    volatile %s c2 = vreinterpretq_f64_s64(vdupq_n_s64(0x3FE0000000000000LL)); // 0.5\n", vecType)
		fmt.Fprintf(buf, "    volatile %s c3 = vreinterpretq_f64_s64(vdupq_n_s64(0x3FC5555555555555LL)); // 0.16666666666666666\n", vecType)
		fmt.Fprintf(buf, "    volatile %s c4 = vreinterpretq_f64_s64(vdupq_n_s64(0x3FA5555555555555LL)); // 0.041666666666666664\n", vecType)
		fmt.Fprintf(buf, "    volatile %s c5 = vreinterpretq_f64_s64(vdupq_n_s64(0x3F81111111111111LL)); // 0.008333333333333333\n", vecType)
		fmt.Fprintf(buf, "    volatile %s c6 = vreinterpretq_f64_s64(vdupq_n_s64(0x3F56C16C16C16C17LL)); // 0.001388888888888889\n", vecType)
		fmt.Fprintf(buf, "    volatile %s zero = vreinterpretq_f64_s64(vdupq_n_s64(0x0000000000000000LL)); // 0.0\n", vecType)
		fmt.Fprintf(buf, "    volatile %s inf = vreinterpretq_f64_s64(vdupq_n_s64(0x7FF0000000000000LL)); // +inf\n", vecType)
		fmt.Fprintf(buf, "    volatile int64x2_t bias = vdupq_n_s64(1023);\n")
	}
}

// emitSigmoidConstants emits constants for sigmoid computation.
func (e *CEmitter) emitSigmoidConstants(buf *bytes.Buffer, vecType string) {
	e.emitExpConstants(buf, vecType)
	if e.elemType == "float32" {
		fmt.Fprintf(buf, "    %s one = vdupq_n_f32(1.0f);\n", vecType)
		fmt.Fprintf(buf, "    %s satHi = vdupq_n_f32(20.0f);\n", vecType)
		fmt.Fprintf(buf, "    %s satLo = vdupq_n_f32(-20.0f);\n", vecType)
	} else {
		// Use volatile + hex bit patterns to prevent clang from hoisting to rodata
		fmt.Fprintf(buf, "    volatile %s one = vreinterpretq_f64_s64(vdupq_n_s64(0x3FF0000000000000LL)); // 1.0\n", vecType)
		fmt.Fprintf(buf, "    volatile %s satHi = vreinterpretq_f64_s64(vdupq_n_s64(0x4034000000000000LL)); // 20.0\n", vecType)
		fmt.Fprintf(buf, "    volatile %s satLo = vreinterpretq_f64_s64(vdupq_n_s64(0xC034000000000000LL)); // -20.0\n", vecType)
	}
}

// emitErfConstants emits constants for erf computation.
func (e *CEmitter) emitErfConstants(buf *bytes.Buffer, vecType string) {
	e.emitExpConstants(buf, vecType)
	if e.elemType == "float32" {
		fmt.Fprintf(buf, "    %s a1 = vdupq_n_f32(0.254829592f);\n", vecType)
		fmt.Fprintf(buf, "    %s a2 = vdupq_n_f32(-0.284496736f);\n", vecType)
		fmt.Fprintf(buf, "    %s a3 = vdupq_n_f32(1.421413741f);\n", vecType)
		fmt.Fprintf(buf, "    %s a4 = vdupq_n_f32(-1.453152027f);\n", vecType)
		fmt.Fprintf(buf, "    %s a5 = vdupq_n_f32(1.061405429f);\n", vecType)
		fmt.Fprintf(buf, "    %s erfP = vreinterpretq_f32_s32(vdupq_n_s32(0x3EA7BA27));\n", vecType)
		fmt.Fprintf(buf, "    %s one = vdupq_n_f32(1.0f);\n", vecType)
	} else {
		// Use volatile + hex bit patterns to prevent clang from hoisting to rodata
		fmt.Fprintf(buf, "    volatile %s a1 = vreinterpretq_f64_s64(vdupq_n_s64(0x3FD04DDF3B72BEF5LL)); // 0.254829592\n", vecType)
		fmt.Fprintf(buf, "    volatile %s a2 = vreinterpretq_f64_s64(vdupq_n_s64(0xBFD2327F14F2FE0ELL)); // -0.284496736\n", vecType)
		fmt.Fprintf(buf, "    volatile %s a3 = vreinterpretq_f64_s64(vdupq_n_s64(0x3FF6C0CC1ECAC78CLL)); // 1.421413741\n", vecType)
		fmt.Fprintf(buf, "    volatile %s a4 = vreinterpretq_f64_s64(vdupq_n_s64(0xBFF7412D14DC4B0DLL)); // -1.453152027\n", vecType)
		fmt.Fprintf(buf, "    volatile %s a5 = vreinterpretq_f64_s64(vdupq_n_s64(0x3FF0FB926CE8EF39LL)); // 1.061405429\n", vecType)
		fmt.Fprintf(buf, "    volatile %s erfP = vreinterpretq_f64_s64(vdupq_n_s64(0x3FD4F80D2CC7BBF6LL)); // 0.3275911\n", vecType)
		fmt.Fprintf(buf, "    volatile %s one = vreinterpretq_f64_s64(vdupq_n_s64(0x3FF0000000000000LL)); // 1.0\n", vecType)
	}
}

// emitVecComputation emits the NEON computation for a single vector.
func (e *CEmitter) emitVecComputation(buf *bytes.Buffer, pf *ParsedFunc, idx int, indent string) {
	// This generates the actual computation based on the function type
	// For now, handle the most common functions explicitly
	suffix := fmt.Sprintf("%d", idx)
	x := "x" + suffix
	res := "res" + suffix

	switch {
	case strings.Contains(pf.Name, "ExpVec") && !strings.Contains(pf.Name, "Sigmoid"):
		e.emitExpComputation(buf, x, res, indent)
	case strings.Contains(pf.Name, "SigmoidVec"):
		e.emitSigmoidComputation(buf, x, res, indent)
	case strings.Contains(pf.Name, "ErfVec"):
		e.emitErfComputation(buf, x, res, indent)
	default:
		// Generic fallback - just copy
		fmt.Fprintf(buf, "%s%s %s = %s; // TODO: implement %s\n", indent, e.vecType(), res, x, pf.Name)
	}
}

// emitVecComputationSingle emits computation for a single vector (non-indexed).
func (e *CEmitter) emitVecComputationSingle(buf *bytes.Buffer, pf *ParsedFunc, indent string) {
	switch {
	case strings.Contains(pf.Name, "ExpVec") && !strings.Contains(pf.Name, "Sigmoid"):
		e.emitExpComputation(buf, "x", "res", indent)
	case strings.Contains(pf.Name, "SigmoidVec"):
		e.emitSigmoidComputation(buf, "x", "res", indent)
	case strings.Contains(pf.Name, "ErfVec"):
		e.emitErfComputation(buf, "x", "res", indent)
	default:
		fmt.Fprintf(buf, "%s%s res = x; // TODO: implement %s\n", indent, e.vecType(), pf.Name)
	}
}

// emitExpComputation emits the exp computation.
func (e *CEmitter) emitExpComputation(buf *bytes.Buffer, x, res, indent string) {
	vecType := e.vecType()

	if e.elemType == "float32" {
		fmt.Fprintf(buf, "%s// Overflow/underflow check\n", indent)
		fmt.Fprintf(buf, "%suint32x4_t over_%s = vcgtq_f32(%s, overflow);\n", indent, res, x)
		fmt.Fprintf(buf, "%suint32x4_t under_%s = vcltq_f32(%s, underflow);\n", indent, res, x)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Range reduction: k = round(x * invLn2)\n", indent)
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f32(vmulq_f32(%s, invLn2));\n", indent, vecType, res, x)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f32(%s, kf_%s, ln2Hi);\n", indent, vecType, res, x, res)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f32(r_%s, kf_%s, ln2Lo);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Horner's polynomial\n", indent)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f32(c5, c6, r_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c4, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c3, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c2, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Scale by 2^k\n", indent)
		fmt.Fprintf(buf, "%sint32x4_t ki_%s = vcvtnq_s32_f32(kf_%s);\n", indent, res, res)
		fmt.Fprintf(buf, "%sint32x4_t scale_bits_%s = vshlq_n_s32(vaddq_s32(ki_%s, bias), 23);\n", indent, res, res)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f32_s32(scale_bits_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s %s = vmulq_f32(p_%s, scale_%s);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Handle overflow/underflow\n", indent)
		fmt.Fprintf(buf, "%s%s = vbslq_f32(over_%s, inf, %s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s%s = vbslq_f32(under_%s, zero, %s);\n", indent, res, res, res)
	} else {
		fmt.Fprintf(buf, "%s// Overflow/underflow check\n", indent)
		fmt.Fprintf(buf, "%suint64x2_t over_%s = vcgtq_f64(%s, overflow);\n", indent, res, x)
		fmt.Fprintf(buf, "%suint64x2_t under_%s = vcltq_f64(%s, underflow);\n", indent, res, x)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Range reduction\n", indent)
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f64(vmulq_f64(%s, invLn2));\n", indent, vecType, res, x)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f64(%s, kf_%s, ln2Hi);\n", indent, vecType, res, x, res)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f64(r_%s, kf_%s, ln2Lo);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Horner's polynomial\n", indent)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f64(c5, c6, r_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c4, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c3, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c2, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Scale by 2^k\n", indent)
		fmt.Fprintf(buf, "%sint64x2_t ki_%s = vcvtq_s64_f64(kf_%s);\n", indent, res, res)
		fmt.Fprintf(buf, "%sint64x2_t scale_bits_%s = vshlq_n_s64(vaddq_s64(ki_%s, bias), 52);\n", indent, res, res)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f64_s64(scale_bits_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s %s = vmulq_f64(p_%s, scale_%s);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Handle overflow/underflow\n", indent)
		fmt.Fprintf(buf, "%s%s = vbslq_f64(over_%s, inf, %s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s%s = vbslq_f64(under_%s, zero, %s);\n", indent, res, res, res)
	}
}

// emitSigmoidComputation emits sigmoid computation that calls exp internally.
func (e *CEmitter) emitSigmoidComputation(buf *bytes.Buffer, x, res, indent string) {
	vecType := e.vecType()

	if e.elemType == "float32" {
		fmt.Fprintf(buf, "%s// Clamp input\n", indent)
		fmt.Fprintf(buf, "%s%s clamped_%s = vmaxq_f32(vminq_f32(%s, satHi), satLo);\n", indent, vecType, res, x)
		fmt.Fprintf(buf, "%s%s negX_%s = vnegq_f32(clamped_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Compute exp(-x) inline\n", indent)
		// Inline the exp computation
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f32(vmulq_f32(negX_%s, invLn2));\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f32(negX_%s, kf_%s, ln2Hi);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f32(r_%s, kf_%s, ln2Lo);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f32(c5, c6, r_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c4, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c3, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c2, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sint32x4_t ki_%s = vcvtnq_s32_f32(kf_%s);\n", indent, res, res)
		fmt.Fprintf(buf, "%sint32x4_t scale_bits_%s = vshlq_n_s32(vaddq_s32(ki_%s, bias), 23);\n", indent, res, res)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f32_s32(scale_bits_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s expNegX_%s = vmulq_f32(p_%s, scale_%s);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// sigmoid = 1 / (1 + exp(-x))\n", indent)
		fmt.Fprintf(buf, "%s%s denom_%s = vaddq_f32(one, expNegX_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s %s = vdivq_f32(one, denom_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Handle saturation\n", indent)
		fmt.Fprintf(buf, "%s%s = vbslq_f32(vcgtq_f32(%s, satHi), one, %s);\n", indent, res, x, res)
		fmt.Fprintf(buf, "%s%s = vbslq_f32(vcltq_f32(%s, satLo), zero, %s);\n", indent, res, x, res)
	} else {
		// Float64 version
		fmt.Fprintf(buf, "%s// Clamp input\n", indent)
		fmt.Fprintf(buf, "%s%s clamped_%s = vmaxq_f64(vminq_f64(%s, satHi), satLo);\n", indent, vecType, res, x)
		fmt.Fprintf(buf, "%s%s negX_%s = vnegq_f64(clamped_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Compute exp(-x) inline\n", indent)
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f64(vmulq_f64(negX_%s, invLn2));\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f64(negX_%s, kf_%s, ln2Hi);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f64(r_%s, kf_%s, ln2Lo);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f64(c5, c6, r_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c4, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c3, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c2, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sint64x2_t ki_%s = vcvtq_s64_f64(kf_%s);\n", indent, res, res)
		fmt.Fprintf(buf, "%sint64x2_t scale_bits_%s = vshlq_n_s64(vaddq_s64(ki_%s, bias), 52);\n", indent, res, res)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f64_s64(scale_bits_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s expNegX_%s = vmulq_f64(p_%s, scale_%s);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// sigmoid = 1 / (1 + exp(-x))\n", indent)
		fmt.Fprintf(buf, "%s%s denom_%s = vaddq_f64(one, expNegX_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s %s = vdivq_f64(one, denom_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Handle saturation\n", indent)
		fmt.Fprintf(buf, "%s%s = vbslq_f64(vcgtq_f64(%s, satHi), one, %s);\n", indent, res, x, res)
		fmt.Fprintf(buf, "%s%s = vbslq_f64(vcltq_f64(%s, satLo), zero, %s);\n", indent, res, x, res)
	}
}

// emitErfComputation emits erf computation.
func (e *CEmitter) emitErfComputation(buf *bytes.Buffer, x, res, indent string) {
	vecType := e.vecType()

	if e.elemType == "float32" {
		fmt.Fprintf(buf, "%s// Get sign and absolute value\n", indent)
		fmt.Fprintf(buf, "%suint32x4_t isNeg_%s = vcltq_f32(%s, zero);\n", indent, res, x)
		fmt.Fprintf(buf, "%s%s absX_%s = vabsq_f32(%s);\n", indent, vecType, res, x)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// t = 1 / (1 + p * |x|)\n", indent)
		fmt.Fprintf(buf, "%s%s t_%s = vdivq_f32(one, vfmaq_f32(one, erfP, absX_%s));\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Compute -x^2\n", indent)
		fmt.Fprintf(buf, "%s%s negX2_%s = vnegq_f32(vmulq_f32(%s, %s));\n", indent, vecType, res, x, x)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Compute exp(-x^2) inline\n", indent)
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f32(vmulq_f32(negX2_%s, invLn2));\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f32(negX2_%s, kf_%s, ln2Hi);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f32(r_%s, kf_%s, ln2Lo);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f32(c5, c6, r_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c4, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c3, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c2, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sint32x4_t ki_%s = vcvtnq_s32_f32(kf_%s);\n", indent, res, res)
		fmt.Fprintf(buf, "%sint32x4_t scale_bits_%s = vshlq_n_s32(vaddq_s32(ki_%s, bias), 23);\n", indent, res, res)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f32_s32(scale_bits_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s expNegX2_%s = vmulq_f32(p_%s, scale_%s);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Polynomial: t*(a1 + t*(a2 + t*(a3 + t*(a4 + t*a5))))\n", indent)
		fmt.Fprintf(buf, "%s%s poly_%s = a5;\n", indent, vecType, res)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a4, poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a3, poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a2, poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a1, poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%spoly_%s = vmulq_f32(poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// erf = 1 - poly * exp(-x^2)\n", indent)
		fmt.Fprintf(buf, "%s%s erfAbs_%s = vfmsq_f32(one, poly_%s, expNegX2_%s);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Apply sign\n", indent)
		fmt.Fprintf(buf, "%s%s %s = vbslq_f32(isNeg_%s, vnegq_f32(erfAbs_%s), erfAbs_%s);\n", indent, vecType, res, res, res, res)
	} else {
		// Float64 version - similar structure
		fmt.Fprintf(buf, "%s// Get sign and absolute value\n", indent)
		fmt.Fprintf(buf, "%suint64x2_t isNeg_%s = vcltq_f64(%s, zero);\n", indent, res, x)
		fmt.Fprintf(buf, "%s%s absX_%s = vabsq_f64(%s);\n", indent, vecType, res, x)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// t = 1 / (1 + p * |x|)\n", indent)
		fmt.Fprintf(buf, "%s%s t_%s = vdivq_f64(one, vfmaq_f64(one, erfP, absX_%s));\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Compute -x^2\n", indent)
		fmt.Fprintf(buf, "%s%s negX2_%s = vnegq_f64(vmulq_f64(%s, %s));\n", indent, vecType, res, x, x)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Compute exp(-x^2) inline\n", indent)
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f64(vmulq_f64(negX2_%s, invLn2));\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f64(negX2_%s, kf_%s, ln2Hi);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f64(r_%s, kf_%s, ln2Lo);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f64(c5, c6, r_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c4, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c3, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c2, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%sint64x2_t ki_%s = vcvtq_s64_f64(kf_%s);\n", indent, res, res)
		fmt.Fprintf(buf, "%sint64x2_t scale_bits_%s = vshlq_n_s64(vaddq_s64(ki_%s, bias), 52);\n", indent, res, res)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f64_s64(scale_bits_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s expNegX2_%s = vmulq_f64(p_%s, scale_%s);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Polynomial\n", indent)
		fmt.Fprintf(buf, "%s%s poly_%s = a5;\n", indent, vecType, res)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f64(a4, poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f64(a3, poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f64(a2, poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f64(a1, poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%spoly_%s = vmulq_f64(poly_%s, t_%s);\n", indent, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// erf = 1 - poly * exp(-x^2)\n", indent)
		fmt.Fprintf(buf, "%s%s erfAbs_%s = vfmsq_f64(one, poly_%s, expNegX2_%s);\n", indent, vecType, res, res, res)
		fmt.Fprintf(buf, "%s\n", indent)
		fmt.Fprintf(buf, "%s// Apply sign\n", indent)
		fmt.Fprintf(buf, "%s%s %s = vbslq_f64(isNeg_%s, vnegq_f64(erfAbs_%s), erfAbs_%s);\n", indent, vecType, res, res, res, res)
	}
}

// Helper methods for type-specific intrinsics
func (e *CEmitter) typeSuffix() string {
	return cTypeSuffix(e.elemType)
}

func (e *CEmitter) cType() string {
	if e.elemType == "float32" {
		return "float"
	}
	return "double"
}

func (e *CEmitter) vecType() string {
	if e.elemType == "float32" {
		return "float32x4_t"
	}
	return "float64x2_t"
}

func (e *CEmitter) lanes() int {
	if e.elemType == "float32" {
		return 4
	}
	return 2
}

func (e *CEmitter) loadIntrinsic() string {
	if e.elemType == "float32" {
		return "vld1q_f32"
	}
	return "vld1q_f64"
}

func (e *CEmitter) storeIntrinsic() string {
	if e.elemType == "float32" {
		return "vst1q_f32"
	}
	return "vst1q_f64"
}

func (e *CEmitter) dupIntrinsic() string {
	if e.elemType == "float32" {
		return "vdupq_n_f32"
	}
	return "vdupq_n_f64"
}

func (e *CEmitter) getLaneIntrinsic() string {
	if e.elemType == "float32" {
		return "vgetq_lane_f32"
	}
	return "vgetq_lane_f64"
}

func (e *CEmitter) cFuncName(baseName string) string {
	// BaseExpVec -> exp_c_f32_neon or exp_c_f64_neon
	name := strings.TrimPrefix(baseName, "Base")
	name = strings.TrimSuffix(name, "Vec")
	targetSuffix := "neon"
	if e.profile != nil {
		targetSuffix = strings.ToLower(e.profile.TargetName)
	}
	return strings.ToLower(name) + "_c_" + e.typeSuffix() + "_" + targetSuffix
}

// IsCEligible checks if a function can be generated as C code for GOAT compilation.
func IsCEligible(pf *ParsedFunc) bool {
	// Check if it's a Vec→Vec function (has Vec in params and returns)
	hasVecParam := false
	hasVecReturn := false

	for _, param := range pf.Params {
		if strings.Contains(param.Type, "Vec[") {
			hasVecParam = true
			break
		}
	}

	for _, ret := range pf.Returns {
		if strings.Contains(ret.Type, "Vec[") {
			hasVecReturn = true
			break
		}
	}

	// Only Vec→Vec functions are eligible for C generation
	return hasVecParam && hasVecReturn
}

// EmitCompositeC generates C code for a composite array operation like GELU.
// Composite functions operate on slices and internally use multiple math functions.
func (e *CEmitter) EmitCompositeC(pf *ParsedFunc, outPath string) (string, error) {
	var buf bytes.Buffer

	// Determine include and target label from profile
	include := "#include <arm_neon.h>"
	targetLabel := "ARM64 NEON"
	targetSuffix := "neon"
	archSuffix := "arm64"
	if e.profile != nil {
		include = e.profile.Include
		targetLabel = e.profile.TargetName + " " + e.profile.ElemType
		targetSuffix = strings.ToLower(e.profile.TargetName)
		if e.profile.GoatTarget == "amd64" {
			archSuffix = "amd64"
		}
	}

	// File header
	fmt.Fprintf(&buf, "// Generated by hwygen -c. DO NOT EDIT.\n")
	fmt.Fprintf(&buf, "// %s for %s\n\n", pf.Name, targetLabel)
	fmt.Fprintf(&buf, "%s\n\n", include)

	// Generate the composite function
	if err := e.emitCompositeFunction(&buf, pf); err != nil {
		return "", fmt.Errorf("emit composite function: %w", err)
	}

	// Determine output filename
	suffix := e.typeSuffix()
	filename := filepath.Join(outPath, fmt.Sprintf("%s_c_%s_%s_%s.c", strings.ToLower(pf.Name), suffix, targetSuffix, archSuffix))

	if err := os.WriteFile(filename, buf.Bytes(), 0644); err != nil {
		return "", fmt.Errorf("write C file: %w", err)
	}

	fmt.Printf("Generated %s\n", filename)
	return filename, nil
}

// emitCompositeFunction generates a composite C function.
func (e *CEmitter) emitCompositeFunction(buf *bytes.Buffer, pf *ParsedFunc) error {
	funcName := e.cFuncName(pf.Name)
	cType := e.cType()
	vecType := e.vecType()
	lanes := e.lanes()

	// Function signature following GOAT conventions
	fmt.Fprintf(buf, "// %s: processes entire array with NEON SIMD\n", funcName)
	fmt.Fprintf(buf, "void %s(%s *input, %s *output, long *len) {\n", funcName, cType, cType)
	fmt.Fprintf(buf, "    long n = *len;\n")
	fmt.Fprintf(buf, "    long i = 0;\n\n")

	// Emit constants based on composite function type
	e.emitCompositeConstants(buf, pf)

	// Main SIMD loop - process multiple vectors at once
	fmt.Fprintf(buf, "    // Process %d elements at a time (4 vectors)\n", lanes*4)
	fmt.Fprintf(buf, "    for (; i + %d < n; i += %d) {\n", lanes*4-1, lanes*4)

	// Load 4 vectors
	for j := range 4 {
		fmt.Fprintf(buf, "        %s x%d = %s(input + i + %d);\n", vecType, j, e.loadIntrinsic(), j*lanes)
	}
	fmt.Fprintf(buf, "\n")

	// Emit the composite computation for each vector
	for j := range 4 {
		e.emitCompositeComputation(buf, pf, j, "        ")
	}

	// Store results
	fmt.Fprintf(buf, "\n")
	for j := range 4 {
		fmt.Fprintf(buf, "        %s(output + i + %d, res%d);\n", e.storeIntrinsic(), j*lanes, j)
	}
	fmt.Fprintf(buf, "    }\n\n")

	// Single vector loop for remaining elements
	fmt.Fprintf(buf, "    // Process %d elements at a time\n", lanes)
	fmt.Fprintf(buf, "    for (; i + %d < n; i += %d) {\n", lanes-1, lanes)
	fmt.Fprintf(buf, "        %s x = %s(input + i);\n", vecType, e.loadIntrinsic())
	e.emitCompositeComputationSingle(buf, pf, "        ")
	fmt.Fprintf(buf, "        %s(output + i, res);\n", e.storeIntrinsic())
	fmt.Fprintf(buf, "    }\n\n")

	// Scalar tail
	fmt.Fprintf(buf, "    // Scalar remainder using NEON for consistency\n")
	fmt.Fprintf(buf, "    for (; i < n; i++) {\n")
	fmt.Fprintf(buf, "        %s xv = input[i];\n", cType)
	fmt.Fprintf(buf, "        %s x = %s(xv);\n", vecType, e.dupIntrinsic())
	e.emitCompositeComputationSingle(buf, pf, "        ")
	fmt.Fprintf(buf, "        output[i] = %s(res, 0);\n", e.getLaneIntrinsic())
	fmt.Fprintf(buf, "    }\n")

	fmt.Fprintf(buf, "}\n")
	return nil
}

// emitCompositeConstants emits constants for composite functions.
func (e *CEmitter) emitCompositeConstants(buf *bytes.Buffer, pf *ParsedFunc) {
	vecType := e.vecType()

	switch {
	case strings.Contains(pf.Name, "GELU") && strings.Contains(pf.Name, "Approx"):
		// GELUApprox uses sigmoid: x * sigmoid(1.702 * x)
		e.emitExpConstants(buf, vecType) // for sigmoid
		if e.elemType == "float32" {
			fmt.Fprintf(buf, "    %s one = vdupq_n_f32(1.0f);\n", vecType)
			fmt.Fprintf(buf, "    %s satHi = vdupq_n_f32(20.0f);\n", vecType)
			fmt.Fprintf(buf, "    %s satLo = vdupq_n_f32(-20.0f);\n", vecType)
			fmt.Fprintf(buf, "    %s geluCoeff = vdupq_n_f32(1.702f);\n", vecType)
		} else {
			// Use volatile + hex bit patterns to prevent clang from hoisting to rodata
			fmt.Fprintf(buf, "    volatile %s one = vreinterpretq_f64_s64(vdupq_n_s64(0x3FF0000000000000LL)); // 1.0\n", vecType)
			fmt.Fprintf(buf, "    volatile %s satHi = vreinterpretq_f64_s64(vdupq_n_s64(0x4034000000000000LL)); // 20.0\n", vecType)
			fmt.Fprintf(buf, "    volatile %s satLo = vreinterpretq_f64_s64(vdupq_n_s64(0xC034000000000000LL)); // -20.0\n", vecType)
			fmt.Fprintf(buf, "    volatile %s geluCoeff = vreinterpretq_f64_s64(vdupq_n_s64(0x3FFB3D70A3D70A3DLL)); // 1.702\n", vecType)
		}
	case strings.Contains(pf.Name, "GELU"):
		// GELU uses erf: x * 0.5 * (1 + erf(x / sqrt(2)))
		e.emitErfConstants(buf, vecType)
		if e.elemType == "float32" {
			fmt.Fprintf(buf, "    %s half = vdupq_n_f32(0.5f);\n", vecType)
			fmt.Fprintf(buf, "    %s invSqrt2 = vdupq_n_f32(0.7071067811865476f);\n", vecType)
		} else {
			// Use volatile + hex bit patterns to prevent clang from hoisting to rodata
			fmt.Fprintf(buf, "    volatile %s half = vreinterpretq_f64_s64(vdupq_n_s64(0x3FE0000000000000LL)); // 0.5\n", vecType)
			fmt.Fprintf(buf, "    volatile %s invSqrt2 = vreinterpretq_f64_s64(vdupq_n_s64(0x3FE6A09E667F3BCDLL)); // 0.7071067811865476\n", vecType)
		}
	}
	fmt.Fprintf(buf, "\n")
}

// emitCompositeComputation emits the computation for one vector of a composite function.
func (e *CEmitter) emitCompositeComputation(buf *bytes.Buffer, pf *ParsedFunc, idx int, indent string) {
	suffix := fmt.Sprintf("%d", idx)
	x := "x" + suffix
	res := "res" + suffix

	switch {
	case strings.Contains(pf.Name, "GELUApprox"):
		e.emitGELUApproxComputation(buf, x, res, indent)
	case strings.Contains(pf.Name, "GELU"):
		e.emitGELUComputation(buf, x, res, indent)
	default:
		// Fallback - just copy
		fmt.Fprintf(buf, "%s%s %s = %s; // TODO: implement %s\n", indent, e.vecType(), res, x, pf.Name)
	}
}

// emitCompositeComputationSingle emits computation for a single vector (non-indexed).
func (e *CEmitter) emitCompositeComputationSingle(buf *bytes.Buffer, pf *ParsedFunc, indent string) {
	switch {
	case strings.Contains(pf.Name, "GELUApprox"):
		e.emitGELUApproxComputation(buf, "x", "res", indent)
	case strings.Contains(pf.Name, "GELU"):
		e.emitGELUComputation(buf, "x", "res", indent)
	default:
		fmt.Fprintf(buf, "%s%s res = x; // TODO: implement %s\n", indent, e.vecType(), pf.Name)
	}
}

// emitGELUComputation emits the exact GELU computation: x * 0.5 * (1 + erf(x / sqrt(2)))
func (e *CEmitter) emitGELUComputation(buf *bytes.Buffer, x, res, indent string) {
	vecType := e.vecType()

	fmt.Fprintf(buf, "%s// GELU: x * 0.5 * (1 + erf(x / sqrt(2)))\n", indent)
	fmt.Fprintf(buf, "%s\n", indent)

	// Scale x by 1/sqrt(2)
	if e.elemType == "float32" {
		fmt.Fprintf(buf, "%s%s xScaled_%s = vmulq_f32(%s, invSqrt2);\n", indent, vecType, res, x)
	} else {
		fmt.Fprintf(buf, "%s%s xScaled_%s = vmulq_f64(%s, invSqrt2);\n", indent, vecType, res, x)
	}

	// Compute erf(xScaled) - inline the erf computation
	fmt.Fprintf(buf, "%s// Compute erf(x / sqrt(2))\n", indent)
	e.emitErfComputationInline(buf, "xScaled_"+res, "erf_"+res, indent)

	// Compute 0.5 * (1 + erf)
	if e.elemType == "float32" {
		fmt.Fprintf(buf, "%s%s onePlusErf_%s = vaddq_f32(one, erf_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s halfOnePlusErf_%s = vmulq_f32(half, onePlusErf_%s);\n", indent, vecType, res, res)
		// Compute result = x * 0.5 * (1 + erf)
		fmt.Fprintf(buf, "%s%s %s = vmulq_f32(%s, halfOnePlusErf_%s);\n", indent, vecType, res, x, res)
	} else {
		fmt.Fprintf(buf, "%s%s onePlusErf_%s = vaddq_f64(one, erf_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s halfOnePlusErf_%s = vmulq_f64(half, onePlusErf_%s);\n", indent, vecType, res, res)
		fmt.Fprintf(buf, "%s%s %s = vmulq_f64(%s, halfOnePlusErf_%s);\n", indent, vecType, res, x, res)
	}
}

// emitGELUApproxComputation emits approximate GELU: x * sigmoid(1.702 * x)
func (e *CEmitter) emitGELUApproxComputation(buf *bytes.Buffer, x, res, indent string) {
	vecType := e.vecType()

	fmt.Fprintf(buf, "%s// GELUApprox: x * sigmoid(1.702 * x)\n", indent)
	fmt.Fprintf(buf, "%s\n", indent)

	// Scale x by 1.702
	if e.elemType == "float32" {
		fmt.Fprintf(buf, "%s%s xScaled_%s = vmulq_f32(%s, geluCoeff);\n", indent, vecType, res, x)
	} else {
		fmt.Fprintf(buf, "%s%s xScaled_%s = vmulq_f64(%s, geluCoeff);\n", indent, vecType, res, x)
	}

	// Compute sigmoid(xScaled) - inline the sigmoid computation
	fmt.Fprintf(buf, "%s// Compute sigmoid(1.702 * x)\n", indent)
	e.emitSigmoidComputationInline(buf, "xScaled_"+res, "sig_"+res, indent)

	// Compute result = x * sigmoid
	if e.elemType == "float32" {
		fmt.Fprintf(buf, "%s%s %s = vmulq_f32(%s, sig_%s);\n", indent, vecType, res, x, res)
	} else {
		fmt.Fprintf(buf, "%s%s %s = vmulq_f64(%s, sig_%s);\n", indent, vecType, res, x, res)
	}
}

// emitErfComputationInline emits an inline erf computation that stores result in a named variable.
func (e *CEmitter) emitErfComputationInline(buf *bytes.Buffer, xVar, resVar, indent string) {
	vecType := e.vecType()

	if e.elemType == "float32" {
		fmt.Fprintf(buf, "%suint32x4_t isNeg_%s = vcltq_f32(%s, zero);\n", indent, resVar, xVar)
		fmt.Fprintf(buf, "%s%s absX_%s = vabsq_f32(%s);\n", indent, vecType, resVar, xVar)
		fmt.Fprintf(buf, "%s%s t_%s = vdivq_f32(one, vfmaq_f32(one, erfP, absX_%s));\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s negX2_%s = vnegq_f32(vmulq_f32(%s, %s));\n", indent, vecType, resVar, xVar, xVar)
		// Compute exp(-x^2)
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f32(vmulq_f32(negX2_%s, invLn2));\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f32(negX2_%s, kf_%s, ln2Hi);\n", indent, vecType, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f32(r_%s, kf_%s, ln2Lo);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f32(c5, c6, r_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c4, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c3, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c2, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sint32x4_t ki_%s = vcvtnq_s32_f32(kf_%s);\n", indent, resVar, resVar)
		fmt.Fprintf(buf, "%sint32x4_t scale_bits_%s = vshlq_n_s32(vaddq_s32(ki_%s, bias), 23);\n", indent, resVar, resVar)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f32_s32(scale_bits_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s expNegX2_%s = vmulq_f32(p_%s, scale_%s);\n", indent, vecType, resVar, resVar, resVar)
		// Polynomial
		fmt.Fprintf(buf, "%s%s poly_%s = a5;\n", indent, vecType, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a4, poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a3, poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a2, poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a1, poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vmulq_f32(poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		// erf = 1 - poly * exp(-x^2)
		fmt.Fprintf(buf, "%s%s erfAbs_%s = vfmsq_f32(one, poly_%s, expNegX2_%s);\n", indent, vecType, resVar, resVar, resVar)
		// Apply sign
		fmt.Fprintf(buf, "%s%s %s = vbslq_f32(isNeg_%s, vnegq_f32(erfAbs_%s), erfAbs_%s);\n", indent, vecType, resVar, resVar, resVar, resVar)
	} else {
		// Float64 version
		fmt.Fprintf(buf, "%suint64x2_t isNeg_%s = vcltq_f64(%s, zero);\n", indent, resVar, xVar)
		fmt.Fprintf(buf, "%s%s absX_%s = vabsq_f64(%s);\n", indent, vecType, resVar, xVar)
		fmt.Fprintf(buf, "%s%s t_%s = vdivq_f64(one, vfmaq_f64(one, erfP, absX_%s));\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s negX2_%s = vnegq_f64(vmulq_f64(%s, %s));\n", indent, vecType, resVar, xVar, xVar)
		// Compute exp(-x^2)
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f64(vmulq_f64(negX2_%s, invLn2));\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f64(negX2_%s, kf_%s, ln2Hi);\n", indent, vecType, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f64(r_%s, kf_%s, ln2Lo);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f64(c5, c6, r_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c4, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c3, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c2, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sint64x2_t ki_%s = vcvtq_s64_f64(kf_%s);\n", indent, resVar, resVar)
		fmt.Fprintf(buf, "%sint64x2_t scale_bits_%s = vshlq_n_s64(vaddq_s64(ki_%s, bias), 52);\n", indent, resVar, resVar)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f64_s64(scale_bits_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s expNegX2_%s = vmulq_f64(p_%s, scale_%s);\n", indent, vecType, resVar, resVar, resVar)
		// Polynomial
		fmt.Fprintf(buf, "%s%s poly_%s = a5;\n", indent, vecType, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f64(a4, poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f64(a3, poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f64(a2, poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vfmaq_f64(a1, poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%spoly_%s = vmulq_f64(poly_%s, t_%s);\n", indent, resVar, resVar, resVar)
		// erf = 1 - poly * exp(-x^2)
		fmt.Fprintf(buf, "%s%s erfAbs_%s = vfmsq_f64(one, poly_%s, expNegX2_%s);\n", indent, vecType, resVar, resVar, resVar)
		// Apply sign
		fmt.Fprintf(buf, "%s%s %s = vbslq_f64(isNeg_%s, vnegq_f64(erfAbs_%s), erfAbs_%s);\n", indent, vecType, resVar, resVar, resVar, resVar)
	}
}

// emitSigmoidComputationInline emits an inline sigmoid computation.
func (e *CEmitter) emitSigmoidComputationInline(buf *bytes.Buffer, xVar, resVar, indent string) {
	vecType := e.vecType()

	if e.elemType == "float32" {
		fmt.Fprintf(buf, "%s%s clamped_%s = vmaxq_f32(vminq_f32(%s, satHi), satLo);\n", indent, vecType, resVar, xVar)
		fmt.Fprintf(buf, "%s%s negX_%s = vnegq_f32(clamped_%s);\n", indent, vecType, resVar, resVar)
		// Compute exp(-x)
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f32(vmulq_f32(negX_%s, invLn2));\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f32(negX_%s, kf_%s, ln2Hi);\n", indent, vecType, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f32(r_%s, kf_%s, ln2Lo);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f32(c5, c6, r_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c4, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c3, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c2, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sint32x4_t ki_%s = vcvtnq_s32_f32(kf_%s);\n", indent, resVar, resVar)
		fmt.Fprintf(buf, "%sint32x4_t scale_bits_%s = vshlq_n_s32(vaddq_s32(ki_%s, bias), 23);\n", indent, resVar, resVar)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f32_s32(scale_bits_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s expNegX_%s = vmulq_f32(p_%s, scale_%s);\n", indent, vecType, resVar, resVar, resVar)
		// sigmoid = 1 / (1 + exp(-x))
		fmt.Fprintf(buf, "%s%s denom_%s = vaddq_f32(one, expNegX_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s %s = vdivq_f32(one, denom_%s);\n", indent, vecType, resVar, resVar)
		// Saturation
		fmt.Fprintf(buf, "%s%s = vbslq_f32(vcgtq_f32(%s, satHi), one, %s);\n", indent, resVar, xVar, resVar)
		fmt.Fprintf(buf, "%s%s = vbslq_f32(vcltq_f32(%s, satLo), zero, %s);\n", indent, resVar, xVar, resVar)
	} else {
		fmt.Fprintf(buf, "%s%s clamped_%s = vmaxq_f64(vminq_f64(%s, satHi), satLo);\n", indent, vecType, resVar, xVar)
		fmt.Fprintf(buf, "%s%s negX_%s = vnegq_f64(clamped_%s);\n", indent, vecType, resVar, resVar)
		// Compute exp(-x)
		fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f64(vmulq_f64(negX_%s, invLn2));\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f64(negX_%s, kf_%s, ln2Hi);\n", indent, vecType, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sr_%s = vfmsq_f64(r_%s, kf_%s, ln2Lo);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f64(c5, c6, r_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c4, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c3, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c2, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sp_%s = vfmaq_f64(c1, p_%s, r_%s);\n", indent, resVar, resVar, resVar)
		fmt.Fprintf(buf, "%sint64x2_t ki_%s = vcvtq_s64_f64(kf_%s);\n", indent, resVar, resVar)
		fmt.Fprintf(buf, "%sint64x2_t scale_bits_%s = vshlq_n_s64(vaddq_s64(ki_%s, bias), 52);\n", indent, resVar, resVar)
		fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f64_s64(scale_bits_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s expNegX_%s = vmulq_f64(p_%s, scale_%s);\n", indent, vecType, resVar, resVar, resVar)
		// sigmoid = 1 / (1 + exp(-x))
		fmt.Fprintf(buf, "%s%s denom_%s = vaddq_f64(one, expNegX_%s);\n", indent, vecType, resVar, resVar)
		fmt.Fprintf(buf, "%s%s %s = vdivq_f64(one, denom_%s);\n", indent, vecType, resVar, resVar)
		// Saturation
		fmt.Fprintf(buf, "%s%s = vbslq_f64(vcgtq_f64(%s, satHi), one, %s);\n", indent, resVar, xVar, resVar)
		fmt.Fprintf(buf, "%s%s = vbslq_f64(vcltq_f64(%s, satLo), zero, %s);\n", indent, resVar, xVar, resVar)
	}
}

// ---------------------------------------------------------------------------
// Profile-driven C code generation for f16/bf16 (and native f32/f64) targets.
// These methods use CIntrinsicProfile to emit target-agnostic C via intrinsic
// tables rather than the legacy hard-coded NEON if/else branches above.
// When e.profile is nil the caller should fall through to the existing methods.
// ---------------------------------------------------------------------------

// emitBinaryOpC generates a C function for a binary elementwise operation
// (Add, Sub, Mul, Div, Min, Max) using the profile's intrinsic tables.
// The generated function signature is:
//
//	void <name>_c_<type>_<target>(cType *a, cType *b, cType *result, long *len)
func (e *CEmitter) emitBinaryOpC(buf *bytes.Buffer, opName string, pf *ParsedFunc) error {
	if e.profile == nil {
		return fmt.Errorf("emitBinaryOpC requires a non-nil CIntrinsicProfile")
	}
	prf := e.profile

	funcName := strings.ToLower(opName) + "_c_" + e.typeSuffix() + "_" + strings.ToLower(prf.TargetName)
	cType := prf.CType

	// Look up the operation intrinsic function for the first non-scalar tier.
	opFn := e.profileOpFn(opName)
	if opFn == "" {
		return fmt.Errorf("no intrinsic for binary op %q in profile %s/%s", opName, prf.TargetName, prf.ElemType)
	}

	fmt.Fprintf(buf, "// %s: binary elementwise %s\n", funcName, opName)
	fmt.Fprintf(buf, "void %s(%s *a, %s *b, %s *result, long *len) {\n", funcName, cType, cType, cType)
	fmt.Fprintf(buf, "    long n = *len;\n")
	fmt.Fprintf(buf, "    long i = 0;\n\n")

	// Emit each SIMD tier (main unrolled loop, single-vector loop).
	for _, tier := range prf.Tiers {
		if tier.IsScalar {
			// Scalar tail.
			e.emitBinaryScalarTail(buf, opName, prf)
			continue
		}

		vecType := prf.VecTypes[tier.Name]
		loadFn := prf.LoadFn[tier.Name]
		storeFn := prf.StoreFn[tier.Name]
		tierOp := e.profileTierOpFn(opName, tier.Name)
		if tierOp == "" {
			tierOp = opFn
		}

		totalLanes := tier.Lanes * tier.Unroll

		fmt.Fprintf(buf, "    // %s loop: %d vectors x %d lanes = %d elements\n", tier.Name, tier.Unroll, tier.Lanes, totalLanes)
		fmt.Fprintf(buf, "    for (; i + %d < n; i += %d) {\n", totalLanes-1, totalLanes)

		if prf.MathStrategy == "promoted" {
			e.emitBinaryPromoted(buf, tier, tierOp, "        ")
		} else {
			// Native path (NEON f16, or native f32/f64 profiles).
			for j := range tier.Unroll {
				off := j * tier.Lanes
				fmt.Fprintf(buf, "        %s a%d = %s(%s);\n", vecType, j, loadFn, prf.ptrExpr("a", off))
				fmt.Fprintf(buf, "        %s b%d = %s(%s);\n", vecType, j, loadFn, prf.ptrExpr("b", off))
			}
			for j := range tier.Unroll {
				off := j * tier.Lanes
				fmt.Fprintf(buf, "        %s(%s, %s(a%d, b%d));\n", storeFn, prf.ptrExpr("result", off), tierOp, j, j)
			}
		}

		fmt.Fprintf(buf, "    }\n\n")
	}

	fmt.Fprintf(buf, "}\n")
	return nil
}

// emitBinaryPromoted emits the body of a promoted binary loop tier.
// It loads narrow values, promotes to f32, applies the op, and demotes back.
func (e *CEmitter) emitBinaryPromoted(buf *bytes.Buffer, tier CLoopTier, opFn, indent string) {
	prf := e.profile
	narrowLoad := prf.LoadFn[tier.Name]
	narrowStore := prf.StoreFn[tier.Name]
	narrowVec := prf.VecTypes[tier.Name]
	wideVec := prf.VecTypes["wide"]
	promote := prf.PromoteFn
	demote := prf.DemoteFn

	for j := range tier.Unroll {
		off := j * tier.Lanes
		fmt.Fprintf(buf, "%s%s rawA%d = %s(%s);\n", indent, narrowVec, j, narrowLoad, prf.ptrExpr("a", off))
		fmt.Fprintf(buf, "%s%s rawB%d = %s(%s);\n", indent, narrowVec, j, narrowLoad, prf.ptrExpr("b", off))
		fmt.Fprintf(buf, "%s%s af%d = %s(rawA%d);\n", indent, wideVec, j, promote, j)
		fmt.Fprintf(buf, "%s%s bf%d = %s(rawB%d);\n", indent, wideVec, j, promote, j)
		fmt.Fprintf(buf, "%s%s rf%d = %s(af%d, bf%d);\n", indent, wideVec, j, opFn, j, j)
		fmt.Fprintf(buf, "%s%s(%s, %s);\n", indent, narrowStore, prf.ptrExpr("result", off), fmt.Sprintf(demote, fmt.Sprintf("rf%d", j)))
	}
}

// emitBinaryScalarTail emits the scalar remainder loop for a binary operation.
func (e *CEmitter) emitBinaryScalarTail(buf *bytes.Buffer, opName string, prf *CIntrinsicProfile) {
	scalarOp := scalarBinaryOp(opName)
	fmt.Fprintf(buf, "    // Scalar tail\n")
	fmt.Fprintf(buf, "    for (; i < n; i++) {\n")
	if scalarOp != "" {
		fmt.Fprintf(buf, "        result[i] = a[i] %s b[i];\n", scalarOp)
	} else {
		// Min / Max need conditional
		switch opName {
		case "Min":
			fmt.Fprintf(buf, "        result[i] = (a[i] < b[i]) ? a[i] : b[i];\n")
		case "Max":
			fmt.Fprintf(buf, "        result[i] = (a[i] > b[i]) ? a[i] : b[i];\n")
		default:
			fmt.Fprintf(buf, "        result[i] = a[i]; // TODO: scalar %s\n", opName)
		}
	}
	fmt.Fprintf(buf, "    }\n")
}

// emitUnaryOpC generates a C function for a unary elementwise operation
// (Neg, Abs, Sqrt) using the profile's intrinsic tables.
// Signature:
//
//	void <name>_c_<type>_<target>(cType *input, cType *result, long *len)
func (e *CEmitter) emitUnaryOpC(buf *bytes.Buffer, opName string, pf *ParsedFunc) error {
	if e.profile == nil {
		return fmt.Errorf("emitUnaryOpC requires a non-nil CIntrinsicProfile")
	}
	prf := e.profile

	funcName := strings.ToLower(opName) + "_c_" + e.typeSuffix() + "_" + strings.ToLower(prf.TargetName)
	cType := prf.CType

	opFn := e.profileOpFn(opName)
	if opFn == "" {
		return fmt.Errorf("no intrinsic for unary op %q in profile %s/%s", opName, prf.TargetName, prf.ElemType)
	}

	fmt.Fprintf(buf, "// %s: unary elementwise %s\n", funcName, opName)
	fmt.Fprintf(buf, "void %s(%s *input, %s *result, long *len) {\n", funcName, cType, cType)
	fmt.Fprintf(buf, "    long n = *len;\n")
	fmt.Fprintf(buf, "    long i = 0;\n\n")

	for _, tier := range prf.Tiers {
		if tier.IsScalar {
			e.emitUnaryScalarTail(buf, opName, prf)
			continue
		}

		vecType := prf.VecTypes[tier.Name]
		loadFn := prf.LoadFn[tier.Name]
		storeFn := prf.StoreFn[tier.Name]
		tierOp := e.profileTierOpFn(opName, tier.Name)
		if tierOp == "" {
			tierOp = opFn
		}

		totalLanes := tier.Lanes * tier.Unroll

		fmt.Fprintf(buf, "    // %s loop: %d vectors x %d lanes = %d elements\n", tier.Name, tier.Unroll, tier.Lanes, totalLanes)
		fmt.Fprintf(buf, "    for (; i + %d < n; i += %d) {\n", totalLanes-1, totalLanes)

		if prf.MathStrategy == "promoted" {
			e.emitUnaryPromoted(buf, tier, tierOp, "        ")
		} else {
			for j := range tier.Unroll {
				off := j * tier.Lanes
				fmt.Fprintf(buf, "        %s x%d = %s(%s);\n", vecType, j, loadFn, prf.ptrExpr("input", off))
			}
			for j := range tier.Unroll {
				off := j * tier.Lanes
				fmt.Fprintf(buf, "        %s(%s, %s(x%d));\n", storeFn, prf.ptrExpr("result", off), tierOp, j)
			}
		}

		fmt.Fprintf(buf, "    }\n\n")
	}

	fmt.Fprintf(buf, "}\n")
	return nil
}

// emitUnaryPromoted emits the body of a promoted unary loop tier.
func (e *CEmitter) emitUnaryPromoted(buf *bytes.Buffer, tier CLoopTier, opFn, indent string) {
	prf := e.profile
	narrowLoad := prf.LoadFn[tier.Name]
	narrowStore := prf.StoreFn[tier.Name]
	narrowVec := prf.VecTypes[tier.Name]
	wideVec := prf.VecTypes["wide"]
	promote := prf.PromoteFn
	demote := prf.DemoteFn

	for j := range tier.Unroll {
		off := j * tier.Lanes
		fmt.Fprintf(buf, "%s%s raw%d = %s(%s);\n", indent, narrowVec, j, narrowLoad, prf.ptrExpr("input", off))
		fmt.Fprintf(buf, "%s%s xf%d = %s(raw%d);\n", indent, wideVec, j, promote, j)
		fmt.Fprintf(buf, "%s%s rf%d = %s(xf%d);\n", indent, wideVec, j, opFn, j)
		fmt.Fprintf(buf, "%s%s(%s, %s);\n", indent, narrowStore, prf.ptrExpr("result", off), fmt.Sprintf(demote, fmt.Sprintf("rf%d", j)))
	}
}

// emitUnaryScalarTail emits the scalar remainder loop for a unary operation.
func (e *CEmitter) emitUnaryScalarTail(buf *bytes.Buffer, opName string, prf *CIntrinsicProfile) {
	fmt.Fprintf(buf, "    // Scalar tail\n")
	fmt.Fprintf(buf, "    for (; i < n; i++) {\n")
	switch opName {
	case "Neg":
		fmt.Fprintf(buf, "        result[i] = -input[i];\n")
	case "Abs":
		// Works for unsigned short (f16 storage): mask off sign bit.
		// For float types the compiler handles it.
		fmt.Fprintf(buf, "        result[i] = (input[i] < 0) ? -input[i] : input[i];\n")
	case "Sqrt":
		fmt.Fprintf(buf, "        result[i] = input[i]; // scalar sqrt requires type-specific handling\n")
	default:
		fmt.Fprintf(buf, "        result[i] = input[i]; // TODO: scalar %s\n", opName)
	}
	fmt.Fprintf(buf, "    }\n")
}

// emitPromotedMathC generates a C function that wraps f32 math (Exp, Sigmoid,
// Erf) with promote/demote for f16 or bf16 element types.  For each SIMD tier
// the generated code:
//  1. Loads the narrow (f16/bf16) vector
//  2. Promotes to f32 (or two f32 halves for NEON f16)
//  3. Runs the f32 polynomial computation (reusing emitExpComputation, etc.)
//  4. Demotes the f32 result back to f16/bf16
//  5. Stores the narrow result
//
// For profiles that use a split promote (NEON f16 where one f16x8 becomes two
// f32x4 halves), the profile's SplitPromoteLo / SplitPromoteHi / CombineFn
// fields are used.
func (e *CEmitter) emitPromotedMathC(buf *bytes.Buffer, mathOp string, pf *ParsedFunc) error {
	if e.profile == nil {
		return fmt.Errorf("emitPromotedMathC requires a non-nil CIntrinsicProfile")
	}
	prf := e.profile

	funcName := strings.ToLower(mathOp) + "_c_" + e.typeSuffix() + "_" + strings.ToLower(prf.TargetName)
	cType := prf.CType

	fmt.Fprintf(buf, "// %s: promoted %s via f32 polynomial\n", funcName, mathOp)
	fmt.Fprintf(buf, "void %s(%s *input, %s *result, long *len) {\n", funcName, cType, cType)
	fmt.Fprintf(buf, "    long n = *len;\n")
	fmt.Fprintf(buf, "    long i = 0;\n\n")

	// Emit f32 constants for the math kernel.  We temporarily pretend to be
	// f32 so the existing constant emitters produce the right declarations.
	f32Vec := prf.VecTypes["wide"] // e.g. float32x4_t or __m256
	e.emitPromotedMathConstants(buf, mathOp, f32Vec, prf)
	fmt.Fprintf(buf, "\n")

	for _, tier := range prf.Tiers {
		if tier.IsScalar {
			e.emitPromotedMathScalarTail(buf, mathOp, prf)
			continue
		}

		totalLanes := tier.Lanes * tier.Unroll
		fmt.Fprintf(buf, "    // %s loop: %d elements per iteration\n", tier.Name, totalLanes)
		fmt.Fprintf(buf, "    for (; i + %d < n; i += %d) {\n", totalLanes-1, totalLanes)

		if prf.SplitPromoteLo != "" {
			// Split promote path (NEON f16: one f16x8 -> two f32x4 halves).
			e.emitPromotedMathSplit(buf, tier, mathOp, "        ")
		} else {
			// Simple promote path (AVX2 F16C: one __m128i -> one __m256).
			e.emitPromotedMathSimple(buf, tier, mathOp, "        ")
		}

		fmt.Fprintf(buf, "    }\n\n")
	}

	fmt.Fprintf(buf, "}\n")
	return nil
}

// emitPromotedMathConstants emits f32 constants needed by the math kernel.
func (e *CEmitter) emitPromotedMathConstants(buf *bytes.Buffer, mathOp, f32Vec string, prf *CIntrinsicProfile) {
	// Look up the scalar broadcast intrinsic for the wide (f32) type from the
	// DupFn map.  We try several common tier keys.
	dupFn := ""
	for _, key := range []string{"q", "ymm", "zmm"} {
		if v, ok := prf.DupFn[key]; ok {
			dupFn = v
			break
		}
	}
	if dupFn == "" {
		// Fallback: if profile doesn't specify, derive from target name.
		if strings.Contains(prf.TargetName, "NEON") || strings.Contains(prf.TargetName, "neon") {
			dupFn = "vdupq_n_f32"
		} else {
			dupFn = "_mm256_set1_ps"
		}
	}

	isAVX := strings.Contains(prf.TargetName, "AVX") || strings.Contains(prf.TargetName, "avx")

	switch {
	case strings.Contains(mathOp, "Exp"), strings.Contains(mathOp, "Sigmoid"), strings.Contains(mathOp, "Erf"):
		e.emitPromotedExpConstants(buf, f32Vec, dupFn, prf, isAVX)
	}

	switch {
	case strings.Contains(mathOp, "Sigmoid"):
		fmt.Fprintf(buf, "    %s one = %s(1.0f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s satHi = %s(20.0f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s satLo = %s(-20.0f);\n", f32Vec, dupFn)
	case strings.Contains(mathOp, "Erf"):
		fmt.Fprintf(buf, "    %s a1 = %s(0.254829592f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s a2 = %s(-0.284496736f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s a3 = %s(1.421413741f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s a4 = %s(-1.453152027f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s a5 = %s(1.061405429f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s erfP = %s(0.3275911f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s one = %s(1.0f);\n", f32Vec, dupFn)
	}
}

// emitPromotedExpConstants emits the base exp constants for promoted math.
func (e *CEmitter) emitPromotedExpConstants(buf *bytes.Buffer, f32Vec, dupFn string, prf *CIntrinsicProfile, isAVX bool) {
	fmt.Fprintf(buf, "    // Exp constants (f32 for promoted path)\n")
	if isAVX {
		// AVX uses set1_ps / reinterpret via _mm256_castsi256_ps for hex constants
		fmt.Fprintf(buf, "    %s invLn2 = %s(1.4426950408889634f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s ln2Hi = %s(0.693145751953125f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s ln2Lo = %s(1.428606765330187e-06f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s overflow = %s(88.72283172607422f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s underflow = %s(-87.33654022216797f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s c1 = %s(1.0f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s c2 = %s(0.5f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s c3 = %s(0.16666666666666666f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s c4 = %s(0.041666666666666664f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s c5 = %s(0.008333333333333333f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s c6 = %s(0.001388888888888889f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s zero = %s(0.0f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    %s inf = %s(1.0f / 0.0f);\n", f32Vec, dupFn)
		fmt.Fprintf(buf, "    __m256i bias = _mm256_set1_epi32(127);\n")
	} else {
		// NEON path - use hex bit patterns via vreinterpret
		fmt.Fprintf(buf, "    %s invLn2 = vreinterpretq_f32_s32(vdupq_n_s32(0x3FB8AA3B));\n", f32Vec)
		fmt.Fprintf(buf, "    %s ln2Hi = vreinterpretq_f32_s32(vdupq_n_s32(0x3F317200));\n", f32Vec)
		fmt.Fprintf(buf, "    %s ln2Lo = vreinterpretq_f32_s32(vdupq_n_s32(0xB95E8083));\n", f32Vec)
		fmt.Fprintf(buf, "    %s overflow = vreinterpretq_f32_s32(vdupq_n_s32(0x42B17218));\n", f32Vec)
		fmt.Fprintf(buf, "    %s underflow = vreinterpretq_f32_s32(vdupq_n_s32(0xC2AEAC50));\n", f32Vec)
		fmt.Fprintf(buf, "    %s c1 = vdupq_n_f32(1.0f);\n", f32Vec)
		fmt.Fprintf(buf, "    %s c2 = vdupq_n_f32(0.5f);\n", f32Vec)
		fmt.Fprintf(buf, "    %s c3 = vdupq_n_f32(0.16666666666666666f);\n", f32Vec)
		fmt.Fprintf(buf, "    %s c4 = vdupq_n_f32(0.041666666666666664f);\n", f32Vec)
		fmt.Fprintf(buf, "    %s c5 = vdupq_n_f32(0.008333333333333333f);\n", f32Vec)
		fmt.Fprintf(buf, "    %s c6 = vdupq_n_f32(0.001388888888888889f);\n", f32Vec)
		fmt.Fprintf(buf, "    %s zero = vdupq_n_f32(0.0f);\n", f32Vec)
		fmt.Fprintf(buf, "    %s inf = vdupq_n_f32(1.0f / 0.0f);\n", f32Vec)
		fmt.Fprintf(buf, "    int32x4_t bias = vdupq_n_s32(127);\n")
	}
}

// emitPromotedMathSimple emits the simple promote path for one tier.
// Used by AVX2 F16C where __m128i promotes directly to __m256 (f32).
func (e *CEmitter) emitPromotedMathSimple(buf *bytes.Buffer, tier CLoopTier, mathOp, indent string) {
	prf := e.profile
	narrowVec := prf.VecTypes[tier.Name]
	wideVec := prf.VecTypes["wide"]
	narrowLoad := prf.LoadFn[tier.Name]
	narrowStore := prf.StoreFn[tier.Name]
	promote := prf.PromoteFn
	demote := prf.DemoteFn

	for j := range tier.Unroll {
		off := j * tier.Lanes
		raw := fmt.Sprintf("raw%d", j)
		xf := fmt.Sprintf("xf%d", j)
		res := fmt.Sprintf("res%d", j)

		fmt.Fprintf(buf, "%s%s %s = %s(%s);\n", indent, narrowVec, raw, narrowLoad, prf.ptrExpr("input", off))
		fmt.Fprintf(buf, "%s%s %s = %s(%s);\n", indent, wideVec, xf, promote, raw)

		// Emit the f32 computation. We reuse the existing emitters by
		// temporarily swapping into an f32 context.
		e.emitPromotedF32Computation(buf, mathOp, xf, res, indent, wideVec, prf)

		fmt.Fprintf(buf, "%s%s(%s, %s);\n", indent, narrowStore, prf.ptrExpr("result", off), fmt.Sprintf(demote, res))
	}
}

// emitPromotedMathSplit emits the split-promote path for one tier.
// Used by NEON f16 where one float16x8_t is split into two float32x4_t halves.
func (e *CEmitter) emitPromotedMathSplit(buf *bytes.Buffer, tier CLoopTier, mathOp, indent string) {
	prf := e.profile
	narrowVec := prf.VecTypes[tier.Name]
	wideVec := prf.VecTypes["wide"]
	narrowLoad := prf.LoadFn[tier.Name]
	narrowStore := prf.StoreFn[tier.Name]
	splitLo := prf.SplitPromoteLo
	splitHi := prf.SplitPromoteHi
	demote := prf.DemoteFn
	combineFn := prf.CombineFn

	for j := range tier.Unroll {
		off := j * tier.Lanes
		h := fmt.Sprintf("h%d", j)
		lo := fmt.Sprintf("lo%d", j)
		hi := fmt.Sprintf("hi%d", j)
		resLo := fmt.Sprintf("resLo%d", j)
		resHi := fmt.Sprintf("resHi%d", j)
		dLo := fmt.Sprintf("dLo%d", j)
		dHi := fmt.Sprintf("dHi%d", j)

		// Load the narrow vector.
		fmt.Fprintf(buf, "%s%s %s = %s(%s);\n", indent, narrowVec, h, narrowLoad, prf.ptrExpr("input", off))

		// Split and promote to f32.
		// SplitPromoteLo/Hi are fmt.Sprintf templates with %s for the vector variable.
		fmt.Fprintf(buf, "%s%s %s = %s;\n", indent, wideVec, lo, fmt.Sprintf(splitLo, h))
		fmt.Fprintf(buf, "%s%s %s = %s;\n", indent, wideVec, hi, fmt.Sprintf(splitHi, h))

		// Compute the f32 math on each half.
		e.emitPromotedF32Computation(buf, mathOp, lo, resLo, indent, wideVec, prf)
		e.emitPromotedF32Computation(buf, mathOp, hi, resHi, indent, wideVec, prf)

		// Demote each half back to narrow and combine.
		halfVec := prf.VecTypes["half"] // e.g. float16x4_t
		fmt.Fprintf(buf, "%s%s %s = %s;\n", indent, halfVec, dLo, fmt.Sprintf(demote, resLo))
		fmt.Fprintf(buf, "%s%s %s = %s;\n", indent, halfVec, dHi, fmt.Sprintf(demote, resHi))

		// Combine and store.
		// CombineFn is a fmt.Sprintf template with two %s placeholders for the halves.
		combined := fmt.Sprintf(combineFn, dLo, dHi)
		fmt.Fprintf(buf, "%s%s(%s, %s);\n", indent, narrowStore, prf.ptrExpr("result", off), combined)
	}
}

// emitPromotedF32Computation emits an f32 math computation for a single
// promoted vector.  It dispatches to the appropriate polynomial based on
// mathOp (Exp, Sigmoid, Erf).
func (e *CEmitter) emitPromotedF32Computation(buf *bytes.Buffer, mathOp, xVar, resVar, indent, f32Vec string, prf *CIntrinsicProfile) {
	isAVX := strings.Contains(prf.TargetName, "AVX") || strings.Contains(prf.TargetName, "avx")

	switch {
	case strings.Contains(mathOp, "Exp") && !strings.Contains(mathOp, "Sigmoid"):
		if isAVX {
			e.emitExpComputationAVX(buf, xVar, resVar, indent, f32Vec)
		} else {
			e.emitExpComputationNEONF32(buf, xVar, resVar, indent, f32Vec)
		}
	case strings.Contains(mathOp, "Sigmoid"):
		if isAVX {
			e.emitSigmoidComputationAVX(buf, xVar, resVar, indent, f32Vec)
		} else {
			e.emitSigmoidComputationNEONF32(buf, xVar, resVar, indent, f32Vec)
		}
	case strings.Contains(mathOp, "Erf"):
		if isAVX {
			e.emitErfComputationAVX(buf, xVar, resVar, indent, f32Vec)
		} else {
			e.emitErfComputationNEONF32(buf, xVar, resVar, indent, f32Vec)
		}
	default:
		fmt.Fprintf(buf, "%s%s %s = %s; // TODO: promoted %s\n", indent, f32Vec, resVar, xVar, mathOp)
	}
}

// emitPromotedMathScalarTail emits the scalar tail for promoted math ops.
// For f16/bf16 scalars we cast through float for the computation.
func (e *CEmitter) emitPromotedMathScalarTail(buf *bytes.Buffer, mathOp string, prf *CIntrinsicProfile) {
	fmt.Fprintf(buf, "    // Scalar tail (promote to float, compute, demote)\n")
	fmt.Fprintf(buf, "    for (; i < n; i++) {\n")
	if prf.ScalarPromote != "" && prf.ScalarDemote != "" {
		fmt.Fprintf(buf, "        float xf = %s(input[i]);\n", prf.ScalarPromote)
		fmt.Fprintf(buf, "        float rf = xf; // scalar %s approximation\n", mathOp)
		fmt.Fprintf(buf, "        result[i] = %s(rf);\n", prf.ScalarDemote)
	} else {
		fmt.Fprintf(buf, "        result[i] = input[i]; // scalar %s not implemented\n", mathOp)
	}
	fmt.Fprintf(buf, "    }\n")
}

// ---------------------------------------------------------------------------
// NEON f32 computation helpers (used by promoted-math for NEON f16/bf16).
// These are equivalent to the existing emitExpComputation etc. but operate
// on explicitly named variables rather than indexed vectors, so they can be
// called from the promoted-math emitters for each half.
// ---------------------------------------------------------------------------

func (e *CEmitter) emitExpComputationNEONF32(buf *bytes.Buffer, x, res, indent, vecType string) {
	fmt.Fprintf(buf, "%s// Overflow/underflow check\n", indent)
	fmt.Fprintf(buf, "%suint32x4_t over_%s = vcgtq_f32(%s, overflow);\n", indent, res, x)
	fmt.Fprintf(buf, "%suint32x4_t under_%s = vcltq_f32(%s, underflow);\n", indent, res, x)
	fmt.Fprintf(buf, "%s// Range reduction\n", indent)
	fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f32(vmulq_f32(%s, invLn2));\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f32(%s, kf_%s, ln2Hi);\n", indent, vecType, res, x, res)
	fmt.Fprintf(buf, "%sr_%s = vfmsq_f32(r_%s, kf_%s, ln2Lo);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s// Horner polynomial\n", indent)
	fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f32(c5, c6, r_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c4, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c3, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c2, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s// Scale by 2^k\n", indent)
	fmt.Fprintf(buf, "%sint32x4_t ki_%s = vcvtnq_s32_f32(kf_%s);\n", indent, res, res)
	fmt.Fprintf(buf, "%sint32x4_t scale_bits_%s = vshlq_n_s32(vaddq_s32(ki_%s, bias), 23);\n", indent, res, res)
	fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f32_s32(scale_bits_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s %s = vmulq_f32(p_%s, scale_%s);\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%s// Handle overflow/underflow\n", indent)
	fmt.Fprintf(buf, "%s%s = vbslq_f32(over_%s, inf, %s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s%s = vbslq_f32(under_%s, zero, %s);\n", indent, res, res, res)
}

func (e *CEmitter) emitSigmoidComputationNEONF32(buf *bytes.Buffer, x, res, indent, vecType string) {
	fmt.Fprintf(buf, "%s// Clamp input\n", indent)
	fmt.Fprintf(buf, "%s%s clamped_%s = vmaxq_f32(vminq_f32(%s, satHi), satLo);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s negX_%s = vnegq_f32(clamped_%s);\n", indent, vecType, res, res)
	// Inline exp(-x)
	fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f32(vmulq_f32(negX_%s, invLn2));\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f32(negX_%s, kf_%s, ln2Hi);\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%sr_%s = vfmsq_f32(r_%s, kf_%s, ln2Lo);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f32(c5, c6, r_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c4, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c3, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c2, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sint32x4_t ki_%s = vcvtnq_s32_f32(kf_%s);\n", indent, res, res)
	fmt.Fprintf(buf, "%sint32x4_t scale_bits_%s = vshlq_n_s32(vaddq_s32(ki_%s, bias), 23);\n", indent, res, res)
	fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f32_s32(scale_bits_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s expNegX_%s = vmulq_f32(p_%s, scale_%s);\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%s// sigmoid = 1 / (1 + exp(-x))\n", indent)
	fmt.Fprintf(buf, "%s%s denom_%s = vaddq_f32(one, expNegX_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s %s = vdivq_f32(one, denom_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s// Saturation\n", indent)
	fmt.Fprintf(buf, "%s%s = vbslq_f32(vcgtq_f32(%s, satHi), one, %s);\n", indent, res, x, res)
	fmt.Fprintf(buf, "%s%s = vbslq_f32(vcltq_f32(%s, satLo), zero, %s);\n", indent, res, x, res)
}

func (e *CEmitter) emitErfComputationNEONF32(buf *bytes.Buffer, x, res, indent, vecType string) {
	fmt.Fprintf(buf, "%s// Get sign and absolute value\n", indent)
	fmt.Fprintf(buf, "%suint32x4_t isNeg_%s = vcltq_f32(%s, zero);\n", indent, res, x)
	fmt.Fprintf(buf, "%s%s absX_%s = vabsq_f32(%s);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s t_%s = vdivq_f32(one, vfmaq_f32(one, erfP, absX_%s));\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s negX2_%s = vnegq_f32(vmulq_f32(%s, %s));\n", indent, vecType, res, x, x)
	// exp(-x^2)
	fmt.Fprintf(buf, "%s%s kf_%s = vrndnq_f32(vmulq_f32(negX2_%s, invLn2));\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s r_%s = vfmsq_f32(negX2_%s, kf_%s, ln2Hi);\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%sr_%s = vfmsq_f32(r_%s, kf_%s, ln2Lo);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s%s p_%s = vfmaq_f32(c5, c6, r_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c4, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c3, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c2, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = vfmaq_f32(c1, p_%s, r_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sint32x4_t ki_%s = vcvtnq_s32_f32(kf_%s);\n", indent, res, res)
	fmt.Fprintf(buf, "%sint32x4_t scale_bits_%s = vshlq_n_s32(vaddq_s32(ki_%s, bias), 23);\n", indent, res, res)
	fmt.Fprintf(buf, "%s%s scale_%s = vreinterpretq_f32_s32(scale_bits_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s expNegX2_%s = vmulq_f32(p_%s, scale_%s);\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%s// Polynomial\n", indent)
	fmt.Fprintf(buf, "%s%s poly_%s = a5;\n", indent, vecType, res)
	fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a4, poly_%s, t_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a3, poly_%s, t_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a2, poly_%s, t_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%spoly_%s = vfmaq_f32(a1, poly_%s, t_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%spoly_%s = vmulq_f32(poly_%s, t_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s%s erfAbs_%s = vfmsq_f32(one, poly_%s, expNegX2_%s);\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%s// Apply sign\n", indent)
	fmt.Fprintf(buf, "%s%s %s = vbslq_f32(isNeg_%s, vnegq_f32(erfAbs_%s), erfAbs_%s);\n", indent, vecType, res, res, res, res)
}

// ---------------------------------------------------------------------------
// AVX f32 computation helpers (used by promoted-math for AVX2 f16/bf16).
// ---------------------------------------------------------------------------

func (e *CEmitter) emitExpComputationAVX(buf *bytes.Buffer, x, res, indent, vecType string) {
	fmt.Fprintf(buf, "%s// Overflow/underflow check\n", indent)
	fmt.Fprintf(buf, "%s%s over_%s = _mm256_cmp_ps(%s, overflow, _CMP_GT_OQ);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s under_%s = _mm256_cmp_ps(%s, underflow, _CMP_LT_OQ);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s// Range reduction\n", indent)
	fmt.Fprintf(buf, "%s%s kf_%s = _mm256_round_ps(_mm256_mul_ps(%s, invLn2), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s r_%s = _mm256_sub_ps(%s, _mm256_mul_ps(kf_%s, ln2Hi));\n", indent, vecType, res, x, res)
	fmt.Fprintf(buf, "%sr_%s = _mm256_sub_ps(r_%s, _mm256_mul_ps(kf_%s, ln2Lo));\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s// Horner polynomial\n", indent)
	fmt.Fprintf(buf, "%s%s p_%s = _mm256_fmadd_ps(c6, r_%s, c5);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c4);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c3);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c2);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c1);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c1);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s// Scale by 2^k\n", indent)
	fmt.Fprintf(buf, "%s__m256i ki_%s = _mm256_cvtps_epi32(kf_%s);\n", indent, res, res)
	fmt.Fprintf(buf, "%s__m256i scale_bits_%s = _mm256_slli_epi32(_mm256_add_epi32(ki_%s, bias), 23);\n", indent, res, res)
	fmt.Fprintf(buf, "%s%s scale_%s = _mm256_castsi256_ps(scale_bits_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s %s = _mm256_mul_ps(p_%s, scale_%s);\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%s// Handle overflow/underflow\n", indent)
	fmt.Fprintf(buf, "%s%s = _mm256_blendv_ps(%s, inf, over_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s%s = _mm256_blendv_ps(%s, zero, under_%s);\n", indent, res, res, res)
}

func (e *CEmitter) emitSigmoidComputationAVX(buf *bytes.Buffer, x, res, indent, vecType string) {
	fmt.Fprintf(buf, "%s// Clamp input\n", indent)
	fmt.Fprintf(buf, "%s%s clamped_%s = _mm256_max_ps(_mm256_min_ps(%s, satHi), satLo);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s negX_%s = _mm256_sub_ps(zero, clamped_%s);\n", indent, vecType, res, res)
	// Inline exp(-x)
	fmt.Fprintf(buf, "%s%s kf_%s = _mm256_round_ps(_mm256_mul_ps(negX_%s, invLn2), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s r_%s = _mm256_sub_ps(negX_%s, _mm256_mul_ps(kf_%s, ln2Hi));\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%sr_%s = _mm256_sub_ps(r_%s, _mm256_mul_ps(kf_%s, ln2Lo));\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s%s p_%s = _mm256_fmadd_ps(c6, r_%s, c5);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c4);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c3);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c2);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c1);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c1);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s__m256i ki_%s = _mm256_cvtps_epi32(kf_%s);\n", indent, res, res)
	fmt.Fprintf(buf, "%s__m256i scale_bits_%s = _mm256_slli_epi32(_mm256_add_epi32(ki_%s, bias), 23);\n", indent, res, res)
	fmt.Fprintf(buf, "%s%s scale_%s = _mm256_castsi256_ps(scale_bits_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s expNegX_%s = _mm256_mul_ps(p_%s, scale_%s);\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%s// sigmoid = 1 / (1 + exp(-x))\n", indent)
	fmt.Fprintf(buf, "%s%s denom_%s = _mm256_add_ps(one, expNegX_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s %s = _mm256_div_ps(one, denom_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s// Saturation\n", indent)
	fmt.Fprintf(buf, "%s%s satMaskHi_%s = _mm256_cmp_ps(%s, satHi, _CMP_GT_OQ);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s satMaskLo_%s = _mm256_cmp_ps(%s, satLo, _CMP_LT_OQ);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s = _mm256_blendv_ps(%s, one, satMaskHi_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s%s = _mm256_blendv_ps(%s, zero, satMaskLo_%s);\n", indent, res, res, res)
}

func (e *CEmitter) emitErfComputationAVX(buf *bytes.Buffer, x, res, indent, vecType string) {
	fmt.Fprintf(buf, "%s// Get sign and absolute value\n", indent)
	fmt.Fprintf(buf, "%s%s signMask_%s = _mm256_cmp_ps(%s, zero, _CMP_LT_OQ);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s absX_%s = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), %s);\n", indent, vecType, res, x)
	fmt.Fprintf(buf, "%s%s t_%s = _mm256_div_ps(one, _mm256_fmadd_ps(erfP, absX_%s, one));\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s negX2_%s = _mm256_sub_ps(zero, _mm256_mul_ps(%s, %s));\n", indent, vecType, res, x, x)
	// exp(-x^2)
	fmt.Fprintf(buf, "%s%s kf_%s = _mm256_round_ps(_mm256_mul_ps(negX2_%s, invLn2), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s r_%s = _mm256_sub_ps(negX2_%s, _mm256_mul_ps(kf_%s, ln2Hi));\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%sr_%s = _mm256_sub_ps(r_%s, _mm256_mul_ps(kf_%s, ln2Lo));\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s%s p_%s = _mm256_fmadd_ps(c6, r_%s, c5);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c4);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c3);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c2);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c1);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%sp_%s = _mm256_fmadd_ps(p_%s, r_%s, c1);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s__m256i ki_%s = _mm256_cvtps_epi32(kf_%s);\n", indent, res, res)
	fmt.Fprintf(buf, "%s__m256i scale_bits_%s = _mm256_slli_epi32(_mm256_add_epi32(ki_%s, bias), 23);\n", indent, res, res)
	fmt.Fprintf(buf, "%s%s scale_%s = _mm256_castsi256_ps(scale_bits_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s expNegX2_%s = _mm256_mul_ps(p_%s, scale_%s);\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%s// Polynomial\n", indent)
	fmt.Fprintf(buf, "%s%s poly_%s = a5;\n", indent, vecType, res)
	fmt.Fprintf(buf, "%spoly_%s = _mm256_fmadd_ps(poly_%s, t_%s, a4);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%spoly_%s = _mm256_fmadd_ps(poly_%s, t_%s, a3);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%spoly_%s = _mm256_fmadd_ps(poly_%s, t_%s, a2);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%spoly_%s = _mm256_fmadd_ps(poly_%s, t_%s, a1);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%spoly_%s = _mm256_mul_ps(poly_%s, t_%s);\n", indent, res, res, res)
	fmt.Fprintf(buf, "%s// erf = 1 - poly * exp(-x^2)\n", indent)
	fmt.Fprintf(buf, "%s%s erfAbs_%s = _mm256_sub_ps(one, _mm256_mul_ps(poly_%s, expNegX2_%s));\n", indent, vecType, res, res, res)
	fmt.Fprintf(buf, "%s// Apply sign\n", indent)
	fmt.Fprintf(buf, "%s%s negErf_%s = _mm256_sub_ps(zero, erfAbs_%s);\n", indent, vecType, res, res)
	fmt.Fprintf(buf, "%s%s %s = _mm256_blendv_ps(erfAbs_%s, negErf_%s, signMask_%s);\n", indent, vecType, res, res, res, res)
}

// ---------------------------------------------------------------------------
// Profile intrinsic lookup helpers
// ---------------------------------------------------------------------------

// profileOpFn returns the intrinsic function name for an operation, checking
// the profile's op maps.  It uses the first non-scalar tier as the default.
func (e *CEmitter) profileOpFn(opName string) string {
	if e.profile == nil {
		return ""
	}
	prf := e.profile

	// Check the dedicated fields first.
	switch opName {
	case "Add":
		return mapFirstValue(prf.AddFn)
	case "Sub":
		return mapFirstValue(prf.SubFn)
	case "Mul":
		return mapFirstValue(prf.MulFn)
	case "Div":
		return mapFirstValue(prf.DivFn)
	case "Min":
		return mapFirstValue(prf.MinFn)
	case "Max":
		return mapFirstValue(prf.MaxFn)
	case "Neg":
		return mapFirstValue(prf.NegFn)
	case "Abs":
		return mapFirstValue(prf.AbsFn)
	case "Sqrt":
		return mapFirstValue(prf.SqrtFn)
	}
	return ""
}

// profileTierOpFn returns the intrinsic for a specific tier, or "" if not found.
func (e *CEmitter) profileTierOpFn(opName, tierName string) string {
	if e.profile == nil {
		return ""
	}
	prf := e.profile

	var m map[string]string
	switch opName {
	case "Add":
		m = prf.AddFn
	case "Sub":
		m = prf.SubFn
	case "Mul":
		m = prf.MulFn
	case "Div":
		m = prf.DivFn
	case "Min":
		m = prf.MinFn
	case "Max":
		m = prf.MaxFn
	case "Neg":
		m = prf.NegFn
	case "Abs":
		m = prf.AbsFn
	case "Sqrt":
		m = prf.SqrtFn
	}
	if m == nil {
		return ""
	}
	return m[tierName]
}

// scalarBinaryOp returns the C infix operator for a binary op, or "" for
// operations that require conditional logic (Min, Max).
func scalarBinaryOp(opName string) string {
	switch opName {
	case "Add":
		return "+"
	case "Sub":
		return "-"
	case "Mul":
		return "*"
	case "Div":
		return "/"
	}
	return ""
}

// mapFirstValue returns the first value from a string map (arbitrary order).
func mapFirstValue(m map[string]string) string {
	for _, v := range m {
		return v
	}
	return ""
}

// ptrExpr returns the pointer expression for a load or store intrinsic.
// If CastExpr is set (e.g. "(float16_t*)"), it wraps the pointer arithmetic:
//
//	(float16_t*)(a + i + 0)
//
// If CastExpr is empty, it returns the bare pointer arithmetic:
//
//	a + i + 0
func (p *CIntrinsicProfile) ptrExpr(ptrName string, offset int) string {
	var base string
	if offset == 0 {
		base = ptrName + " + i"
	} else {
		base = fmt.Sprintf("%s + i + %d", ptrName, offset)
	}
	if p.CastExpr == "" {
		return base
	}
	return fmt.Sprintf("%s(%s)", p.CastExpr, base)
}

// Placeholder for unused AST argument
var _ ast.Node
