 So it's actually the NEON Blocked MatMul showing up in the profile, does that mean the matrix has a specific shape?

 Yes. Looking at the dispatch logic in z_matmul_arm64.go:415-451, the SME blocked path falls back to NEON in several cases:

 Alignment requirements (line 418):
 if m%16 != 0 || n%16 != 0 || k%16 != 0 {
     asm.MatMulNEONF32(a, b, c, m, n, k)
     return
 }

 Small dimension threshold (line 424):
 if m < minDimForBlockedSME || n < minDimForBlockedSME || k < minDimForBlockedSME {
     asm.MatMulNEONF32(a, b, c, m, n, k)
     return
 }

 Where minDimForBlockedSME = 32.

 So you're hitting NEON blocked if any of M, N, or K is:
 - Not a multiple of 16, or
 - Less than 32

 The most common case in transformer workloads would be M not being a multiple of 16 -- M is typically the batch size or sequence length, which can be any value (1 for single-token inference, 7 for a prompt, etc.). N and K are usually model
 dimensions (multiples of 64 or 128), so those are rarely the culprit.

 Can you check what M, N, K values are being passed? If M is small (like 1-8 for token-at-a-time inference), that's expected behavior -- SME's 16×16 tile minimum makes it unsuitable for M < 16. But if M is something like 17 or 33 (just barely
 not aligned), you could pad to the next multiple of 16 and get a massive speedup by hitting the SME path instead.

 Should we add padding to our version?

 Could you look at the model we're using ```⏺ Bash(cat /Users/ajroetker/go/src/github.com/antflydb/antfly/termite/models/embedders/Snowflake/snowflake-arctic-embed-l-v2.0/config.json 2>/dev/null | python3 -m jso…)
 ⎿  {
 "word_embedding_dimension": 1024,
 "pooling_mode_cls_token": true,
 … +7 lines (ctrl+o to expand)
 ⎿  Interrupted · What should Claude do instead?

 Explore(Find model config and dimensions)
 ⎿  Done (24 tool uses · 91.1k tokens · 3m 10s)

 This clarifies the situation. The model dimensions are:
 ┌─────────────────────────┬─────┬──────┬──────┐
 │ Operation (×24 layers)  │  M  │  K   │  N   │
 ├─────────────────────────┼─────┼──────┼──────┤
 │ QKV projections (×3)    │ S   │ 1024 │ 1024 │
 ├─────────────────────────┼─────┼──────┼──────┤
 │ Attn scores (×16 heads) │ S   │ 64   │ S    │
 ├─────────────────────────┼─────┼──────┼──────┤
 │ Attn × V (×16 heads)    │ S   │ S    │ 64   │
 ├─────────────────────────┼─────┼──────┼──────┤
 │ Output projection       │ S   │ 1024 │ 1024 │
 ├─────────────────────────┼─────┼──────┼──────┤
 │ FFN up                  │ S   │ 1024 │ 4096 │
 ├─────────────────────────┼─────┼──────┼──────┤
 │ FFN down                │ S   │ 4096 │ 1024 │
 └─────────────────────────┴─────┴──────┴──────┘
 N and K are always 64, 1024, or 4096 -- all multiples of 16. The problem is M = S (sequence length), which is whatever the input text tokenizes to. A 50-word sentence might tokenize to 73 tokens, which fails the m%16 != 0 check and drops to
 NEON.

 Padding M makes sense here because:

 - You only need to pad the input rows of A (and corresponding output rows of C) up to the next multiple of 16
 - The extra output rows are garbage that the caller ignores -- the actual result in rows [0, M) is correct
 - N and K are already aligned, so no padding needed there
 - The cost of padding is small: at worst you compute 15 extra rows, but you gain SME's ~300-500 GFLOPS vs NEON's ~25-75 GFLOPS

 For the attention score matmuls where N = S too, you'd need to pad both M and N, which means padding the B matrix columns as well (zero-padded, so the extra attention scores are zero and don't affect softmax meaningfully -- though you'd want
 to mask them properly).

 The cleanest place to add padding would be in blockedMatMulFMOPA and matmulKLastFMOPA in z_matmul_arm64.go, so it's transparent to callers. Want me to implement that?

 Churned for 2m 4s

 Yes please!

 Entered plan mode
 Claude is now exploring and designing an implementation approach.

 Explore(Explore padding patterns and SME dispatch)
 ⎿  Done (23 tool uses · 91.4k tokens · 2m 6s)

 Read 2 files (ctrl+o to expand)

 Now I have all the context needed. Let me write the plan.

 Updated plan
 ⎿  /plan to preview

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Ready to code?

Here is Claude's plan:
╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
Plan: Add M-dimension padding to SME FMOPA matmul paths

Problem

SME FMOPA requires M to be a multiple of 16 (f32) or 8 (f64). When M is the sequence length (e.g., 73 tokens), the alignment check fails and falls back to NEON (~25-75 GFLOPS vs SME's ~300-500 GFLOPS). This affects both direct calls and
parallel strip processing (e.g., M=100 → strips of 64 + 36, where 36 is not aligned).

Approach

Add internal M-padding in each SME FMOPA function in z_matmul_arm64.go. When M is not tile-aligned:

1. Allocate a padded A buffer (paddedM × K), copy A data, zero the extra rows
2. Allocate a padded C buffer (paddedM × N) for output
3. Run FMOPA with paddedM instead of m
4. Copy the first m × n elements from padded C to caller's C
5. Return buffers to pools

When M is already aligned, no overhead — the existing fast path is unchanged.

Key design decisions

- Pad only M: N and K are model dimensions (64, 1024, 4096) — always aligned. Only M (sequence length / strip size) is variable.
- Padding is internal: Callers size output buffers as exactly m*n. We cannot write extra rows to caller's C. Must use a temporary padded C buffer.
- Use the padded M for the min-size threshold: paddedM >= 32 instead of m >= 32. This lets M=17 (paddedM=32) hit SME. M <= 16 (paddedM=16 < 32) stays on NEON, which is correct since streaming mode overhead dominates at that size.
- Pool-based buffer management: Add sync.Pool instances for padded A and C buffers, matching the existing pattern.
- C output copy is contiguous: Since both padded C and caller's C are row-major with stride N, the first m*n elements are identical in layout. A single copy(c[:m*n], paddedC[:m*n]) suffices (no row-by-row copy needed).

Files to modify

hwy/contrib/matmul/z_matmul_arm64.go

Add pools (after existing pool declarations, ~line 90):
- paddedAPool32, paddedCPool32 — for f32 padding buffers
- paddedAPool64, paddedCPool64 — for f64 padding buffers
- paddedAPoolF16, paddedCPoolF16 — for f16 padding buffers
- paddedAPoolBF16, paddedCPoolBF16 — for bf16 padding buffers

Add helper alignUp(m, tileSize int) int — rounds m up to next multiple of tileSize.

Modify 10 SME FMOPA functions — all in the same file, same pattern:
┌──────────────────────┬───────────┬─────────┬─────────┬─────────┐
│       Function       │ Tile size │ Pads A? │ Pads B? │ Pads C? │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ matmulFMOPA          │ 16        │ yes     │ no      │ yes     │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ matmulFMOPA64        │ 8         │ yes     │ no      │ yes     │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ matmulFMOPAF16       │ 16        │ yes     │ no      │ yes     │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ matmulFMOPABF16      │ 16        │ yes     │ no      │ yes     │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ blockedMatMulFMOPA   │ 16        │ yes     │ no      │ yes     │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ blockedMatMulFMOPA64 │ 8         │ yes     │ no      │ yes     │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ matmulKLastFMOPA     │ 16        │ yes     │ no      │ yes     │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ matmulKLastFMOPA64   │ 8         │ yes     │ no      │ yes     │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ matmulKLastFMOPAF16  │ 16        │ yes     │ no      │ yes     │
├──────────────────────┼───────────┼─────────┼─────────┼─────────┤
│ matmulKLastFMOPABF16 │ 16        │ yes     │ no      │ yes     │
└──────────────────────┴───────────┴─────────┴─────────┴─────────┘
For each function, the change is:
1. Change alignment gate from m%tile != 0 || n%tile != 0 || k%tile != 0 to n%tile != 0 || k%tile != 0 (keep N/K check, remove M check)
2. Compute paddedM := alignUp(m, tile)
3. Use paddedM for the min-size threshold check instead of m
4. If paddedM != m: get padded A from pool, copy + zero-pad, get padded C from pool
5. Use paddedM for transpose buffer sizing and FMOPA call
6. If padded: copy result back, return buffers to pools

What stays on NEON (no change)

- M <= 16 for f32 (paddedM=16 < minDimForSME=32)
- M <= 24 for f64 (paddedM=24 < 32)
- N or K not aligned to tile size (rare for model dims, correct to skip)
- Fine-grained parallel paths (M=1 per call → paddedM=16 < 32, stays on NEON as intended)

Verification

# Run existing matmul tests (should all still pass)
GOEXPERIMENT=simd go test -v ./hwy/contrib/matmul/...

# Run with non-aligned M values specifically
GOEXPERIMENT=simd go test -v -run 'TestMatMul|TestBlocked|TestKLast' ./hwy/contrib/matmul/...

# Benchmark aligned vs non-aligned M to confirm SME is used
GOEXPERIMENT=simd go test -bench='BenchmarkBlockedMatMul' -benchmem ./hwy/contrib/matmul/...

# Force fallback to verify NEON path still works
HWY_NO_SME=1 GOEXPERIMENT=simd go test ./hwy/contrib/matmul/...

# Run nn tests to verify end-to-end
GOEXPERIMENT=simd go test -v ./hwy/contrib/nn/...
