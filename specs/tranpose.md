# SIMD Transpose Implementation Plan

## Overview

Accelerate the existing `transposeMatrix` function in go-highway's `matmul` package. This function is already called internally by FMOPA matmul and can be exported for gomlx's `dgNormalizeShape`. This addresses the 40% CPU time spent on element-by-element transposition in the simplego backend.

## Current State

The scalar `transposeMatrix` in `matmul_sme_arm64.go:61-67`:
```go
func transposeMatrix[T hwy.Floats](a []T, m, k int, at []T) {
    for i := range m {
        for j := range k {
            at[j*m+i] = a[i*k+j]
        }
    }
}
```

Already used by:
- `matmulFMOPA` (float32)
- `matmulFMOPA64` (float64)
- `matmulFMOPAF16` (float16)
- `matmulFMOPABF16` (bfloat16)

## Architecture

```
go-highway/hwy/contrib/matmul/
├── transpose_base.go              # hwygen: BaseTranspose2D with hwy.Vec ops
├── transpose_neon.gen.go          # Generated by hwygen (NEON)
├── transpose_avx2.gen.go          # Generated by hwygen (AVX2)
├── transpose_avx512.gen.go        # Generated by hwygen (AVX-512)
├── transpose_fallback.gen.go      # Generated by hwygen (scalar fallback)
├── transpose.go                   # Public API + dispatch variables
├── transpose_arm64.go             # ARM64 dispatch (NEON/SME selection)
├── transpose_amd64.go             # AMD64 dispatch (AVX selection)
├── c/
│   ├── transpose_neon_arm64.c     # GOAT: NEON 4x4/8x8 tiled transpose
│   └── transpose_sme_arm64.c      # GOAT: SME vertical load / horizontal store
└── asm/
    ├── transpose_neon_wrappers.go # Go wrappers for NEON assembly
    ├── transpose_neon_arm64.s     # Generated by GOAT
    ├── transpose_sme_wrappers.go  # Go wrappers for SME assembly
    └── transpose_sme_arm64.s      # Generated by GOAT
```

## Phase 1: Base Implementation (hwygen)

### 1.1 transpose_base.go

```go
// Copyright 2025 go-highway Authors. Apache-2.0 License.

package matmul

import "github.com/ajroetker/go-highway/hwy"

//go:generate go run ../../../cmd/hwygen -input transpose_base.go -output . -targets avx2,avx512,neon,fallback -dispatch transpose

// BaseTranspose2D transposes an M×K row-major matrix to K×M.
// Uses block-based approach: load lanes×lanes block, transpose in-register, store.
func BaseTranspose2D[T hwy.Floats](src []T, m, k int, dst []T) {
    if len(src) < m*k || len(dst) < k*m {
        return
    }

    lanes := hwy.MaxLanes[T]()

    // Process lanes×lanes blocks with SIMD
    for i := 0; i <= m-lanes; i += lanes {
        for j := 0; j <= k-lanes; j += lanes {
            transposeBlockSIMD(src, dst, i, j, m, k, lanes)
        }
    }

    // Handle edges with scalar
    transposeEdgesScalar(src, m, k, dst, lanes)
}

// transposeBlockSIMD transposes a lanes×lanes block using SIMD interleave ops.
func transposeBlockSIMD[T hwy.Floats](src, dst []T, startI, startJ, m, k, lanes int) {
    // Load `lanes` rows
    rows := make([]hwy.Vec[T], lanes)
    for r := 0; r < lanes; r++ {
        rows[r] = hwy.LoadFull(src[(startI+r)*k+startJ:])
    }

    // In-register transpose using butterfly pattern with InterleaveLower/Upper
    // For 4 lanes: 2 levels of interleave
    // For 8 lanes: 3 levels of interleave
    for level := 0; (1 << level) < lanes; level++ {
        stride := 1 << level
        newRows := make([]hwy.Vec[T], lanes)
        for i := 0; i < lanes; i += 2 * stride {
            for j := 0; j < stride; j++ {
                newRows[i+j] = hwy.InterleaveLower(rows[i+j], rows[i+j+stride])
                newRows[i+j+stride] = hwy.InterleaveUpper(rows[i+j], rows[i+j+stride])
            }
        }
        rows = newRows
    }

    // Store transposed: column c of input -> row c of output
    for c := 0; c < lanes; c++ {
        hwy.StoreFull(rows[c], dst[(startJ+c)*m+startI:])
    }
}

// transposeEdgesScalar handles non-block-aligned edges.
func transposeEdgesScalar[T hwy.Floats](src []T, m, k int, dst []T, lanes int) {
    blockM := (m / lanes) * lanes
    blockK := (k / lanes) * lanes

    // Right edge: columns [blockK, k)
    for i := 0; i < m; i++ {
        for j := blockK; j < k; j++ {
            dst[j*m+i] = src[i*k+j]
        }
    }

    // Bottom edge: rows [blockM, m), columns [0, blockK)
    for i := blockM; i < m; i++ {
        for j := 0; j < blockK; j++ {
            dst[j*m+i] = src[i*k+j]
        }
    }
}
```

### 1.2 transpose.go (Public API)

```go
// Copyright 2025 go-highway Authors. Apache-2.0 License.

package matmul

import "github.com/ajroetker/go-highway/hwy"

// Transpose function variables - set by architecture-specific init()
var (
    // TransposeFloat32 transposes M×K float32 matrix to K×M.
    TransposeFloat32 = Transpose2D_fallback[float32]

    // TransposeFloat64 transposes M×K float64 matrix to K×M.
    TransposeFloat64 = Transpose2D_fallback[float64]

    // TransposeFloat16 transposes M×K float16 matrix to K×M.
    TransposeFloat16 = Transpose2D_fallback[hwy.Float16]

    // TransposeBFloat16 transposes M×K bfloat16 matrix to K×M.
    TransposeBFloat16 = Transpose2D_fallback[hwy.BFloat16]
)

// Transpose2D is the generic transpose for any float type.
// Dispatches to the appropriate typed function.
func Transpose2D[T hwy.Floats](src []T, m, k int, dst []T) {
    switch s := any(src).(type) {
    case []float32:
        TransposeFloat32(s, m, k, any(dst).([]float32))
    case []float64:
        TransposeFloat64(s, m, k, any(dst).([]float64))
    case []hwy.Float16:
        TransposeFloat16(s, m, k, any(dst).([]hwy.Float16))
    case []hwy.BFloat16:
        TransposeBFloat16(s, m, k, any(dst).([]hwy.BFloat16))
    }
}

// transposeMatrix is the internal function used by matmul.
// Now dispatches to SIMD implementation.
func transposeMatrix[T hwy.Floats](a []T, m, k int, at []T) {
    Transpose2D(a, m, k, at)
}
```

## Phase 2: NEON Implementation (GOAT)

### 2.1 c/transpose_neon_arm64.c

```c
/*
 * Copyright 2025 go-highway Authors. Apache-2.0 License.
 *
 * NEON Tiled Transpose for ARM64
 * Uses TRN1/TRN2 for efficient 4x4 (f32/f64) or 8x8 (f16/bf16) transpose.
 */

#ifndef GOAT_PARSER
#include <arm_neon.h>
#endif

// ============================================================================
// 4x4 float32 transpose kernel
// ============================================================================
static inline void transpose_4x4_f32(const float *src, float *dst,
                                      long srcStride, long dstStride) {
    // Load 4 rows
    float32x4_t r0 = vld1q_f32(src);
    float32x4_t r1 = vld1q_f32(src + srcStride);
    float32x4_t r2 = vld1q_f32(src + 2*srcStride);
    float32x4_t r3 = vld1q_f32(src + 3*srcStride);

    // Level 1: transpose pairs of 32-bit elements
    float32x4_t t0 = vtrn1q_f32(r0, r1);
    float32x4_t t1 = vtrn2q_f32(r0, r1);
    float32x4_t t2 = vtrn1q_f32(r2, r3);
    float32x4_t t3 = vtrn2q_f32(r2, r3);

    // Level 2: transpose pairs of 64-bit elements
    float32x4_t d0 = vreinterpretq_f32_f64(vtrn1q_f64(
        vreinterpretq_f64_f32(t0), vreinterpretq_f64_f32(t2)));
    float32x4_t d1 = vreinterpretq_f32_f64(vtrn1q_f64(
        vreinterpretq_f64_f32(t1), vreinterpretq_f64_f32(t3)));
    float32x4_t d2 = vreinterpretq_f32_f64(vtrn2q_f64(
        vreinterpretq_f64_f32(t0), vreinterpretq_f64_f32(t2)));
    float32x4_t d3 = vreinterpretq_f32_f64(vtrn2q_f64(
        vreinterpretq_f64_f32(t1), vreinterpretq_f64_f32(t3)));

    // Store 4 transposed rows
    vst1q_f32(dst, d0);
    vst1q_f32(dst + dstStride, d1);
    vst1q_f32(dst + 2*dstStride, d2);
    vst1q_f32(dst + 3*dstStride, d3);
}

// Full matrix transpose using 4x4 tiles
// func transpose_neon_f32(src, dst unsafe.Pointer, m, k *int64)
void transpose_neon_f32(const float *src, float *dst, long *pm, long *pk) {
    long m = *pm;
    long k = *pk;

    // Process 4x4 blocks
    long blockM = (m / 4) * 4;
    long blockK = (k / 4) * 4;

    for (long i = 0; i < blockM; i += 4) {
        for (long j = 0; j < blockK; j += 4) {
            transpose_4x4_f32(
                src + i*k + j,
                dst + j*m + i,
                k, m
            );
        }
    }

    // Right edge: columns [blockK, k)
    for (long i = 0; i < m; i++) {
        for (long j = blockK; j < k; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }

    // Bottom edge: rows [blockM, m), columns [0, blockK)
    for (long i = blockM; i < m; i++) {
        for (long j = 0; j < blockK; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
}

// ============================================================================
// 2x2 float64 transpose kernel
// ============================================================================
static inline void transpose_2x2_f64(const double *src, double *dst,
                                      long srcStride, long dstStride) {
    float64x2_t r0 = vld1q_f64(src);
    float64x2_t r1 = vld1q_f64(src + srcStride);

    float64x2_t d0 = vtrn1q_f64(r0, r1);
    float64x2_t d1 = vtrn2q_f64(r0, r1);

    vst1q_f64(dst, d0);
    vst1q_f64(dst + dstStride, d1);
}

// func transpose_neon_f64(src, dst unsafe.Pointer, m, k *int64)
void transpose_neon_f64(const double *src, double *dst, long *pm, long *pk) {
    long m = *pm;
    long k = *pk;

    long blockM = (m / 2) * 2;
    long blockK = (k / 2) * 2;

    for (long i = 0; i < blockM; i += 2) {
        for (long j = 0; j < blockK; j += 2) {
            transpose_2x2_f64(src + i*k + j, dst + j*m + i, k, m);
        }
    }

    // Edges
    for (long i = 0; i < m; i++) {
        for (long j = blockK; j < k; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
    for (long i = blockM; i < m; i++) {
        for (long j = 0; j < blockK; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
}

// ============================================================================
// 8x8 float16 transpose kernel
// ============================================================================
static inline void transpose_8x8_f16(const __fp16 *src, __fp16 *dst,
                                      long srcStride, long dstStride) {
    // Load 8 rows
    float16x8_t r0 = vld1q_f16(src);
    float16x8_t r1 = vld1q_f16(src + srcStride);
    float16x8_t r2 = vld1q_f16(src + 2*srcStride);
    float16x8_t r3 = vld1q_f16(src + 3*srcStride);
    float16x8_t r4 = vld1q_f16(src + 4*srcStride);
    float16x8_t r5 = vld1q_f16(src + 5*srcStride);
    float16x8_t r6 = vld1q_f16(src + 6*srcStride);
    float16x8_t r7 = vld1q_f16(src + 7*srcStride);

    // Level 1: 16-bit interleave
    float16x8_t t0 = vtrn1q_f16(r0, r1);
    float16x8_t t1 = vtrn2q_f16(r0, r1);
    float16x8_t t2 = vtrn1q_f16(r2, r3);
    float16x8_t t3 = vtrn2q_f16(r2, r3);
    float16x8_t t4 = vtrn1q_f16(r4, r5);
    float16x8_t t5 = vtrn2q_f16(r4, r5);
    float16x8_t t6 = vtrn1q_f16(r6, r7);
    float16x8_t t7 = vtrn2q_f16(r6, r7);

    // Level 2: 32-bit interleave (via reinterpret)
    float32x4_t s0 = vtrn1q_f32(vreinterpretq_f32_f16(t0), vreinterpretq_f32_f16(t2));
    float32x4_t s1 = vtrn2q_f32(vreinterpretq_f32_f16(t0), vreinterpretq_f32_f16(t2));
    float32x4_t s2 = vtrn1q_f32(vreinterpretq_f32_f16(t1), vreinterpretq_f32_f16(t3));
    float32x4_t s3 = vtrn2q_f32(vreinterpretq_f32_f16(t1), vreinterpretq_f32_f16(t3));
    float32x4_t s4 = vtrn1q_f32(vreinterpretq_f32_f16(t4), vreinterpretq_f32_f16(t6));
    float32x4_t s5 = vtrn2q_f32(vreinterpretq_f32_f16(t4), vreinterpretq_f32_f16(t6));
    float32x4_t s6 = vtrn1q_f32(vreinterpretq_f32_f16(t5), vreinterpretq_f32_f16(t7));
    float32x4_t s7 = vtrn2q_f32(vreinterpretq_f32_f16(t5), vreinterpretq_f32_f16(t7));

    // Level 3: 64-bit interleave
    float16x8_t d0 = vreinterpretq_f16_f64(vtrn1q_f64(vreinterpretq_f64_f32(s0), vreinterpretq_f64_f32(s4)));
    float16x8_t d1 = vreinterpretq_f16_f64(vtrn1q_f64(vreinterpretq_f64_f32(s2), vreinterpretq_f64_f32(s6)));
    float16x8_t d2 = vreinterpretq_f16_f64(vtrn1q_f64(vreinterpretq_f64_f32(s1), vreinterpretq_f64_f32(s5)));
    float16x8_t d3 = vreinterpretq_f16_f64(vtrn1q_f64(vreinterpretq_f64_f32(s3), vreinterpretq_f64_f32(s7)));
    float16x8_t d4 = vreinterpretq_f16_f64(vtrn2q_f64(vreinterpretq_f64_f32(s0), vreinterpretq_f64_f32(s4)));
    float16x8_t d5 = vreinterpretq_f16_f64(vtrn2q_f64(vreinterpretq_f64_f32(s2), vreinterpretq_f64_f32(s6)));
    float16x8_t d6 = vreinterpretq_f16_f64(vtrn2q_f64(vreinterpretq_f64_f32(s1), vreinterpretq_f64_f32(s5)));
    float16x8_t d7 = vreinterpretq_f16_f64(vtrn2q_f64(vreinterpretq_f64_f32(s3), vreinterpretq_f64_f32(s7)));

    // Store
    vst1q_f16(dst, d0);
    vst1q_f16(dst + dstStride, d1);
    vst1q_f16(dst + 2*dstStride, d2);
    vst1q_f16(dst + 3*dstStride, d3);
    vst1q_f16(dst + 4*dstStride, d4);
    vst1q_f16(dst + 5*dstStride, d5);
    vst1q_f16(dst + 6*dstStride, d6);
    vst1q_f16(dst + 7*dstStride, d7);
}

// func transpose_neon_f16(src, dst unsafe.Pointer, m, k *int64)
void transpose_neon_f16(__fp16 *src, __fp16 *dst, long *pm, long *pk) {
    long m = *pm;
    long k = *pk;

    long blockM = (m / 8) * 8;
    long blockK = (k / 8) * 8;

    for (long i = 0; i < blockM; i += 8) {
        for (long j = 0; j < blockK; j += 8) {
            transpose_8x8_f16(src + i*k + j, dst + j*m + i, k, m);
        }
    }

    // Edges (scalar)
    for (long i = 0; i < m; i++) {
        for (long j = blockK; j < k; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
    for (long i = blockM; i < m; i++) {
        for (long j = 0; j < blockK; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
}

// BFloat16 uses same 8x8 pattern (same size as f16)
// func transpose_neon_bf16(src, dst unsafe.Pointer, m, k *int64)
void transpose_neon_bf16(void *src, void *dst, long *pm, long *pk) {
    // bfloat16 is same size as float16, use same kernel
    transpose_neon_f16((__fp16*)src, (__fp16*)dst, pm, pk);
}
```

### 2.2 asm/transpose_neon_wrappers.go

```go
//go:build !noasm && arm64

package asm

import (
    "unsafe"
    "github.com/ajroetker/go-highway/hwy"
)

//go:generate go tool goat ../c/transpose_neon_arm64.c -O3 --target arm64 -e="-march=armv8.2-a+fp16"

func TransposeNEONF32(src, dst []float32, m, k int) {
    if m == 0 || k == 0 { return }
    mVal, kVal := int64(m), int64(k)
    transpose_neon_f32(
        unsafe.Pointer(&src[0]), unsafe.Pointer(&dst[0]),
        unsafe.Pointer(&mVal), unsafe.Pointer(&kVal))
}

func TransposeNEONF64(src, dst []float64, m, k int) {
    if m == 0 || k == 0 { return }
    mVal, kVal := int64(m), int64(k)
    transpose_neon_f64(
        unsafe.Pointer(&src[0]), unsafe.Pointer(&dst[0]),
        unsafe.Pointer(&mVal), unsafe.Pointer(&kVal))
}

func TransposeNEONF16(src, dst []hwy.Float16, m, k int) {
    if m == 0 || k == 0 { return }
    mVal, kVal := int64(m), int64(k)
    transpose_neon_f16(
        unsafe.Pointer(&src[0]), unsafe.Pointer(&dst[0]),
        unsafe.Pointer(&mVal), unsafe.Pointer(&kVal))
}

func TransposeNEONBF16(src, dst []hwy.BFloat16, m, k int) {
    if m == 0 || k == 0 { return }
    mVal, kVal := int64(m), int64(k)
    transpose_neon_bf16(
        unsafe.Pointer(&src[0]), unsafe.Pointer(&dst[0]),
        unsafe.Pointer(&mVal), unsafe.Pointer(&kVal))
}
```

## Phase 3: SME Implementation (GOAT)

### 3.1 c/transpose_sme_arm64.c

```c
/*
 * Copyright 2025 go-highway Authors. Apache-2.0 License.
 *
 * SME Transpose for Apple Silicon M4+
 * Key insight: Load rows into ZA columns (vertical), store ZA rows (horizontal).
 * The matrix tile handles the data reorganization at memory bandwidth speed.
 */

#ifndef GOAT_PARSER
#include <arm_sme.h>
#endif

// SME tile size depends on SVL (Streaming Vector Length)
// Apple M4: SVL = 512 bits = 16 float32 = 32 float16

// ============================================================================
// SME 16x16 float32 transpose
// ============================================================================
// Load rows into ZA tile columns, store tile rows to output
// This achieves transpose "for free" via the matrix coprocessor

__arm_locally_streaming __arm_new("za")
void transpose_sme_tile_f32(const float *src, float *dst,
                             long srcStride, long dstStride) {
    svbool_t pg = svptrue_b32();  // Predicate for 16 f32 elements

    // Load 16 source rows into ZA tile columns (vertical writes)
    // ZA column i gets source row i
    for (int i = 0; i < 16; i++) {
        svfloat32_t row = svld1_f32(pg, src + i * srcStride);
        svwrite_ver_za32_f32_m(0, i, pg, row);
    }

    // Store ZA tile rows (horizontal reads) to destination
    // ZA row j is column j of source = row j of transposed output
    for (int j = 0; j < 16; j++) {
        svfloat32_t col = svread_hor_za32_f32_m(svundef_f32(), pg, 0, j);
        svst1_f32(pg, dst + j * dstStride, col);
    }
}

// Full matrix transpose using SME 16x16 tiles
// func transpose_sme_f32(src, dst unsafe.Pointer, m, k *int64)
__arm_locally_streaming
void transpose_sme_f32(const float *src, float *dst, long *pm, long *pk) {
    long m = *pm;
    long k = *pk;

    long blockM = (m / 16) * 16;
    long blockK = (k / 16) * 16;

    // Process 16x16 tiles with SME
    for (long i = 0; i < blockM; i += 16) {
        for (long j = 0; j < blockK; j += 16) {
            transpose_sme_tile_f32(
                src + i*k + j,
                dst + j*m + i,
                k, m);
        }
    }

    // Fall back to NEON for edges (SME streaming mode overhead not worth it)
    // Right edge
    for (long i = 0; i < m; i++) {
        for (long j = blockK; j < k; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
    // Bottom edge
    for (long i = blockM; i < m; i++) {
        for (long j = 0; j < blockK; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
}

// ============================================================================
// SME 8x8 float64 transpose
// ============================================================================
__arm_locally_streaming __arm_new("za")
void transpose_sme_tile_f64(const double *src, double *dst,
                             long srcStride, long dstStride) {
    svbool_t pg = svptrue_b64();  // Predicate for 8 f64 elements

    for (int i = 0; i < 8; i++) {
        svfloat64_t row = svld1_f64(pg, src + i * srcStride);
        svwrite_ver_za64_f64_m(0, i, pg, row);
    }

    for (int j = 0; j < 8; j++) {
        svfloat64_t col = svread_hor_za64_f64_m(svundef_f64(), pg, 0, j);
        svst1_f64(pg, dst + j * dstStride, col);
    }
}

// func transpose_sme_f64(src, dst unsafe.Pointer, m, k *int64)
__arm_locally_streaming
void transpose_sme_f64(const double *src, double *dst, long *pm, long *pk) {
    long m = *pm;
    long k = *pk;

    long blockM = (m / 8) * 8;
    long blockK = (k / 8) * 8;

    for (long i = 0; i < blockM; i += 8) {
        for (long j = 0; j < blockK; j += 8) {
            transpose_sme_tile_f64(src + i*k + j, dst + j*m + i, k, m);
        }
    }

    // Edges
    for (long i = 0; i < m; i++) {
        for (long j = blockK; j < k; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
    for (long i = blockM; i < m; i++) {
        for (long j = 0; j < blockK; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
}

// ============================================================================
// SME 32x32 float16 transpose (SVL=512 means 32 f16 per vector)
// ============================================================================
__arm_locally_streaming __arm_new("za")
void transpose_sme_tile_f16(const __fp16 *src, __fp16 *dst,
                             long srcStride, long dstStride) {
    svbool_t pg = svptrue_b16();  // 32 f16 elements

    for (int i = 0; i < 32; i++) {
        svfloat16_t row = svld1_f16(pg, src + i * srcStride);
        svwrite_ver_za16_f16_m(0, i, pg, row);
    }

    for (int j = 0; j < 32; j++) {
        svfloat16_t col = svread_hor_za16_f16_m(svundef_f16(), pg, 0, j);
        svst1_f16(pg, dst + j * dstStride, col);
    }
}

// func transpose_sme_f16(src, dst unsafe.Pointer, m, k *int64)
__arm_locally_streaming
void transpose_sme_f16(const __fp16 *src, __fp16 *dst, long *pm, long *pk) {
    long m = *pm;
    long k = *pk;

    long blockM = (m / 32) * 32;
    long blockK = (k / 32) * 32;

    for (long i = 0; i < blockM; i += 32) {
        for (long j = 0; j < blockK; j += 32) {
            transpose_sme_tile_f16(src + i*k + j, dst + j*m + i, k, m);
        }
    }

    // Edges
    for (long i = 0; i < m; i++) {
        for (long j = blockK; j < k; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
    for (long i = blockM; i < m; i++) {
        for (long j = 0; j < blockK; j++) {
            dst[j*m + i] = src[i*k + j];
        }
    }
}

// BFloat16
// func transpose_sme_bf16(src, dst unsafe.Pointer, m, k *int64)
__arm_locally_streaming
void transpose_sme_bf16(void *src, void *dst, long *pm, long *pk) {
    transpose_sme_f16((__fp16*)src, (__fp16*)dst, pm, pk);
}
```

### 3.2 asm/transpose_sme_wrappers.go

```go
//go:build !noasm && darwin && arm64

package asm

import (
    "unsafe"
    "github.com/ajroetker/go-highway/hwy"
)

//go:generate go tool goat ../c/transpose_sme_arm64.c -O3 --target arm64 -e="-march=armv9-a+sme"

func TransposeSMEF32(src, dst []float32, m, k int) {
    if m == 0 || k == 0 { return }
    mVal, kVal := int64(m), int64(k)
    transpose_sme_f32(
        unsafe.Pointer(&src[0]), unsafe.Pointer(&dst[0]),
        unsafe.Pointer(&mVal), unsafe.Pointer(&kVal))
}

func TransposeSMEF64(src, dst []float64, m, k int) {
    if m == 0 || k == 0 { return }
    mVal, kVal := int64(m), int64(k)
    transpose_sme_f64(
        unsafe.Pointer(&src[0]), unsafe.Pointer(&dst[0]),
        unsafe.Pointer(&mVal), unsafe.Pointer(&kVal))
}

func TransposeSMEF16(src, dst []hwy.Float16, m, k int) {
    if m == 0 || k == 0 { return }
    mVal, kVal := int64(m), int64(k)
    transpose_sme_f16(
        unsafe.Pointer(&src[0]), unsafe.Pointer(&dst[0]),
        unsafe.Pointer(&mVal), unsafe.Pointer(&kVal))
}

func TransposeSMEBF16(src, dst []hwy.BFloat16, m, k int) {
    if m == 0 || k == 0 { return }
    mVal, kVal := int64(m), int64(k)
    transpose_sme_bf16(
        unsafe.Pointer(&src[0]), unsafe.Pointer(&dst[0]),
        unsafe.Pointer(&mVal), unsafe.Pointer(&kVal))
}
```

## Phase 4: Dispatch

### 4.1 transpose_arm64.go

```go
//go:build !noasm && arm64

package matmul

import (
    "github.com/ajroetker/go-highway/hwy"
    "github.com/ajroetker/go-highway/hwy/contrib/matmul/asm"
)

// Minimum size for SME (streaming mode has fixed overhead)
const minSizeForSME = 64

func init() {
    // NEON is always available on ARM64
    TransposeFloat32 = asm.TransposeNEONF32
    TransposeFloat64 = asm.TransposeNEONF64
    TransposeFloat16 = asm.TransposeNEONF16
    TransposeBFloat16 = asm.TransposeNEONBF16
}
```

### 4.2 transpose_sme_darwin_arm64.go

```go
//go:build !noasm && darwin && arm64

package matmul

import (
    "github.com/ajroetker/go-highway/hwy"
    "github.com/ajroetker/go-highway/hwy/contrib/matmul/asm"
)

func init() {
    if !hwy.HasSME() {
        return
    }

    // Override with SME for large matrices
    neonF32 := TransposeFloat32
    TransposeFloat32 = func(src []float32, m, k int, dst []float32) {
        if m >= minSizeForSME && k >= minSizeForSME {
            asm.TransposeSMEF32(src, dst, m, k)
        } else {
            neonF32(src, m, k, dst)
        }
    }

    neonF64 := TransposeFloat64
    TransposeFloat64 = func(src []float64, m, k int, dst []float64) {
        if m >= minSizeForSME && k >= minSizeForSME {
            asm.TransposeSMEF64(src, dst, m, k)
        } else {
            neonF64(src, m, k, dst)
        }
    }

    neonF16 := TransposeFloat16
    TransposeFloat16 = func(src []hwy.Float16, m, k int, dst []hwy.Float16) {
        if m >= minSizeForSME && k >= minSizeForSME {
            asm.TransposeSMEF16(src, dst, m, k)
        } else {
            neonF16(src, m, k, dst)
        }
    }

    neonBF16 := TransposeBFloat16
    TransposeBFloat16 = func(src []hwy.BFloat16, m, k int, dst []hwy.BFloat16) {
        if m >= minSizeForSME && k >= minSizeForSME {
            asm.TransposeSMEBF16(src, dst, m, k)
        } else {
            neonBF16(src, m, k, dst)
        }
    }
}
```

## Phase 5: Update Existing matmul Code

### 5.1 matmul_sme_arm64.go changes

Replace the existing scalar `transposeMatrix` calls with the new SIMD versions:

```go
// Before:
transposeMatrix(a, m, k, atBuf)

// After (automatic via dispatch):
// transposeMatrix now calls Transpose2D which dispatches to SIMD
transposeMatrix(a, m, k, atBuf)
```

The generic `transposeMatrix[T]` function in `transpose.go` handles the dispatch.

## Phase 6: gomlx Integration

### 6.1 Export from go-highway

The `TransposeFloat32`, `TransposeFloat64`, etc. functions are already exported.
gomlx can call them directly or through the generic `Transpose2D`.

### 6.2 Update gomlx highway package

```go
// backends/simplego/highway/transpose.go
package highway

import (
    "unsafe"

    "github.com/ajroetker/go-highway/hwy"
    "github.com/ajroetker/go-highway/hwy/contrib/matmul"
    "github.com/gomlx/gomlx/backends/simplego"
    "github.com/gomlx/gomlx/pkg/core/dtypes"
    "github.com/gomlx/gomlx/pkg/core/dtypes/bfloat16"
    "github.com/x448/float16"
)

func init() {
    simplego.RegisterTranspose(transposeImpl{})
}

type transposeImpl struct{}

func (transposeImpl) Transpose2D(dtype dtypes.DType, src, dst any, m, k int) {
    switch dtype {
    case dtypes.Float32:
        matmul.TransposeFloat32(src.([]float32), m, k, dst.([]float32))
    case dtypes.Float64:
        matmul.TransposeFloat64(src.([]float64), m, k, dst.([]float64))
    case dtypes.Float16:
        srcSlice := src.([]float16.Float16)
        dstSlice := dst.([]float16.Float16)
        srcHwy := unsafe.Slice((*hwy.Float16)(unsafe.Pointer(unsafe.SliceData(srcSlice))), len(srcSlice))
        dstHwy := unsafe.Slice((*hwy.Float16)(unsafe.Pointer(unsafe.SliceData(dstSlice))), len(dstSlice))
        matmul.TransposeFloat16(srcHwy, m, k, dstHwy)
    case dtypes.BFloat16:
        srcSlice := src.([]bfloat16.BFloat16)
        dstSlice := dst.([]bfloat16.BFloat16)
        srcHwy := unsafe.Slice((*hwy.BFloat16)(unsafe.Pointer(unsafe.SliceData(srcSlice))), len(srcSlice))
        dstHwy := unsafe.Slice((*hwy.BFloat16)(unsafe.Pointer(unsafe.SliceData(dstSlice))), len(dstSlice))
        matmul.TransposeBFloat16(srcHwy, m, k, dstHwy)
    }
}
```

## Testing

### Unit Tests
```go
func TestTranspose2D(t *testing.T) {
    sizes := []struct{ m, k int }{
        {4, 4}, {8, 8}, {16, 16}, {32, 32},
        {64, 64}, {256, 256}, {1024, 1024},
        {5, 7}, {17, 23}, {100, 200},  // Non-aligned
    }
    for _, size := range sizes {
        t.Run(fmt.Sprintf("%dx%d", size.m, size.k), func(t *testing.T) {
            src := make([]float32, size.m*size.k)
            for i := range src { src[i] = float32(i) }

            got := make([]float32, size.k*size.m)
            want := make([]float32, size.k*size.m)

            // Reference
            for i := 0; i < size.m; i++ {
                for j := 0; j < size.k; j++ {
                    want[j*size.m+i] = src[i*size.k+j]
                }
            }

            TransposeFloat32(src, size.m, size.k, got)

            if !slices.Equal(got, want) {
                t.Errorf("mismatch")
            }
        })
    }
}
```

### Benchmarks
```go
func BenchmarkTranspose(b *testing.B) {
    for _, size := range []int{64, 256, 1024, 4096} {
        b.Run(fmt.Sprintf("%dx%d", size, size), func(b *testing.B) {
            src := make([]float32, size*size)
            dst := make([]float32, size*size)
            b.SetBytes(int64(size * size * 4 * 2)) // read + write
            b.ResetTimer()
            for i := 0; i < b.N; i++ {
                TransposeFloat32(src, size, size, dst)
            }
        })
    }
}
```

## Expected Performance

| Size | Scalar | NEON 4×4 | SME 16×16 |
|------|--------|----------|-----------|
| 64×64 | 1x | 3-4x | 5-6x |
| 256×256 | 1x | 4x | 10-12x |
| 1024×1024 | 1x | 3-4x | 12-15x |
| 4096×4096 | 1x | 3x | 15-20x |

SME excels at large sizes because:
- No shuffle instructions needed (transpose is implicit)
- Memory bandwidth bound, not compute bound
- Large 16×16 (f32) or 32×32 (f16) tiles reduce loop overhead

## Implementation Checklist

- [ ] **Phase 1**: `transpose_base.go` + hwygen generation
- [ ] **Phase 2**: `c/transpose_neon_arm64.c` + GOAT + wrappers
- [ ] **Phase 3**: `c/transpose_sme_arm64.c` + GOAT + wrappers
- [ ] **Phase 4**: Dispatch files (`transpose_arm64.go`, `transpose_sme_darwin_arm64.go`)
- [ ] **Phase 5**: Update internal `transposeMatrix` to use dispatch
- [ ] **Phase 6**: gomlx integration
- [ ] **Phase 7**: Tests and benchmarks
